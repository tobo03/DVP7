{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse.linalg import eigsh\n",
    "#from pretrainedModel import pretrainedModel\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "root = '../../'\n",
    "bits = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 3, 2,  ..., 1, 5, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor( np.load( root + r\"Features/HPO og Validering/CIFAR/y_hpo_CIfar.npy\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar():\n",
    "    \"\"\"    \n",
    "    X_train, y_train, X_val, y_val\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train = torch.tensor( np.load( root + r\"Features/HPO og Validering/CIFAR/X_hpo_Cifar.npy\" ) )\n",
    "    y_train = torch.tensor( np.load( root + r\"Features/HPO og Validering/CIFAR/y_hpo_CIfar.npy\" ) )\n",
    "\n",
    "    label_amnt = len(y_train.unique())\n",
    "    y_train = torch.nn.functional.one_hot(y_train, label_amnt)\n",
    "\n",
    "    X_val = torch.tensor( np.load( root + r\"Features/HPO og Validering/CIFAR/X_val_Cifar.npy\" ) )\n",
    "    y_val = torch.tensor( np.load( root + r\"Features/HPO og Validering/CIFAR/y_val_Cifar.npy\" ) )\n",
    "\n",
    "    label_amnt = len(y_val.unique())\n",
    "    y_val = torch.nn.functional.one_hot(y_val, label_amnt)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader(file, batchSize = 100):\n",
    "    \"\"\"\n",
    "    \"cifar\"\n",
    "\n",
    "    \"\"\"\n",
    "    file = file.lower()\n",
    "    legal_files = [\"cifar\"]\n",
    "    if file not in legal_files:\n",
    "        raise ValueError(f'The given file name was \"{file}\", expected from {legal_files}')\n",
    "\n",
    "\n",
    "    # == LOAD IN THE DATA ==\n",
    "    if file == \"cifar\":\n",
    "        X_train, y_train, _, _ = get_cifar()\n",
    "\n",
    "\n",
    "    # == MAKE DATA LOADER ==\n",
    "    train_data = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_data.append([X_train[i], y_train[i]])\n",
    "    dataloader = DataLoader(train_data, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "for i,batch  in enumerate(get_dataloader(\"cifar\")):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTSHLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DTSHLoss, self).__init__()\n",
    "\n",
    "    def forward(self, u, y, LAMBDA=1, ALPHA=1):\n",
    "        #LAMBDA = 1\n",
    "        #ALPHA  = 1\n",
    "\n",
    "        inner_product = u @ u.t()   # Similarity Matrix\n",
    "        s = y @ y.t() > 0           # A matrix that show if the two idexes are the same or not\n",
    "        count = 0\n",
    "\n",
    "        loss1 = 0\n",
    "        for row in range(s.shape[0]):\n",
    "            # if has positive pairs and negative pairs\n",
    "            if s[row].sum() != 0 and (~s[row]).sum() != 0:\n",
    "                count += 1\n",
    "                theta_positive = inner_product[row][s[row] == 1]                \n",
    "                theta_negative = inner_product[row][s[row] == 0]\n",
    "\n",
    "                triple = (theta_positive.unsqueeze(1) - theta_negative.unsqueeze(0) - ALPHA ).clamp(min=-100,max=50)\n",
    "                loss1 += -(triple - torch.log(1 + torch.exp(triple))).mean()\n",
    "\n",
    "        if count != 0:\n",
    "            loss1 = loss1 / count\n",
    "        else:\n",
    "            loss1 = 0\n",
    "\n",
    "        loss2 = LAMBDA * (u - u.sign()).pow(2).mean()\n",
    "\n",
    "        return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def one_hot_encode(a):\n",
    "    b = np.zeros((a.size, a.max() + 1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return b\n",
    "\n",
    "def mean_average_precision(test_hashes, training_hashes, test_labels, training_labels):\n",
    "    aps = []\n",
    "    if len(training_labels.shape) == 1:\n",
    "        training_labels = one_hot_encode(training_labels)\n",
    "        test_labels = one_hot_encode(test_labels)\n",
    "    for i, test_hash in enumerate(tqdm(test_hashes)):\n",
    "        label = test_labels[i]\n",
    "        distances = np.abs(training_hashes - test_hashes[i]).sum(axis=1)\n",
    "        tp = np.where((training_labels*label).sum(axis=1)>0, 1, 0)\n",
    "        hash_df = pd.DataFrame({\"distances\":distances, \"tp\":tp}).reset_index()\n",
    "        hash_df = hash_df.sort_values([\"distances\", \"index\"]).reset_index(drop=True)\n",
    "        hash_df = hash_df.drop([\"index\", \"distances\"], axis=1).reset_index()\n",
    "        hash_df = hash_df[hash_df[\"tp\"]==1]\n",
    "        hash_df[\"tp\"] = hash_df[\"tp\"].cumsum()\n",
    "        hash_df[\"index\"] = hash_df[\"index\"] +1 \n",
    "        precision = np.array(hash_df[\"tp\"]) / np.array(hash_df[\"index\"])\n",
    "        ap = precision.mean()\n",
    "        aps.append(ap)\n",
    "    \n",
    "    return np.array(aps).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earlyStop(LossList, n = 10):\n",
    "    bestVal = min(LossList)\n",
    "\n",
    "    bestVal_i = LossList.index(bestVal)\n",
    "\n",
    "    if bestVal_i < len(LossList) - n: return True\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res(X, model):\n",
    "    results =  model(X)\n",
    "    results = results.detach().numpy()\n",
    "\n",
    "    results = (results > 0.5).astype(int) \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTSHLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DTSHLoss, self).__init__()\n",
    "\n",
    "    def forward(self, u, y, LAMBDA=1, ALPHA=1):\n",
    "        #LAMBDA = 1\n",
    "        #ALPHA  = 1\n",
    "\n",
    "        inner_product = torch.cdist(u, u, p=2) \n",
    "        s = y @ y.t() > 0           # A matrix that show if the two idexes are the same or not\n",
    "\n",
    "        loss1 = torch.tensor(0.0, requires_grad=True) + 0\n",
    "        for row in range(s.shape[0]):\n",
    "            # if has positive pairs and negative pairs\n",
    "            if s[row].sum() != 0 and (~s[row]).sum() != 0:\n",
    "                \n",
    "                theta_negative = inner_product[row][s[row] == 0]\n",
    "                \n",
    "                theta_positive = inner_product[row][s[row] == 1]\n",
    "                theta_positive = theta_positive[theta_positive != 0] # remove the anchor\n",
    "\n",
    "                for p in theta_positive: \n",
    "                    n_i = torch.logical_and( (p < theta_negative), (theta_negative < p + ALPHA) )\n",
    "                    \n",
    "                    if sum(n_i) != 0:\n",
    "                        n = torch.min( theta_negative[n_i] )\n",
    "\n",
    "                        loss1 += (p - n + ALPHA).clamp(min=0)\n",
    "        \n",
    "        #if loss1 == 0: return torch.Tensor(0,  requires_grad=True)\n",
    "\n",
    "        return loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlr           = 1e-6\\nweight_decay = 1e-6\\n\\nmodel = nn.Sequential(  nn.Linear(4096,256),\\n                            nn.ReLU(),\\n                            nn.Linear(256, bits),\\n                            nn.Sigmoid()\\n                            )\\ncriterion = DTSHLoss()\\noptimizer = optim.RMSprop(model.parameters(), lr=lr , weight_decay=weight_decay)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "lr           = 1e-6\n",
    "weight_decay = 1e-6\n",
    "\n",
    "model = nn.Sequential(  nn.Linear(4096,256),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(256, bits),\n",
    "                            nn.Sigmoid()\n",
    "                            )\n",
    "criterion = DTSHLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr , weight_decay=weight_decay)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(10):\\n    \\n    dataloader = get_dataloader(\"cifar\", batchSize=50)\\n    L = []\\n\\n    for j,batch  in enumerate(dataloader):\\n        \\n\\n        X_batch = batch[0]\\n        y_batch = batch[1]\\n\\n        u = model(X_batch)\\n\\n\\n\\n        loss = criterion(u, y_batch.float(), ALPHA=3)\\n        L.append(loss)\\n\\n        loss.backward()\\n        optimizer.step()\\n    print( sum(L) / len(L) )\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(10):\n",
    "    \n",
    "    dataloader = get_dataloader(\"cifar\", batchSize=50)\n",
    "    L = []\n",
    "\n",
    "    for j,batch  in enumerate(dataloader):\n",
    "        \n",
    "\n",
    "        X_batch = batch[0]\n",
    "        y_batch = batch[1]\n",
    "\n",
    "        u = model(X_batch)\n",
    "\n",
    "\n",
    "\n",
    "        loss = criterion(u, y_batch.float(), ALPHA=3)\n",
    "        L.append(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print( sum(L) / len(L) )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HPO(HP):\n",
    "    time_start = time.time()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # LAMBDA=1, \n",
    "    # ALPHA=1\n",
    "    # lr=1e-5\n",
    "    # weight_decay=1e-5\n",
    "    # bits = 16\n",
    "\n",
    "    #ALPHA=  HP[\"alpha\"]\n",
    "    lr=     HP[\"lr\"]\n",
    "    weight_decay= HP[\"wd\"]\n",
    "    bits = HP[\"bits\"]\n",
    "\n",
    "\n",
    "    model = nn.Sequential(  nn.Linear(4096,256),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(256, bits),\n",
    "                            )\n",
    "\n",
    "    criterion = DTSHLoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr , weight_decay=weight_decay)\n",
    "\n",
    "    dataloader = get_dataloader(\"cifar\", batchSize=50)\n",
    "    historical_lostList = []\n",
    "    for i in range(1000):\n",
    "        loss_list = []\n",
    "        for j,batch  in enumerate(dataloader):\n",
    "            X_batch = batch[0]\n",
    "            y_batch = batch[1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            u = model(X_batch)\n",
    "            loss = criterion(u, y_batch.float(), ALPHA=HP[\"alpha\"] )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append( float(loss) )\n",
    "        \n",
    "        \n",
    "        mean_loss = sum(loss_list) / len(loss_list)\n",
    "        if i % 10 == 1:\n",
    "            print(i, mean_loss)\n",
    "        historical_lostList.append(mean_loss)\n",
    "\n",
    "        if earlyStop(historical_lostList, n = 10): \n",
    "            print(i, mean_loss)\n",
    "            print(\"Early Stop!!!\")\n",
    "            data[\"earlyStop\"] = True\n",
    "            break\n",
    "        \n",
    "        if time.time() - time_start > 60*30: \n",
    "            data[\"time stopage\"] = True\n",
    "            break\n",
    "\n",
    "    \n",
    "    # === EVALUATE ===\n",
    "    X_train, y_train, X_val, y_val = get_cifar()\n",
    "\n",
    "    \n",
    "    #data[\"hp\"] = HP\n",
    "    data[\"loss\"] = historical_lostList\n",
    "    data[\"map\"] = mean_average_precision(test_hashes=res(X_val, model), training_hashes=res(X_train, model), test_labels=y_val, training_labels=y_train)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "\n",
    "def _get_combos(D):\n",
    "    l = list(D)\n",
    "    curr_i = dict(zip(l,[0]*len(l)))\n",
    "    combos = []\n",
    "\n",
    "    while True:\n",
    "        for i in range(len(l)):\n",
    "            val = l[i]\n",
    "\n",
    "            if curr_i[val] > (len(D[val])-1):\n",
    "                curr_i[val] = 0\n",
    "                \n",
    "                if i+1 != len(l):\n",
    "                    curr_i[ l[i+1] ] += 1                    \n",
    "                else:\n",
    "                    return combos                    \n",
    "\n",
    "        combo = deepcopy(D)\n",
    "        for key in curr_i:\n",
    "            list_ = combo[key]\n",
    "            index = curr_i[key] \n",
    "             \n",
    "            combo[key] = list_[index]\n",
    "\n",
    "        combos.append( combo )\n",
    "        curr_i[l[0]] += 1\n",
    "\n",
    "def save_dict(d, path, indentifier=\"\"):\n",
    "    # === CLEAN PATH ===\n",
    "    path = path.replace(\"\\\\\", \"/\") # Ensures that it's always \"/\" and not \"\\\"\n",
    "\n",
    "    if path[-1] != \"/\": # Ensures that path ends with \"/\"\n",
    "        path += \"/\"\n",
    "\n",
    "    # === DEFINE FOLDER ===\n",
    "    folder_path = f\"{path}{os.environ['COMPUTERNAME']}\"\n",
    "\n",
    "    if not os.path.exists(folder_path): # Makes the Path if it doesn't exist\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # === DEFINE FILE NAME/PATH ===\n",
    "    now = str(datetime.datetime.now())\n",
    "    now = now[:now.index(\".\")].replace(\" \", \"_\").replace(\":\", \";\")\n",
    "\n",
    "    if indentifier == \"\":\n",
    "        filePath = f\"{folder_path}/{now}.json\"\n",
    "    else:\n",
    "        filePath = f\"{folder_path}/{now}_{indentifier}.json\"\n",
    "\n",
    "    # === SAVE FILE ===\n",
    "    with open(filePath, \"w\") as fp:\n",
    "        json.dump(d , fp)\n",
    "\n",
    "def read_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Opens all folder inside \"folder_path\" and reads the contents of thoose folders.\n",
    "    \"\"\"\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for folder in os.listdir(folder_path):\n",
    "        folderPath = folder_path+\"/\"+folder\n",
    "        \n",
    "        for file in os.listdir(folderPath):\n",
    "            filePath = f\"{folderPath}/{file}\"\n",
    "            with open( filePath ) as json_file:\n",
    "                data = json.load(json_file)\n",
    "                data[\"filePath\"] = filePath\n",
    "\n",
    "                data_list.append( data )\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def work(task, path, func, n_jobs=10, taskName=\"task\", shuffle_tasks=True, verbose=1): \n",
    "    # === CLEAN PATH + MAKE FOLDeR ===\n",
    "    path = path.replace(\"\\\\\", \"/\") # Ensures that it's always \"/\" and not \"\\\"\n",
    "    if path[-1] != \"/\": # Ensures that path ends with \"/\"\n",
    "        path += \"/\"\n",
    "    folder_path = f\"{path}{os.environ['COMPUTERNAME']}\"\n",
    "    if not os.path.exists(folder_path): # Makes the Path if it doesn't exist\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    # === DEFINE COMBOS / TASKS ===\n",
    "    tasks = _get_combos( task )\n",
    "    \n",
    "    done_tasks =[ d[taskName] for d in read_folder( path ) ]\n",
    "\n",
    "    remaining_tasks = [t for t in tasks if t not in done_tasks]\n",
    "\n",
    "    if shuffle_tasks: random.shuffle(remaining_tasks)\n",
    "\n",
    "    # === WRAPPER FOR \"FUNC\" ===\n",
    "    def moddified_func(t, i):               \n",
    "        start_time = time.time()\n",
    "\n",
    "        result_dict = {}\n",
    "        \n",
    "        result_dict[taskName] = t # the task\n",
    "\n",
    "        result_dict[\"result\"] = func(t) # the result of doing the task\n",
    "        \n",
    "        end_time = time.time()\n",
    "        time_spend = end_time - start_time\n",
    "        result_dict[\"time\"] = time_spend\n",
    "\n",
    "        save_dict(result_dict, path, indentifier=i)\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    # === PARALLEL PROCCESING ===\n",
    "    func_results =  Parallel(n_jobs=n_jobs, verbose=verbose)( [ delayed(moddified_func)(t, i) for i, t in enumerate(remaining_tasks) ] )\n",
    "\n",
    "    return func_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m HP \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;241m0.5\u001b[39m , \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m  : [\u001b[38;5;241m0.5\u001b[39m , \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m   : [\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m48\u001b[39m]\n\u001b[0;32m      7\u001b[0m }\n\u001b[1;32m---> 10\u001b[0m \u001b[43mwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResults\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mHPO\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTripletAgain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHPO\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 120\u001b[0m, in \u001b[0;36mwork\u001b[1;34m(task, path, func, n_jobs, taskName, shuffle_tasks, verbose)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_dict\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# === PARALLEL PROCCESING ===\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m func_results \u001b[38;5;241m=\u001b[39m  \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmoddified_func\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mremaining_tasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func_results\n",
      "File \u001b[1;32mc:\\Users\\thors\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thors\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thors\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "HP = {\n",
    "    \"lambda\" : [0.5 , 1, 2, 4],\n",
    "    \"alpha\"  : [0.5 , 1, 2, 4],\n",
    "    \"lr\"     : [1e-4 ,1e-5, 1e-6],\n",
    "    \"wd\"     : [1e-4 ,1e-5, 1e-6],\n",
    "    \"bits\"   : [12, 24, 32, 48]\n",
    "}\n",
    "\n",
    "\n",
    "work(HP, root+r\"Results\\HPO\\TripletAgain\", HPO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
