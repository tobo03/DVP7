{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from scipy.spatial import distance_matrix\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../'\n",
    "sys.path.append(root)\n",
    "from HelpfulFunctions.batchCreation import createBatch\n",
    "from HelpfulFunctions.metrics import meanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor( np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) )\n",
    "y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" )\n",
    "\n",
    "X_test = torch.tensor( np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) )\n",
    "y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAPN(a, pos, dMatrix, margin):\n",
    "    aP = [a]+pos\n",
    "    neg_i = [i for i in range(len(dMatrix)) if i not in aP ]\n",
    "\n",
    "    outPut = []\n",
    "\n",
    "    for p in  pos:\n",
    "        posDist = dMatrix[a][p]\n",
    "        \n",
    "        legal_i = [i for i in neg_i if (posDist < dMatrix[a][i]) and (dMatrix[a][i] <= posDist+margin)] # i = \"global\" index of the Hash\n",
    "        \n",
    "        legal_dist = [dMatrix[a][i] for i in legal_i]\n",
    "        \n",
    "        if legal_dist == []: continue\n",
    "        max_dist = min(legal_dist)\n",
    "\n",
    "        n = legal_i[ legal_dist.index(max_dist) ]\n",
    "        outPut.append( (a, p, n) )\n",
    "\n",
    "    return outPut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customBatch(X_train, y_train, pos_sample_bal, batchSize, pos_label):    \n",
    "    pos_i = [ i for i in range(len(X_train)) if y_train[i] == pos_label ]\n",
    "    neg_i = list(set( range(len(X_train)) ) - set(pos_i))\n",
    "\n",
    "    pos_sample_size = int(batchSize*pos_sample_bal)\n",
    "    neg_sample_size = batchSize - pos_sample_size\n",
    "\n",
    "    pos_i = random.sample( pos_i, pos_sample_size)\n",
    "    neg_i = random.sample( neg_i, neg_sample_size)\n",
    "\n",
    "    X_sample = torch.stack( [X_train[i] for i in pos_i+neg_i] )\n",
    "    y_sample = [y_train[i] for i in pos_i+neg_i]\n",
    "    return X_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tripletLoss(bits, margin, batchSize, pos_sample_bal):\n",
    "    t_start = time.time()\n",
    "    movingAvg_window = 2000\n",
    "    noImprove_breakVal = 2000\n",
    "    \n",
    "    L = []\n",
    "    mean_loss = 0\n",
    "\n",
    "    model = nn.Sequential(  nn.Linear(4096,1024),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(1024, bits),\n",
    "                            nn.Sigmoid()\n",
    "                            )\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam( model.parameters() )\n",
    "    criterion = nn.TripletMarginLoss(p=1, # Manhatten Distance\n",
    "                                    margin=margin\n",
    "                                    )\n",
    "\n",
    "    loss_list = []\n",
    "    lowest_loss = 10000\n",
    "    no_improves = 0\n",
    "\n",
    "    y_unique = list(set(y_train))\n",
    "    for i in tqdm( range(20000) ):\n",
    "        if batchSize != None:   xBatch, yBatch = customBatch( X_train, y_train, pos_sample_bal, batchSize, random.sample(y_unique,1)[0] )\n",
    "        else:                   xBatch, yBatch = createBatch(X_train, y_train, batchSize)\n",
    "\n",
    "        xBatch = xBatch.to(device)\n",
    "\n",
    "        results = model(xBatch)\n",
    "        results_np = results.cpu().detach().numpy()\n",
    "        dMatrix = distance_matrix(results_np, results_np, p=1)\n",
    "\n",
    "        APN_list = []\n",
    "\n",
    "        for label in set(yBatch):\n",
    "            pos_i_list = [j for j in range(len(yBatch)) if yBatch[j] == label]\n",
    "            for anchor_i in pos_i_list:\n",
    "                pos_i = [j for j in pos_i_list if j != anchor_i]\n",
    "                APN_list += getAPN(anchor_i, pos_i, dMatrix, margin)\n",
    "\n",
    "        if len(APN_list) > 0:\n",
    "            a_list = []\n",
    "            p_list = []\n",
    "            n_list = []\n",
    "\n",
    "            for apn in APN_list:\n",
    "                a_list.append(results[ apn[0] ])\n",
    "                p_list.append(results[ apn[1] ])\n",
    "                n_list.append(results[ apn[2] ])\n",
    "            \n",
    "            # === Improve Model ===\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion( torch.stack(a_list).to(device), \n",
    "                              torch.stack(p_list).to(device), \n",
    "                              torch.stack(n_list).to(device) )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append( float(loss) )\n",
    "            L.append( float(loss) )\n",
    "        \n",
    "        if (i > movingAvg_window):\n",
    "            while len(L) > movingAvg_window:\n",
    "                L = L[1:]\n",
    "\n",
    "            mean_loss = sum(L) / len(L)\n",
    "\n",
    "            if mean_loss < lowest_loss:\n",
    "                # print(f\"\\033[92m {'+'}\\033[00m\", float(loss))\n",
    "                lowest_loss = mean_loss\n",
    "                no_improves = 0\n",
    "            else:\n",
    "                # print(f\"\\033[91m {'-'}\\033[00m\", float(loss))\n",
    "                no_improves += 1\n",
    "            \n",
    "            if no_improves > noImprove_breakVal: break\n",
    "\n",
    "        if (i % 500) == 0: \n",
    "            #print(f\"Make Bacthes + get results: {t2-t1}\\n\", f\"Distance Matrix: {t3-t2}\\n\", f\"MAke APN List: {t4-t3}\\n\", f\"Back Propegate: {t5-t4}\")\n",
    "            \n",
    "            print(i, mean_loss , no_improves)\n",
    "            \n",
    "\n",
    "    hash_train = (model(X_train.to(device)).cpu().detach().numpy() > 0.5).astype(int)\n",
    "    hash_test = (model(X_test.to(device)).cpu().detach().numpy() > 0.5).astype(int)\n",
    "    map = meanAveragePrecision( hash_test , hash_train, y_test, y_train)\n",
    "\n",
    "    delta_t = time.time() - t_start\n",
    "\n",
    "    return model, loss_list, map, i, delta_t\n",
    "\n",
    "#bits = 32\n",
    "#margin = random.randint(0,5) # int(bits / 10)\n",
    "#batchSize = 100\n",
    "#pos_sample_bal = 0.50\n",
    "\n",
    "#model, loss_list, map, i = tripletLoss(bits, margin, batchSize, pos_sample_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpo():\n",
    "    bits = 32\n",
    "    margin          = random.uniform(0,5)\n",
    "    batchSize       = random.randint(20,120)\n",
    "    pos_sample_bal  = random.uniform(0.05, 0.5)\n",
    "\n",
    "    if margin == 0: margin = 1\n",
    "\n",
    "    hp = {\"bits\":bits,\"margin\":margin, \"batchSize\": batchSize, \"pos_sample_bal\":pos_sample_bal}\n",
    "    print(hp)\n",
    "\n",
    "    model, loss_list, map, i, delta_t = tripletLoss(bits, margin, batchSize, pos_sample_bal)\n",
    "\n",
    "    res = {}\n",
    "    res[\"hp\"] = hp\n",
    "    res[\"loss_list\"] = loss_list\n",
    "    res[\"map\"] = map\n",
    "    res[\"i\"] = i\n",
    "    res[\"delta_t\"] = delta_t\n",
    "\n",
    "\n",
    "    now = str(datetime.datetime.now())\n",
    "    now = now[:now.index(\".\")].replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "\n",
    "    newpath = f\"{root}Results/HPO/TripletLoss/{os.environ['COMPUTERNAME']}\"\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    filePath = f\"{root}Results/HPO/TripletLoss/{os.environ['COMPUTERNAME']}/{now}.json\"\n",
    "\n",
    "    with open(filePath, \"w\") as fp:\n",
    "        json.dump(res , fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bits': 32, 'margin': 3.9328348315644837, 'batchSize': 115, 'pos_sample_bal': 0.2380118343526239}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20000 [00:00<50:55,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 222/20000 [00:25<38:02,  8.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mhpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mhpo\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m hp \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m:bits,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmargin\u001b[39m\u001b[38;5;124m\"\u001b[39m:margin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchSize\u001b[39m\u001b[38;5;124m\"\u001b[39m: batchSize, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_sample_bal\u001b[39m\u001b[38;5;124m\"\u001b[39m:pos_sample_bal}\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(hp)\n\u001b[1;32m---> 12\u001b[0m model, loss_list, \u001b[38;5;28mmap\u001b[39m, i, delta_t \u001b[38;5;241m=\u001b[39m \u001b[43mtripletLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_sample_bal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m res \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     15\u001b[0m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hp\n",
      "Cell \u001b[1;32mIn[8], line 60\u001b[0m, in \u001b[0;36mtripletLoss\u001b[1;34m(bits, margin, batchSize, pos_sample_bal)\u001b[0m\n\u001b[0;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion( torch\u001b[38;5;241m.\u001b[39mstack(a_list)\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     58\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mstack(p_list)\u001b[38;5;241m.\u001b[39mto(device), \n\u001b[0;32m     59\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mstack(n_list)\u001b[38;5;241m.\u001b[39mto(device) )\n\u001b[1;32m---> 60\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     63\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;28mfloat\u001b[39m(loss) )\n",
      "File \u001b[1;32mc:\\Users\\thors\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thors\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thors\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    hpo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
