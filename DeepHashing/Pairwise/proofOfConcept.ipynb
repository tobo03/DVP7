{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 244)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #Is transformed to be better with VGG16 model\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def getDataset(train=True, sample_size=100):\n",
    "    dataset = CIFAR10(root='./data',\n",
    "                  train=train, \n",
    "                  download=True,\n",
    "                  transform=transform)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        X.append( dataset[i][0] )\n",
    "        y.append( dataset[i][1] )\n",
    "\n",
    "    X = torch.stack( X )\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = getDataset() #Training data\n",
    "X_test, y_test = getDataset(train=False) #Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(X_train.shape[0], batchSize, replace=False) # Not used as of now\n",
    "minibatch = X_train[indices]\n",
    "#minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "pretrained_model = models.vgg16(pretrained=True)\n",
    "pretrained_model.classifier = torch.nn.Identity() # 25088 features output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(25088, 10000),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10000, 4000)    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, cnn_extractor, custom_nn):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.cnn_extractor = cnn_extractor  # Pretrained CNN (frozen)\n",
    "        self.custom_nn = custom_nn  # Custom NN (trainable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn_extractor(x)  # Extract features using the CNN\n",
    "        output = self.custom_nn(features)  # Pass the features through the custom NN\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nn = CustomNN()\n",
    "model = CombinedModel(pretrained_model, custom_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagate Through Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(X_train) # torch.Size([100, 4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 32])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "mean = 0\n",
    "std_dev = np.sqrt(0.01)  # Standard deviation is the square root of the variance\n",
    "\n",
    "# Create the matrix W and V with shape (4000, 32) and (32, 1) - SHOULD BE A FUNCTION WHERE 32 IS THE NUMBER OF BITS WE WANT\n",
    "W = torch.normal(mean, std_dev, (4000, 32))\n",
    "V = torch.normal(mean, std_dev, (32, 1)) \n",
    "\n",
    "print(W.shape)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = []\n",
    "for i in range(len(result)):\n",
    "    dot = torch.matmul(W.T, result[i])\n",
    "    dot = dot.reshape(32, 1)\n",
    "    u = (dot + V)\n",
    "    U.append(u)\n",
    "U = torch.stack(U) # torch.Size([100, 32, 1])\n",
    "U = U.reshape(100, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dot product matrix -> Useful for finding Theta in loss function\n",
    "dot_product_matrix = torch.matmul(U, U.T)\n",
    "dot_product_matrix #(100, 100) Shape -> (sample_size, sample_size) Shape\n",
    "\n",
    "#Calculate Theta\n",
    "Theta = 1/2 * dot_product_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding S Based On Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = torch.tensor(y_train) # (sample_size)\n",
    "S = (y_train[:, None] == y_train).float() # (sample_size, sample_size) Shape -> 1 if labels are the same, 0 if not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding binary codes B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the Binary codes\n",
    "B = torch.sign(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "eta = 0.25 #Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, mean, std_dev, eta = 0.25, hash_length = 32):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        # Initialize W and V as learnable parameters\n",
    "        self.W = nn.Parameter(torch.normal(mean, std_dev, (4000, hash_length)))  # W will be updated during training\n",
    "        self.V = nn.Parameter(torch.normal(mean, std_dev, (hash_length, 1)))     # V will be updated during training\n",
    "        self.eta = eta  # Regularization parameter\n",
    "\n",
    "    def forward(self, inputs, targets, hash_length = 32):\n",
    "        targets = torch.tensor(targets)\n",
    "        S = (targets[:, None] == targets).float() # S calculation\n",
    "        U = (torch.matmul(self.W.T, inputs) + self.V).reshape(100, hash_length) # U calculation\n",
    "\n",
    "        #Calculate Theta\n",
    "        dot_product_matrix = torch.matmul(U, U.T)\n",
    "        dot_product_matrix # (sample_size, sample_size) Shape\n",
    "        Theta = 1/2 * dot_product_matrix\n",
    "\n",
    "        #Calculate hash codes\n",
    "        B = torch.sign(U)\n",
    "\n",
    "        loss = - torch.sum(S * Theta - torch.log(1 + torch.exp(Theta)))\n",
    "        loss += + self.eta * torch.sum(torch.norm(B - U, dim = 1).pow(2))    \n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m targets \u001b[38;5;241m=\u001b[39m y_train  \u001b[38;5;66;03m# Your target labels\u001b[39;00m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Forward pass through the combined model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute the custom loss\u001b[39;00m\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update only the custom NN's parameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m, in \u001b[0;36mCustomLoss.forward\u001b[1;34m(self, inputs, targets, hash_length)\u001b[0m\n\u001b[0;32m     12\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(targets)\n\u001b[0;32m     13\u001b[0m S \u001b[38;5;241m=\u001b[39m (targets[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m==\u001b[39m targets)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m# S calculation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m U \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mW\u001b[49m\u001b[38;5;241m.\u001b[39mT, inputs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m100\u001b[39m, hash_length) \u001b[38;5;66;03m# U calculation\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Calculate Theta\u001b[39;00m\n\u001b[0;32m     17\u001b[0m dot_product_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(U, U\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = CustomLoss(mean=0, std_dev=0.01, eta=0.25)\n",
    "\n",
    "\n",
    "for param in pretrained_model.parameters(): #LÃ¥s parameters i VGG16\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(model.custom_nn.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    inputs = X_train  # Your input data\n",
    "    targets = y_train  # Your target labels\n",
    "    outputs = model(inputs)  # Forward pass through the combined model\n",
    "    loss = loss_fn(outputs, targets)  # Compute the custom loss\n",
    "    loss.backward()  # Backpropagate\n",
    "    optimizer.step()  # Update only the custom NN's parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
