{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 244)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #Is transformed to be better with VGG16 model\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def getDataset(train=True, sample_size=100):\n",
    "    dataset = CIFAR10(root='./data',\n",
    "                  train=train, \n",
    "                  download=True,\n",
    "                  transform=transform)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        X.append( dataset[i][0] )\n",
    "        y.append( dataset[i][1] )\n",
    "\n",
    "    X = torch.stack( X )\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = getDataset() #Training data\n",
    "X_test, y_test = getDataset(train=False) #Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(X_train.shape[0], batchSize, replace=False) # Not used as of now\n",
    "minibatch = X_train[indices]\n",
    "#minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "pretrained_model = models.vgg16(pretrained=True)\n",
    "pretrained_model.classifier = torch.nn.Identity() # 25088 features output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(25088, 10000),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10000, 4000)    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, cnn_extractor, custom_nn):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.cnn_extractor = cnn_extractor  # Pretrained CNN (frozen)\n",
    "        self.custom_nn = custom_nn  # Custom NN (trainable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn_extractor(x)  # Extract features using the CNN\n",
    "        output = self.custom_nn(features)  # Pass the features through the custom NN\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nn = CustomNN()\n",
    "model = CombinedModel(pretrained_model, custom_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagate Through Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(X_train) # torch.Size([100, 4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 32])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "mean = 0\n",
    "std_dev = np.sqrt(0.01)  # Standard deviation is the square root of the variance\n",
    "\n",
    "# Create the matrix W and V with shape (4000, 32) and (32, 1) - SHOULD BE A FUNCTION WHERE 32 IS THE NUMBER OF BITS WE WANT\n",
    "W = torch.normal(mean, std_dev, (4000, 32))\n",
    "V = torch.normal(mean, std_dev, (32, 1)) \n",
    "\n",
    "print(W.shape)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = []\n",
    "for i in range(len(result)):\n",
    "    dot = torch.matmul(W.T, result[i])\n",
    "    dot = dot.reshape(32, 1)\n",
    "    u = (dot + V)\n",
    "    U.append(u)\n",
    "U = torch.stack(U) # torch.Size([100, 32, 1])\n",
    "U = U.reshape(100, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the dot product matrix -> Useful for finding Theta in loss function\n",
    "dot_product_matrix = torch.matmul(U, U.T)\n",
    "dot_product_matrix #(100, 100) Shape -> (sample_size, sample_size) Shape\n",
    "\n",
    "#Calculate Theta\n",
    "Theta = 1/2 * dot_product_matrix\n",
    "Theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding S Based On Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train) # (sample_size)\n",
    "S = (y_train[:, None] == y_train).float() # (sample_size, sample_size) Shape -> 1 if labels are the same, 0 if not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding binary codes B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the Binary codes\n",
    "B = torch.sign(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "eta = 0.25 #Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.6752,  8.6843,  5.0221,  ...,  5.7804,  3.1493,  7.0362],\n",
       "        [ 8.6843, 16.8660,  8.2876,  ...,  9.6693,  4.9741, 11.9777],\n",
       "        [ 5.0221,  8.2876,  9.9056,  ...,  7.3979,  2.4237,  7.0048],\n",
       "        ...,\n",
       "        [ 5.7804,  9.6693,  7.3979,  ..., 12.0618,  3.5190, 10.9604],\n",
       "        [ 3.1493,  4.9741,  2.4237,  ...,  3.5190,  7.1585,  5.3174],\n",
       "        [ 7.0362, 11.9777,  7.0048,  ..., 10.9604,  5.3174, 21.2790]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0 -0.0 0.21 2.32 0.30 1.33 -0.4 1.86 0.17 -0.1 -0.4 0.50 -0.0 0.28 -0.2 -0.4 1.65 -0.0 -0.2 -1.1 0.73 0.18 -0.4 0.24 0.94 -0.5 0.07 -0.2 -0.9 0.05 0.45 0.03 \n",
      "0.24 0.72 -0.9 2.27 0.96 1.50 -1.1 2.24 0.32 1.10 -0.0 0.28 1.47 0.84 -0.2 -0.6 0.67 0.29 -0.7 0.57 0.06 0.51 -0.0 -0.9 0.44 -1.9 0.34 -0.5 -2.1 -0.6 0.75 -0.0 \n",
      "0.38 -0.0 -0.6 0.76 0.00 1.32 0.33 1.05 -0.9 -0.2 -0.2 -1.1 1.36 -0.0 0.01 0.01 0.91 0.57 -0.7 -0.1 -0.1 0.16 -0.3 -0.0 0.75 -2.4 0.55 0.16 -1.1 0.71 0.75 -0.6 \n",
      "0.48 0.68 -0.3 1.49 1.08 -0.0 0.27 0.66 -0.3 0.72 0.29 -0.1 0.36 -0.2 -0.7 -0.8 0.50 0.10 -0.9 0.22 -0.0 -0.6 0.08 -0.6 0.34 -1.0 0.06 -1.3 -1.9 0.38 0.41 0.86 \n",
      "1.27 0.13 -0.3 0.67 1.19 1.18 -0.5 2.02 0.29 -0.1 0.46 -0.4 1.03 0.43 -0.1 0.16 2.18 0.12 -0.2 0.55 -0.0 0.60 -0.3 0.52 1.42 -1.0 0.73 0.29 0.11 0.51 0.42 -0.8 \n",
      "2.30 0.03 0.33 0.44 -0.4 3.67 0.38 0.62 -0.6 0.30 -0.3 -0.2 0.33 0.28 0.41 -0.3 -0.2 0.41 -0.1 0.92 0.17 0.50 1.56 0.03 -0.1 -1.9 0.64 -1.5 -2.4 0.33 1.11 1.34 \n",
      "0.92 -0.1 0.48 1.39 1.26 2.03 0.41 1.45 -0.1 1.31 -0.3 -0.1 0.85 -0.2 -1.2 -0.3 1.34 -0.0 -0.1 -1.2 1.17 -1.8 -0.2 -0.2 -0.8 -0.9 1.07 0.21 -0.1 -0.0 0.64 0.65 \n",
      "-0.3 0.70 -0.4 1.50 1.01 2.00 1.24 1.58 -1.2 -0.9 0.57 0.45 0.25 0.62 -0.5 -1.0 0.84 -0.0 1.40 -0.0 0.43 1.54 0.59 0.05 -0.5 -1.0 0.26 0.97 -1.4 1.94 0.83 -0.2 \n",
      "0.17 -0.8 0.71 0.18 1.28 1.72 1.04 1.21 -0.9 1.31 0.33 0.02 0.04 1.13 -1.4 -0.1 2.25 0.15 -0.2 -0.2 -0.2 0.11 0.93 0.34 -0.4 -1.6 -0.2 0.92 -2.2 0.75 0.19 -0.1 \n",
      "-0.5 0.30 -0.2 0.47 -0.1 1.34 -0.2 0.75 0.13 0.11 0.73 -0.0 0.23 0.18 -0.0 -0.0 1.23 -0.0 0.28 0.45 0.44 0.29 -0.6 -0.2 0.09 -1.2 -0.1 -0.3 -1.3 -0.5 0.15 0.10 \n",
      "0.26 0.88 0.25 0.80 -0.1 1.05 -0.2 0.66 0.20 -0.0 -0.2 0.72 0.43 -0.2 -0.7 -0.4 0.81 -0.2 0.00 -0.1 0.47 0.09 0.26 -0.0 -0.3 -0.5 0.17 -0.6 -0.6 0.36 -0.5 0.76 \n",
      "-1.2 -0.5 0.41 1.73 1.10 2.91 -0.3 -0.1 -1.3 1.92 0.58 -0.2 1.16 1.36 -1.2 0.24 0.27 0.53 -0.0 0.80 1.99 0.13 0.29 0.08 -0.1 -0.4 0.70 -2.8 -1.9 0.72 -0.3 -0.1 \n",
      "0.33 0.32 -1.1 1.37 1.21 2.54 -0.4 0.43 -0.0 0.36 0.72 -0.1 0.86 0.77 -0.7 -0.4 1.44 0.94 0.12 0.52 1.72 0.38 -0.5 1.03 1.99 -1.3 1.24 -0.8 -0.9 0.09 0.44 -0.1 \n",
      "-0.5 0.37 -0.5 1.01 1.88 2.21 -1.2 1.20 -1.0 1.16 -1.0 0.52 0.14 1.46 -1.1 0.14 1.02 0.33 0.42 -0.5 0.72 -0.2 -0.1 0.68 0.22 -0.1 -0.3 -1.0 -1.3 0.97 -0.3 -1.0 \n",
      "-0.9 0.65 -1.8 2.85 -0.3 1.93 -0.3 -0.1 0.19 1.26 -0.6 -0.7 0.57 0.95 -0.2 -0.7 0.41 0.57 0.45 0.12 0.02 0.23 0.85 0.61 -0.7 -1.1 0.81 -0.0 -1.7 -0.6 0.50 -1.6 \n",
      "-0.0 0.34 -0.8 1.26 0.64 1.36 -1.0 -0.2 -1.0 -0.6 -0.1 -0.1 0.40 -0.5 0.13 -0.5 1.29 0.18 -0.0 0.21 1.63 0.19 0.73 -1.2 0.65 -1.5 -0.4 -0.2 -1.3 0.94 1.19 0.90 \n",
      "-0.6 0.87 -0.7 1.35 -0.3 0.92 0.49 1.44 0.08 1.17 0.13 0.03 0.25 0.06 -0.1 -1.2 0.91 0.06 -0.2 -0.1 1.58 0.19 1.26 0.42 1.41 -0.6 0.28 -1.1 -1.1 1.00 -0.5 -0.4 \n",
      "0.76 1.41 -1.0 1.06 0.36 1.70 -0.3 2.36 -0.9 -0.0 0.16 -0.5 0.45 -0.7 -0.6 -0.4 1.25 -0.4 -0.2 0.47 -0.0 0.59 0.42 -0.4 0.80 -0.6 0.46 -0.7 -0.8 0.83 0.98 -0.3 \n",
      "1.15 0.53 0.10 1.34 1.58 1.02 -0.1 -0.1 -0.5 -0.2 -0.3 -1.2 -0.6 0.22 -0.4 0.90 0.27 -0.3 -0.1 -0.4 0.58 -0.5 -1.5 -0.3 0.61 0.64 1.09 -1.0 -1.0 0.43 0.44 -0.2 \n",
      "0.09 -0.3 -1.5 2.13 -0.0 0.75 -1.1 -0.0 -0.7 1.11 -0.6 -0.1 1.57 0.86 -1.7 0.25 2.03 0.22 0.35 1.09 2.25 0.23 -0.3 -0.4 -0.0 0.41 1.17 -0.6 -0.9 -0.4 0.59 1.03 \n",
      "1.43 0.06 -1.0 2.17 0.50 0.93 0.75 1.18 -1.5 0.47 0.36 -1.2 1.28 -0.1 -1.4 -0.7 0.33 0.53 -0.2 0.52 0.56 0.38 -0.7 -1.2 -0.1 0.09 0.59 -2.5 -0.7 0.25 0.62 -0.4 \n",
      "0.21 0.15 -0.4 1.06 0.99 1.60 -1.1 0.87 -0.4 -0.1 0.88 -0.7 0.59 -1.7 0.38 0.41 1.49 0.22 -0.7 0.94 -0.7 -0.3 0.41 -1.4 1.03 -0.3 1.43 0.20 -0.3 -0.0 0.32 -0.1 \n",
      "0.71 -0.0 0.21 1.03 0.47 0.70 -0.6 0.41 -0.1 -0.0 0.44 -0.0 0.24 0.57 -0.9 0.38 0.26 -0.3 -0.2 1.01 -0.0 -1.2 -0.5 -0.3 -0.4 -1.3 1.09 -0.2 -0.3 0.48 0.88 0.80 \n",
      "0.34 0.92 -1.0 0.91 -0.5 0.81 -0.2 0.56 -0.1 -0.0 -1.1 -0.6 0.19 0.04 -1.0 -0.2 0.84 -0.3 -0.5 0.32 1.43 -0.1 -0.1 -0.2 0.76 -0.5 0.45 -0.3 -0.4 -0.6 0.08 0.26 \n",
      "0.01 1.00 -1.1 0.80 0.27 1.13 -0.2 0.82 -0.9 0.57 0.87 -0.6 0.35 0.08 -0.6 0.18 1.42 0.14 -0.4 0.84 -0.1 -0.6 0.09 -0.2 -0.2 -1.4 1.07 -1.1 -1.4 -0.6 0.32 1.02 \n",
      "0.12 -0.0 -0.2 1.13 0.61 1.88 -1.6 0.35 -0.3 -0.1 -0.6 0.13 0.26 0.26 -2.5 -0.7 1.10 0.13 0.02 -1.4 -0.5 -0.5 -0.9 -0.5 0.31 0.87 -0.4 -0.3 -1.1 1.43 0.67 0.89 \n",
      "-0.0 0.85 -0.4 2.17 0.27 2.27 -0.5 1.00 -0.1 1.05 0.35 1.17 1.33 0.20 0.18 0.96 0.46 0.96 0.46 0.08 0.57 0.42 -0.2 -0.0 0.30 -1.6 1.76 -1.9 -3.0 0.99 0.65 0.41 \n",
      "-0.3 0.77 -0.5 1.70 0.07 1.46 -0.8 0.63 -0.6 0.96 -0.5 0.25 0.67 -0.7 -0.8 -0.2 0.71 -0.3 -0.1 0.56 0.94 0.52 1.01 -0.7 1.34 -2.2 0.21 -0.3 -0.6 0.35 -0.4 0.03 \n",
      "0.39 -0.5 -1.4 2.21 1.06 1.98 1.19 1.80 -1.4 -0.9 0.26 0.66 1.97 0.02 -0.3 -0.2 0.26 -0.5 -0.4 0.39 -1.9 0.26 0.33 -0.6 0.05 -1.2 1.09 -0.7 -1.0 0.92 0.61 -0.1 \n",
      "0.59 -1.0 -0.7 0.25 0.62 1.30 0.71 0.42 -0.9 0.92 0.37 0.18 0.98 -0.0 -0.0 -0.0 1.17 0.01 0.70 2.59 -0.1 0.37 -1.2 0.12 0.54 -0.7 0.72 -0.9 -0.7 0.98 0.52 1.08 \n",
      "0.38 0.22 -0.0 -0.5 -0.2 1.36 -1.1 1.52 -1.0 0.06 -0.9 -0.4 0.96 0.52 -0.7 -0.4 1.72 0.67 -0.3 0.69 0.69 -0.8 0.26 -0.1 0.75 -0.0 0.33 -1.2 -0.6 0.00 0.65 -0.3 \n",
      "0.65 0.40 -0.8 1.80 -0.4 1.52 0.40 0.30 -0.0 1.50 -0.0 -1.0 0.86 0.98 -0.3 0.53 0.81 -0.1 0.07 1.24 0.15 0.31 0.28 -0.3 1.50 -1.8 0.83 -0.7 -1.9 1.51 0.86 0.51 \n",
      "0.68 -0.2 -0.0 1.84 2.29 1.27 0.76 0.91 -0.8 0.32 2.00 -0.3 0.67 1.38 0.43 0.25 -0.0 -0.0 -1.6 0.99 -1.6 -0.4 0.04 0.80 -0.3 -0.2 0.59 -0.8 -2.2 -0.4 -0.2 1.34 \n",
      "-0.9 0.52 -0.6 2.57 -0.6 0.13 0.86 1.07 -0.6 0.99 -0.1 1.52 -0.1 0.22 -0.6 0.42 0.09 -0.4 0.02 0.45 0.76 -0.6 0.10 -0.0 0.61 0.24 2.28 -0.7 -1.9 -0.6 0.29 0.15 \n",
      "0.28 -0.0 0.36 2.28 -0.6 1.88 0.06 -1.4 -0.6 0.58 -0.5 -0.2 -0.1 0.15 -1.2 -0.1 0.62 -1.2 -0.4 0.55 0.19 -0.4 -1.7 -0.8 -0.1 -2.0 0.46 -1.1 -0.9 0.85 0.96 0.25 \n",
      "0.59 -1.3 -0.7 1.77 -0.3 1.92 -0.0 0.62 -1.0 0.68 -0.1 -1.0 0.75 -1.0 0.01 0.09 0.81 0.25 -0.0 0.92 -0.6 0.31 0.12 -0.6 0.36 -0.9 0.04 -2.6 -1.1 1.33 1.18 -0.3 \n",
      "1.95 0.74 -0.4 1.63 -0.2 0.73 -0.6 -0.7 -1.0 0.93 0.85 0.31 1.49 0.87 0.02 1.06 0.54 0.63 0.41 -0.4 -0.0 0.47 -0.5 -0.7 -0.1 -1.5 0.87 -0.5 -1.9 0.79 0.66 0.77 \n",
      "0.14 1.37 -0.6 1.93 -0.0 1.25 -0.5 1.01 -0.8 0.81 -0.4 -0.9 0.76 0.00 -0.8 -0.1 0.24 -0.7 -0.1 -0.5 -0.1 0.22 -0.5 -0.2 -1.2 -2.0 0.51 -0.4 -0.9 0.84 -0.0 0.33 \n",
      "0.01 -0.3 0.60 1.50 0.56 0.50 -0.3 -0.4 -1.0 0.73 -0.2 0.31 0.43 0.07 0.17 0.56 1.62 -0.1 1.55 0.24 -0.5 0.20 -0.0 0.07 1.29 -1.0 0.58 -1.4 -1.2 -0.0 -0.3 0.66 \n",
      "0.16 0.12 -0.4 0.77 0.51 0.94 -0.6 0.34 -0.2 0.91 -0.5 -0.1 0.58 -0.5 -0.8 -0.5 1.06 -0.0 0.01 -0.4 -0.0 -0.2 -0.2 -0.4 0.10 -0.5 0.48 -1.0 -0.6 -0.2 0.45 0.22 \n",
      "1.66 0.44 -1.4 2.46 1.54 1.61 -0.3 0.42 0.08 0.93 -0.6 -0.7 0.56 0.78 -1.5 2.34 0.29 1.39 -0.2 -0.5 0.56 -1.1 -0.9 0.72 0.03 -1.6 1.85 -0.6 -0.7 0.21 0.64 1.11 \n",
      "0.21 0.93 -0.4 1.03 -0.5 1.97 -1.5 1.80 -1.5 0.92 -0.1 0.29 1.45 0.60 0.13 0.68 -0.5 -0.8 0.27 0.70 0.88 0.22 0.77 -1.0 0.40 -0.1 0.27 -0.6 -1.7 0.25 0.62 -0.2 \n",
      "-0.0 -0.3 -0.9 2.09 -0.1 1.50 -1.2 0.94 -0.0 0.82 0.42 -0.7 1.26 -1.0 -0.5 0.51 0.62 0.73 -0.0 -0.1 -0.0 0.21 -0.8 -0.9 0.13 -1.5 0.46 -1.1 -0.6 -0.1 0.52 0.06 \n",
      "0.10 1.22 -0.2 1.40 -0.6 1.85 -0.5 1.24 -1.6 1.03 0.75 -0.6 0.79 0.86 -0.2 -1.1 1.44 0.35 0.22 0.60 1.84 1.08 -1.7 1.22 0.87 -0.9 0.54 -0.9 -1.9 1.82 -0.9 -0.9 \n",
      "0.20 1.02 0.72 1.59 2.04 3.07 -0.0 1.54 -1.4 0.44 0.98 -0.6 0.21 -0.4 -1.0 0.72 1.13 -0.0 -0.4 -0.6 0.41 -0.2 0.24 0.34 0.23 -1.1 1.37 -0.4 -0.0 -0.4 0.20 0.72 \n",
      "0.02 0.78 -1.2 0.49 0.73 2.15 1.44 1.37 -0.8 1.00 1.89 -0.1 1.91 1.06 -1.6 0.02 1.78 -0.2 -0.2 -0.5 -0.9 0.27 -0.0 -0.4 0.94 -0.2 0.83 -0.5 -2.6 -1.6 1.90 0.49 \n",
      "1.35 -0.6 0.03 3.00 1.65 0.32 -1.7 1.92 0.88 1.69 0.72 0.05 1.98 0.71 -0.3 -0.6 1.43 1.22 0.02 0.12 -0.2 -1.4 -0.1 -0.0 0.24 -2.4 -0.1 -2.0 -0.7 -0.2 0.29 0.56 \n",
      "0.16 -0.0 -0.5 1.62 1.06 1.51 -0.3 0.55 -0.6 0.47 -0.1 -0.2 0.28 -0.2 -0.8 -0.3 0.77 -0.2 -0.4 -0.2 0.16 0.29 0.08 0.37 -0.1 -0.8 0.06 0.23 -1.4 -0.0 -0.3 -0.0 \n",
      "0.47 0.80 0.02 2.23 1.19 2.60 -1.1 -0.3 -1.5 1.09 -0.5 0.03 2.46 0.39 -0.2 0.42 1.74 0.11 -0.0 -0.7 -0.2 0.17 -1.1 -0.5 -0.1 -0.6 1.37 -0.8 -1.3 -0.9 1.88 -0.1 \n",
      "-1.5 1.14 -0.9 1.00 1.03 0.74 0.98 1.18 -0.0 0.27 -0.2 1.68 0.72 1.16 -1.6 -0.4 1.24 -0.1 0.39 0.72 1.15 -0.6 0.21 -0.4 0.36 -1.2 0.94 -0.5 -0.5 1.39 0.55 -0.9 \n",
      "0.49 -0.8 -0.2 1.69 0.75 2.82 -0.8 -0.1 -2.7 0.55 0.70 -1.0 -0.4 0.55 -1.8 0.45 2.56 0.05 0.11 1.39 0.75 0.94 -0.4 -0.1 0.95 -2.0 -0.3 -2.0 -2.4 0.27 0.69 0.09 \n",
      "1.03 0.00 0.31 1.46 0.10 0.61 0.54 1.28 -1.8 1.62 -0.2 0.32 0.38 -0.0 1.07 0.44 -0.0 0.62 0.24 0.37 0.84 -0.3 0.85 1.06 -1.1 -0.8 -0.2 0.24 -1.7 1.86 0.19 -0.7 \n",
      "-0.5 0.24 -0.0 -0.7 0.46 1.59 -1.2 0.58 -0.0 0.86 -0.1 -0.5 0.60 -0.7 -0.4 -0.5 0.72 -0.0 -0.5 -0.4 -0.1 -1.0 -0.7 -0.0 -0.6 -1.6 0.71 -0.7 -1.0 -0.0 -0.5 0.37 \n",
      "-0.2 0.20 -0.9 0.64 0.93 0.76 -0.9 0.60 -1.2 0.48 0.83 -0.6 1.09 0.00 -0.0 0.03 0.91 1.19 -0.1 0.26 0.04 -0.5 0.05 -0.8 0.01 -1.4 0.13 -1.0 -2.3 1.15 0.21 -0.0 \n",
      "-1.0 0.62 0.23 2.68 0.01 2.57 -0.1 1.15 -1.2 1.53 -0.0 0.06 1.05 -0.4 -0.1 -1.0 0.30 -0.2 -0.7 -0.6 -0.0 -1.4 -0.3 -1.0 -0.1 -1.3 1.05 0.06 -1.8 -0.2 1.52 0.62 \n",
      "-0.4 0.88 0.13 2.57 0.69 2.27 0.86 0.91 -1.7 1.46 0.64 -0.7 1.68 1.83 -2.1 0.54 2.40 1.35 0.07 1.47 0.47 0.26 -0.0 -0.5 -2.2 -0.3 1.32 -0.2 -1.2 0.28 1.90 0.72 \n",
      "0.39 0.22 -0.5 2.99 1.50 1.83 -1.1 0.66 -1.4 0.58 0.80 -1.0 1.68 -1.1 -0.3 0.85 0.63 1.73 -0.1 -0.8 -0.5 -0.6 -1.0 -1.2 -0.9 -1.1 1.09 0.37 -1.1 0.00 -0.2 -0.9 \n",
      "0.11 0.42 -0.1 1.27 0.20 1.26 -1.8 1.11 -0.7 0.23 -0.6 0.15 0.83 0.01 -0.6 0.02 1.69 0.25 -0.6 -0.4 -0.1 -0.2 -0.5 -0.2 -0.3 -0.4 0.25 -0.8 -1.2 0.93 0.45 0.27 \n",
      "0.58 0.57 -0.5 1.07 -0.3 1.87 -0.9 0.81 -0.8 -0.1 -0.1 -0.0 0.00 0.96 -1.1 -0.5 1.22 -0.3 0.61 0.10 -0.2 0.78 0.47 -0.4 -0.1 -0.8 -0.3 -1.6 -0.6 0.48 0.47 0.03 \n",
      "1.28 0.95 -0.5 2.15 -0.5 1.54 -1.0 1.24 -0.3 0.27 -0.2 -0.4 1.41 -0.1 0.16 0.88 0.53 1.19 -1.2 -0.8 -0.2 -0.3 0.35 -0.1 -0.6 -1.4 -0.2 -0.6 -2.2 0.55 0.78 -0.2 \n",
      "0.43 0.43 -0.0 1.97 0.94 2.69 -0.6 2.32 -1.3 0.71 1.15 0.68 0.99 0.04 -0.5 -1.1 0.86 1.08 0.08 1.66 0.62 -0.3 1.23 -1.1 0.99 -2.1 -0.8 -2.1 -0.8 1.06 -0.5 0.65 \n",
      "-0.3 -1.4 -0.4 0.46 -0.0 2.63 -1.4 2.82 -0.6 -0.2 1.67 -0.2 1.34 1.63 -1.0 -0.2 2.11 0.40 -1.2 2.49 -0.0 1.08 1.65 0.45 1.29 -1.5 1.15 -0.4 -1.1 0.32 0.77 0.11 \n",
      "-0.2 -0.4 -0.4 1.33 0.60 2.38 0.12 1.16 -1.1 1.01 0.29 -0.1 0.88 0.51 0.02 0.41 1.94 0.89 -0.2 0.81 0.29 0.56 -0.3 0.83 -0.4 -0.1 0.01 -0.4 -1.6 1.76 0.57 -0.2 \n",
      "-0.0 0.45 -0.9 1.51 0.25 1.77 -0.3 1.05 -0.4 0.48 0.15 -0.4 0.56 0.31 -0.2 0.62 0.53 -0.2 -0.9 0.62 0.47 -0.9 -0.0 -0.5 -0.4 -1.8 0.30 -0.6 -1.3 -0.5 0.17 0.40 \n",
      "1.55 0.54 -0.2 0.85 -0.2 1.05 0.15 -0.4 -0.9 -0.6 0.47 -0.4 0.93 -1.3 -0.5 0.59 0.92 -0.1 0.43 -0.4 0.34 -1.1 0.43 0.63 1.82 -1.0 0.20 -0.3 -0.6 0.32 -0.1 0.29 \n",
      "-1.0 0.91 -0.5 0.73 0.43 1.24 -1.5 1.91 -0.6 0.22 0.74 -1.3 0.25 0.93 -0.2 -0.8 1.13 0.13 -0.6 1.11 -0.5 0.04 -0.9 -0.2 0.06 -1.0 0.12 -0.1 -2.0 -0.1 0.62 0.07 \n",
      "0.47 -0.6 -0.9 0.98 0.70 1.38 -0.9 0.17 0.09 1.58 -0.5 0.90 2.70 -0.0 -0.7 0.40 0.67 0.39 -0.0 0.14 0.56 -0.7 -0.5 1.07 -0.2 -1.4 0.35 -1.1 -0.2 0.38 0.35 0.50 \n",
      "-0.4 0.80 -0.5 0.13 -0.1 1.38 -1.1 0.62 -0.2 0.59 -0.6 -0.1 0.33 -0.5 0.39 -0.1 0.75 0.27 -0.2 0.75 0.17 0.02 0.16 0.43 0.95 -0.3 -0.3 -0.0 -1.4 0.83 0.76 0.57 \n",
      "-0.2 0.84 -0.9 1.57 -0.5 1.42 -0.4 1.62 -0.9 -0.1 0.07 0.73 1.56 1.08 -1.8 -0.2 0.65 -0.0 0.18 -0.2 -1.1 0.27 -0.5 -0.4 -1.1 -1.0 2.13 -1.8 -0.8 -0.3 -1.3 -0.2 \n",
      "0.32 0.42 0.02 0.18 0.45 1.24 0.02 1.56 -0.7 -0.1 -0.1 0.10 -0.2 0.09 0.20 0.07 0.36 -0.3 0.77 -0.1 -0.2 -0.1 -0.4 -0.6 0.40 0.07 0.40 -0.9 -0.6 -0.3 0.17 0.62 \n",
      "-0.3 1.27 -0.8 0.55 0.58 2.67 0.33 0.90 -1.6 -0.9 0.22 -0.8 0.35 -0.5 -0.1 0.62 0.73 0.26 -0.7 0.21 -0.4 0.98 -0.5 -0.1 -0.5 -1.8 0.62 -0.5 -1.4 0.68 0.79 0.34 \n",
      "0.28 0.81 -1.3 1.75 0.15 0.70 0.37 1.49 0.72 0.83 -0.1 -0.7 1.71 0.41 0.32 -0.9 0.47 -0.4 -0.7 0.33 0.13 0.37 0.37 -0.5 0.66 -1.1 0.83 0.00 -1.5 -0.2 0.87 0.50 \n",
      "0.80 0.07 -0.6 3.07 -0.8 1.32 -0.2 0.60 -1.3 0.98 0.36 -0.1 0.42 -0.1 -1.8 -0.9 -0.6 0.28 0.15 -0.2 0.85 0.04 -1.2 -0.9 1.15 -1.2 0.77 -0.9 -1.3 -0.6 0.08 -0.7 \n",
      "-0.1 0.43 -1.1 0.49 0.82 0.45 0.05 0.08 0.09 1.31 0.55 -0.2 1.70 -0.1 0.09 -0.3 1.64 0.60 -0.4 1.38 1.26 -1.0 -0.6 0.42 0.48 -1.0 0.42 -0.9 -1.7 -0.2 -0.2 -0.5 \n",
      "1.74 0.81 -1.2 2.28 1.70 1.76 0.87 -0.1 -0.5 1.42 -0.3 0.13 -0.1 0.10 -0.4 0.16 0.85 0.22 1.36 -0.2 0.13 1.09 -1.0 0.18 0.45 -1.3 -0.4 -1.7 -1.3 0.72 1.36 0.78 \n",
      "1.67 0.44 -1.0 0.09 0.33 1.66 -0.9 1.18 -0.9 0.41 0.87 -1.0 0.52 0.05 0.39 -0.0 1.58 0.62 0.31 0.98 0.60 -0.0 -1.3 -0.0 0.79 -1.6 -0.4 -1.0 -1.8 0.53 0.12 1.03 \n",
      "-0.8 -0.7 -1.5 0.68 -0.7 0.51 -0.8 1.02 0.84 0.19 0.92 -0.6 0.82 0.24 -0.9 0.22 0.48 -0.4 -0.4 -0.3 0.67 -0.1 0.69 0.32 0.91 -0.9 0.43 -0.3 -1.2 1.04 -0.4 0.22 \n",
      "0.46 -0.3 -0.6 1.26 -0.1 0.60 0.46 1.69 -0.4 0.91 0.47 0.00 1.44 0.63 -0.7 -0.6 0.71 0.01 0.14 -0.7 -0.0 -0.9 -0.7 0.44 -0.0 -0.9 0.53 -1.6 -0.3 0.39 0.94 -0.3 \n",
      "0.68 1.00 -0.0 1.00 0.69 -0.0 -0.7 0.13 -0.4 1.00 -0.5 -0.6 -0.0 0.36 -0.1 -0.5 0.14 -0.3 0.34 0.56 -0.1 0.21 -0.0 0.44 0.17 -0.8 -0.1 -0.4 -1.1 0.32 0.89 -0.4 \n",
      "0.82 0.15 0.72 1.57 -0.1 2.66 0.89 2.34 -0.7 -0.1 0.57 0.40 0.21 0.23 -1.4 1.11 0.86 -0.9 0.69 -0.9 0.25 -0.8 0.33 1.02 0.69 -2.0 2.07 -1.5 -1.7 -0.5 -0.0 0.37 \n",
      "-0.1 0.20 -0.8 0.91 1.24 1.44 -1.0 1.64 -0.6 -0.8 0.31 -0.3 -0.8 -0.8 -1.3 -0.0 1.28 0.63 0.37 0.15 -0.7 0.29 -0.3 0.42 0.54 -0.2 0.69 -1.6 -1.9 0.83 0.76 0.83 \n",
      "1.88 1.28 -0.7 1.53 0.52 1.82 -0.4 0.60 0.16 -0.4 0.20 0.47 0.33 -0.3 -0.6 -0.1 1.74 -1.1 0.72 0.51 -0.2 -0.2 -1.2 -0.5 -0.4 -2.7 1.81 0.18 -1.4 0.13 0.52 0.83 \n",
      "-0.2 0.97 -0.2 0.96 0.07 0.73 -1.0 0.64 -1.0 0.67 0.01 -0.1 -0.1 -0.1 -0.2 0.39 0.58 0.70 -0.7 -0.3 0.56 -0.7 -0.4 -0.2 -0.6 -0.3 0.35 -0.0 -0.2 1.04 0.90 0.74 \n",
      "0.50 -0.8 -1.8 0.70 1.41 0.38 -0.2 0.98 0.60 0.73 0.47 -0.2 0.19 -0.2 -0.7 0.12 1.41 0.57 0.54 0.46 0.32 -0.7 1.11 -0.5 -0.5 -0.1 1.05 -0.2 -1.9 -0.0 0.15 0.24 \n",
      "0.07 1.57 -0.4 0.69 0.04 3.02 -1.1 0.96 0.36 -0.0 -1.9 -1.1 0.78 1.85 -1.2 1.02 -0.0 -0.5 -0.9 1.17 0.30 -0.9 -0.4 0.06 0.91 -1.2 0.92 -0.1 -0.0 1.45 0.35 0.88 \n",
      "-0.0 0.77 -0.1 2.69 0.35 2.20 -0.1 0.08 -0.9 0.24 0.05 -1.1 -0.4 0.05 -0.7 -0.5 0.64 -1.0 0.34 0.66 0.28 -0.5 -0.6 -0.2 -0.1 -1.5 1.69 -1.3 -1.6 -0.0 0.72 0.57 \n",
      "-0.7 -0.4 -1.5 0.50 0.98 1.52 -0.6 0.91 -0.2 0.46 0.31 -1.3 1.21 1.18 -0.9 0.05 0.62 0.53 0.36 0.84 1.61 0.26 0.04 -0.0 -0.5 -2.2 0.40 -1.1 -0.7 0.01 0.66 -0.0 \n",
      "-0.5 0.03 0.14 2.19 -0.0 2.31 -0.6 1.48 0.19 0.13 0.08 -1.2 0.96 0.14 -0.6 -0.2 0.67 0.69 0.34 0.14 0.92 0.65 -0.4 -1.1 -0.4 0.65 0.88 -0.9 -1.0 0.67 -0.2 0.04 \n",
      "0.34 1.22 -1.8 0.54 2.13 1.32 -1.5 1.33 -1.6 0.94 1.72 0.58 1.48 1.71 1.24 -0.5 0.74 0.16 -2.3 -0.0 1.23 -1.0 0.21 0.17 0.01 -1.9 -0.9 0.62 -1.7 -0.1 0.75 -0.5 \n",
      "-0.4 1.10 0.14 1.60 0.72 1.29 -0.5 1.31 -1.5 0.59 0.04 -1.4 0.22 -0.7 -1.3 0.30 0.48 -0.1 -1.1 0.01 0.08 -0.2 -0.6 -0.1 -0.6 -0.5 0.86 -1.7 -1.7 -0.2 -0.4 -0.0 \n",
      "1.49 -0.2 -0.4 4.14 0.19 1.57 0.01 1.77 -1.5 0.66 -0.5 -0.2 0.82 0.72 -1.2 1.39 -0.6 0.34 -0.2 0.35 -0.5 0.04 0.28 0.08 -1.4 -0.4 0.71 -0.3 -1.7 0.06 1.01 -0.5 \n",
      "-0.7 -0.0 -1.0 2.19 -0.0 0.99 0.72 1.07 0.13 -1.0 0.40 -1.5 0.47 -0.4 -0.7 0.36 1.56 0.08 1.51 0.21 0.02 -0.0 -0.0 -0.2 0.09 -0.0 1.22 0.49 -2.7 -0.1 0.79 0.31 \n",
      "-0.3 -0.8 -0.6 2.70 -0.0 1.80 0.40 0.33 -0.9 1.98 -0.5 0.36 0.48 0.74 -0.9 0.38 3.21 0.86 -0.5 1.04 -0.2 -0.2 -1.2 0.04 -1.2 -0.4 0.11 0.08 -0.6 1.83 1.08 0.60 \n",
      "0.44 0.41 0.11 1.21 1.22 2.05 -0.2 0.48 -1.3 0.22 0.26 -0.2 0.21 0.50 0.47 -0.0 1.35 -1.1 0.03 0.52 0.42 -0.2 -0.4 0.21 -1.0 -1.8 0.44 0.07 -1.0 0.40 1.23 0.81 \n",
      "0.79 0.58 0.46 1.11 0.53 1.28 0.01 0.86 0.42 0.34 -0.5 -0.1 -0.0 -0.3 -1.0 -0.0 0.36 0.34 0.07 0.51 0.40 -0.8 -0.2 0.85 0.79 -1.7 -0.0 -0.3 -1.0 0.17 -0.4 0.66 \n",
      "-0.5 -0.0 0.57 0.52 0.53 1.53 0.51 0.17 -0.8 0.53 -0.0 -0.4 -0.0 -0.3 -0.2 -0.3 0.59 -0.8 0.45 -0.4 0.97 -0.4 0.10 0.01 0.33 -0.0 0.61 -1.7 -1.3 -0.0 0.92 0.36 \n",
      "-0.1 2.15 0.99 1.31 1.10 1.26 0.00 2.20 -1.3 -0.0 0.03 -0.5 0.79 0.46 -0.9 -1.1 0.12 0.04 -0.4 0.40 0.20 0.31 0.47 -0.4 -0.7 -1.6 0.73 0.01 -2.1 -0.7 -0.3 0.47 \n",
      "0.08 1.90 -0.0 1.32 0.22 1.60 -0.2 1.39 -0.4 0.04 -0.2 -0.8 0.27 0.03 0.33 0.04 0.60 0.45 -1.5 0.81 0.48 -0.1 0.27 -0.2 1.05 -1.9 -0.2 -0.8 -1.3 0.94 -0.2 0.70 \n",
      "0.29 -0.2 -1.2 0.70 0.14 0.47 -0.1 1.17 0.04 0.05 -0.0 -0.5 -0.4 -0.2 -0.8 -0.0 -0.5 0.06 0.38 0.88 1.77 0.09 -1.0 0.16 0.35 -0.3 0.29 -1.3 -1.1 -0.1 0.42 0.32 \n",
      "0.33 1.30 -0.7 0.87 1.03 0.69 0.01 1.91 -0.7 0.37 -0.2 0.14 1.79 1.41 -0.6 -2.0 0.81 -0.3 -1.6 0.86 1.53 0.50 0.62 0.17 0.93 -0.6 0.76 -1.4 -2.1 2.18 0.37 1.68 \n"
     ]
    }
   ],
   "source": [
    "for u in U:\n",
    "    for i in u:\n",
    "        print(str(float(i))[:4], end = \" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dot product matrix -> Useful for finding Theta in loss function\n",
    "dot_product_matrix = torch.matmul(U, U.T)\n",
    "dot_product_matrix #(100, 100) Shape -> (sample_size, sample_size) Shape\n",
    "\n",
    "#Calculate Theta\n",
    "Theta = 1/2 * dot_product_matrix\n",
    "Theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1130.)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.6752,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, 16.8660,  8.2876,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  8.2876,  9.9056,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., 12.0618,  0.0000, 10.9604],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  7.1585,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., 10.9604,  0.0000, 21.2790]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(S * Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8399.2637, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(S * Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, mean, std_dev, eta = 0.25, hash_length = 32):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        # Initialize W and V as learnable parameters\n",
    "        self.W = nn.Parameter(torch.normal(mean, std_dev, (4000, hash_length)))  # W will be updated during training\n",
    "        self.V = nn.Parameter(torch.normal(mean, std_dev, (hash_length, 1)))     # V will be updated during training\n",
    "        self.eta = eta  # Regularization parameter\n",
    "\n",
    "    def forward(self, inputs, targets, hash_length = 32):\n",
    "        targets = torch.tensor(targets)\n",
    "        S = (targets[:, None] == targets).float() # S calculation\n",
    "        U = (torch.matmul(self.W.T, inputs) + self.V).reshape(100, hash_length) # U calculation\n",
    "\n",
    "        #Calculate Theta\n",
    "        dot_product_matrix = torch.matmul(U, U.T)\n",
    "        dot_product_matrix # (sample_size, sample_size) Shape\n",
    "        Theta = 1/2 * dot_product_matrix\n",
    "\n",
    "        #Calculate hash codes\n",
    "        B = torch.sign(U)\n",
    "\n",
    "        loss = - torch.sum(S * Theta - torch.log(1 + torch.exp(Theta)))\n",
    "        loss += + self.eta * torch.sum(torch.norm(B - U, dim = 1).pow(2))    \n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta2 = Theta * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.log( 1 + torch.exp(Theta2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54277"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-8399 + 62676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- torch.sum(S * Theta2 - torch.log(1 + torch.exp(Theta2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m targets \u001b[38;5;241m=\u001b[39m y_train  \u001b[38;5;66;03m# Your target labels\u001b[39;00m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Forward pass through the combined model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute the custom loss\u001b[39;00m\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update only the custom NN's parameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m, in \u001b[0;36mCustomLoss.forward\u001b[1;34m(self, inputs, targets, hash_length)\u001b[0m\n\u001b[0;32m     12\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(targets)\n\u001b[0;32m     13\u001b[0m S \u001b[38;5;241m=\u001b[39m (targets[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m==\u001b[39m targets)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;66;03m# S calculation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m U \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mW\u001b[49m\u001b[38;5;241m.\u001b[39mT, inputs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m100\u001b[39m, hash_length) \u001b[38;5;66;03m# U calculation\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Calculate Theta\u001b[39;00m\n\u001b[0;32m     17\u001b[0m dot_product_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(U, U\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = CustomLoss(mean=0, std_dev=0.01, eta=0.25)\n",
    "\n",
    "\n",
    "for param in pretrained_model.parameters(): #Lås parameters i VGG16\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(model.custom_nn.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    inputs = X_train  # Your input data\n",
    "    targets = y_train  # Your target labels\n",
    "    outputs = model(inputs)  # Forward pass through the combined model\n",
    "    loss = loss_fn(outputs, targets)  # Compute the custom loss\n",
    "    loss.backward()  # Backpropagate\n",
    "    optimizer.step()  # Update only the custom NN's parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
