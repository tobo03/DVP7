{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../../'\n",
    "sys.path.append(root)\n",
    "from HelpfulFunctions.batchCreation import createBatch\n",
    "from HelpfulFunctions.metrics import meanAveragePrecision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(root, num_classes, batch_size, train = 1, HPO = False):\n",
    "    if HPO == False:\n",
    "        if train == 1:\n",
    "            #Create X_train_tensor\n",
    "            X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "\n",
    "            #pca = PCA(n_components=128)\n",
    "            #pca.fit(X_train)\n",
    "            #X_train = torch.tensor(pca.transform(X_train), dtype=torch.float)\n",
    "            #X_test = torch.tensor(pca.transform(X_test), dtype=torch.float)\n",
    "            X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "            #Create Y_train_tensor\n",
    "            y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "            y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return train_loader\n",
    "\n",
    "        elif train == 2:\n",
    "            X_validation = np.load( root + \"Features/val_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "            X_validation_tensor = torch.tensor(X_validation)\n",
    "\n",
    "            y_validation = np.load( root + \"Features/val_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "            y_validation_tensor = torch.tensor(y_validation, dtype=torch.long)\n",
    "            y_validation_tensor = torch.nn.functional.one_hot(y_validation_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_validation = torch.arange(len(X_validation_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_validation_tensor, y_validation_tensor, indices_validation)\n",
    "            validation_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return validation_loader\n",
    "\n",
    "        elif train == 3:\n",
    "            X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "            X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "            y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "            y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "            y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "            test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return test_loader\n",
    "\n",
    "\n",
    "\n",
    "    if HPO == True:\n",
    "        if train == 1:\n",
    "            #Create X_train_tensor\n",
    "            X_train = np.load( root + \"Features/X_hpo_Cifar.npy\" ) # Shape = (45000, 4096)\n",
    "            X_train_tensor = torch.tensor(X_train)\n",
    "            #print(X_train_tensor.shape)\n",
    "\n",
    "            #Create Y_train_tensor\n",
    "            y_train = np.load( root + \"Features/y_hpo_CIfar.npy\" ) # Shape = (45000,)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "            y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "            #print(y_train_tensor.shape)\n",
    "\n",
    "            #Create indices\n",
    "            indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return train_loader\n",
    "\n",
    "        elif train == 2:\n",
    "            X_validation = np.load( root + \"Features/X_val_Cifar.npy\" ) # Shape = (10000, 4096)\n",
    "            X_validation_tensor = torch.tensor(X_validation)\n",
    "            #print(X_validation_tensor.shape)\n",
    "\n",
    "            y_validation = np.load( root + \"Features/y_val_Cifar.npy\" ) # Shape = (10000,)\n",
    "            y_validation_tensor = torch.tensor(y_validation, dtype=torch.long)\n",
    "            y_validation_tensor = torch.nn.functional.one_hot(y_validation_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "            #print(y_validation_tensor.shape)\n",
    "\n",
    "            #Create indices\n",
    "            indices_validation = torch.arange(len(X_validation_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_validation_tensor, y_validation_tensor, indices_validation)\n",
    "            validation_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return validation_loader\n",
    "\n",
    "        elif train == 3:\n",
    "            X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\") # Shape = (10000, 4096)\n",
    "            X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "            y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\") # Shape = (10000,)\n",
    "            y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "            y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "            test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 1, HPO=True)\n",
    "#test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 2)\n",
    "validation_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 2, HPO = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, bits):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, bits),    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "        # Initialize weights and biases from gaussian distribution\n",
    "        for layer in self.fc_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=0.01)  # Initialize weights based on paper\n",
    "                nn.init.normal_(layer.bias, mean=0.0, std=0.01)    # Initialize biases based on paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPSHLoss(torch.nn.Module):\n",
    "    def __init__(self, train_size, n_classes, bit):\n",
    "        super(DPSHLoss, self).__init__()\n",
    "        self.U = torch.zeros(train_size, bit).float().to(device)\n",
    "        self.Y = torch.zeros(train_size, n_classes).float().to(device)\n",
    "\n",
    "    def forward(self, u, y, ind, eta):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        s = (y @ self.Y.t() > 0).float().clamp(max = 1)\n",
    "        inner_product = u @ self.U.t() * 0.5\n",
    "\n",
    "        likelihood_loss = (1 + (-(inner_product.abs())).exp()).log() + inner_product.clamp(min=0) - s * inner_product\n",
    "\n",
    "        likelihood_loss = likelihood_loss.mean()\n",
    "\n",
    "        quantization_loss = eta * (u - u.sign()).pow(2).mean()\n",
    "\n",
    "        return likelihood_loss + quantization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid\n",
    "\n",
    "def DPSH(device: torch.device, train_size: int, n_classes: int, bit: int, num_epoch: int, batch_size: int, eta_values: list, wd_values: list, lr_values: list):\n",
    "\n",
    "    train_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 1, HPO=True)\n",
    "    #test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 2)\n",
    "    validation_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 2, HPO = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "        'eta': eta_values,\n",
    "        'learning_rate': lr_values,\n",
    "        #'batch_size': [16, 32, 64],\n",
    "        'weight_decay': wd_values\n",
    "    }\n",
    "\n",
    "    customLoss = DPSHLoss(train_size, n_classes, bit)\n",
    "\n",
    "\n",
    "    # Get all combinations of parameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    parameter_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # Evaluate each parameter combination\n",
    "    best_params = None\n",
    "    best_map = 0\n",
    "\n",
    "    for params in parameter_combinations:\n",
    "        print(f\"Testing combination: {params}\")\n",
    "    \n",
    "        # Initialize loss function with specific parameters\n",
    "        #loss_fn = customLoss()\n",
    "\n",
    "        # Initialize model and optimizer\n",
    "        model = CustomNN(bits = bit).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay = params['weight_decay'])\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epoch):  # Example epoch count\n",
    "            current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "            print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            \"DPSH\", epoch + 1, num_epoch, current_time, bit, \"CIFAR\"), end=\"\")\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for image, label, ind in train_loader:\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                u = model(image)\n",
    "\n",
    "                loss = customLoss(u, label.float(), ind, eta = params['eta'])\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss = train_loss / (train_size / batch_size)\n",
    "            print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        hash_train = []\n",
    "        label_train = []\n",
    "\n",
    "        hash_val = []\n",
    "        label_val = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, label, ind in train_loader:\n",
    "                image = image.to(device)\n",
    "                label_batch = label.to(device)\n",
    "                hash_train_batch = (model(image)).sign()\n",
    "                hash_train.append(hash_train_batch)\n",
    "                label_train.append(label_batch)\n",
    "                #hash_codes_matrix = np.vstack(hash_code_batches)\n",
    "                #print(hash_codes_matrix.shape) \n",
    "\n",
    "            hash_train = torch.cat(hash_train, dim = 0).cpu().numpy()\n",
    "            label_train = torch.cat(label_train, dim = 0).cpu().numpy()\n",
    "\n",
    "            \n",
    "            for image, label, ind in validation_loader:\n",
    "                image = image.to(device)\n",
    "                label_batch = label.to(device)\n",
    "                hash_val_batch = (model(image)).sign()\n",
    "                hash_val.append(hash_val_batch)\n",
    "                label_val.append(label_batch)\n",
    "            hash_val = torch.cat(hash_val, dim = 0).cpu().numpy()\n",
    "            label_val = torch.cat(label_val, dim = 0).cpu().numpy()\n",
    "\n",
    "            map = meanAveragePrecision(training_hashes = hash_train, training_labels = label_train, test_hashes = hash_val, test_labels = label_val)\n",
    "\n",
    "            print(f\"Validation mAP: {map:.4f}\")\n",
    "\n",
    "            # Update best parameters\n",
    "            if map > best_map:\n",
    "                best_map = map\n",
    "                best_params = params\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best mAP:\", best_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination: {'eta': 0.01, 'learning_rate': 1e-06, 'weight_decay': 1e-06}\n",
      "DPSH[ 1/150][10:39:10] bit:24, dataset:CIFAR, training... loss:0.75330\n",
      "DPSH[ 2/150][10:39:11] bit:24, dataset:CIFAR, training... loss:0.71506\n",
      "DPSH[ 3/150][10:39:11] bit:24, dataset:CIFAR, training... loss:0.72560\n",
      "DPSH[ 4/150][10:39:11] bit:24, dataset:CIFAR, training... loss:0.71337\n",
      "DPSH[ 5/150][10:39:11] bit:24, dataset:CIFAR, training... loss:0.71628\n",
      "DPSH[ 6/150][10:39:11] bit:24, dataset:CIFAR, training... loss:0.71272\n",
      "DPSH[ 7/150][10:39:12] bit:24, dataset:CIFAR, training... loss:0.71190\n",
      "DPSH[ 8/150][10:39:12] bit:24, dataset:CIFAR, training... loss:0.71122\n",
      "DPSH[ 9/150][10:39:12] bit:24, dataset:CIFAR, training... loss:0.71073\n",
      "DPSH[10/150][10:39:12] bit:24, dataset:CIFAR, training... loss:0.71258\n",
      "DPSH[11/150][10:39:12] bit:24, dataset:CIFAR, training... loss:0.71075\n",
      "DPSH[12/150][10:39:13] bit:24, dataset:CIFAR, training... loss:0.71196\n",
      "DPSH[13/150][10:39:13] bit:24, dataset:CIFAR, training... loss:0.71081\n",
      "DPSH[14/150][10:39:13] bit:24, dataset:CIFAR, training... loss:0.71016\n",
      "DPSH[15/150][10:39:13] bit:24, dataset:CIFAR, training... loss:0.70816\n",
      "DPSH[16/150][10:39:13] bit:24, dataset:CIFAR, training... loss:0.70625\n",
      "DPSH[17/150][10:39:14] bit:24, dataset:CIFAR, training... loss:0.70709\n",
      "DPSH[18/150][10:39:14] bit:24, dataset:CIFAR, training... loss:0.70280\n",
      "DPSH[19/150][10:39:14] bit:24, dataset:CIFAR, training... loss:0.70332\n",
      "DPSH[20/150][10:39:14] bit:24, dataset:CIFAR, training... loss:0.70156\n",
      "DPSH[21/150][10:39:14] bit:24, dataset:CIFAR, training... loss:0.69892\n",
      "DPSH[22/150][10:39:15] bit:24, dataset:CIFAR, training... loss:0.69987\n",
      "DPSH[23/150][10:39:15] bit:24, dataset:CIFAR, training... loss:0.69747\n",
      "DPSH[24/150][10:39:15] bit:24, dataset:CIFAR, training... loss:0.69724\n",
      "DPSH[25/150][10:39:15] bit:24, dataset:CIFAR, training... loss:0.69715\n",
      "DPSH[26/150][10:39:15] bit:24, dataset:CIFAR, training... loss:0.69544\n",
      "DPSH[27/150][10:39:16] bit:24, dataset:CIFAR, training... loss:0.69622\n",
      "DPSH[28/150][10:39:16] bit:24, dataset:CIFAR, training... loss:0.69547\n",
      "DPSH[29/150][10:39:16] bit:24, dataset:CIFAR, training... loss:0.69454\n",
      "DPSH[30/150][10:39:16] bit:24, dataset:CIFAR, training... loss:0.69525\n",
      "DPSH[31/150][10:39:16] bit:24, dataset:CIFAR, training... loss:0.69429\n",
      "DPSH[32/150][10:39:17] bit:24, dataset:CIFAR, training... loss:0.69512\n",
      "DPSH[33/150][10:39:17] bit:24, dataset:CIFAR, training... loss:0.69404\n",
      "DPSH[34/150][10:39:17] bit:24, dataset:CIFAR, training... loss:0.69271\n",
      "DPSH[35/150][10:39:17] bit:24, dataset:CIFAR, training... loss:0.69549\n",
      "DPSH[36/150][10:39:17] bit:24, dataset:CIFAR, training... loss:0.69304\n",
      "DPSH[37/150][10:39:17] bit:24, dataset:CIFAR, training... loss:0.69124\n",
      "DPSH[38/150][10:39:18] bit:24, dataset:CIFAR, training... loss:0.69311\n",
      "DPSH[39/150][10:39:18] bit:24, dataset:CIFAR, training... loss:0.69232\n",
      "DPSH[40/150][10:39:18] bit:24, dataset:CIFAR, training... loss:0.69131\n",
      "DPSH[41/150][10:39:18] bit:24, dataset:CIFAR, training... loss:0.69043\n",
      "DPSH[42/150][10:39:18] bit:24, dataset:CIFAR, training... loss:0.69209\n",
      "DPSH[43/150][10:39:19] bit:24, dataset:CIFAR, training... loss:0.68977\n",
      "DPSH[44/150][10:39:19] bit:24, dataset:CIFAR, training... loss:0.68835\n",
      "DPSH[45/150][10:39:19] bit:24, dataset:CIFAR, training... loss:0.69135\n",
      "DPSH[46/150][10:39:19] bit:24, dataset:CIFAR, training... loss:0.68841\n",
      "DPSH[47/150][10:39:19] bit:24, dataset:CIFAR, training... loss:0.68761\n",
      "DPSH[48/150][10:39:20] bit:24, dataset:CIFAR, training... loss:0.69105\n",
      "DPSH[49/150][10:39:20] bit:24, dataset:CIFAR, training... loss:0.68624\n",
      "DPSH[50/150][10:39:20] bit:24, dataset:CIFAR, training... loss:0.68702\n",
      "DPSH[51/150][10:39:20] bit:24, dataset:CIFAR, training... loss:0.68937\n",
      "DPSH[52/150][10:39:20] bit:24, dataset:CIFAR, training... loss:0.68612\n",
      "DPSH[53/150][10:39:21] bit:24, dataset:CIFAR, training... loss:0.68577\n",
      "DPSH[54/150][10:39:21] bit:24, dataset:CIFAR, training... loss:0.68664\n",
      "DPSH[55/150][10:39:21] bit:24, dataset:CIFAR, training... loss:0.68468\n",
      "DPSH[56/150][10:39:21] bit:24, dataset:CIFAR, training... loss:0.68507\n",
      "DPSH[57/150][10:39:21] bit:24, dataset:CIFAR, training... loss:0.68482\n",
      "DPSH[58/150][10:39:21] bit:24, dataset:CIFAR, training... loss:0.68371\n",
      "DPSH[59/150][10:39:22] bit:24, dataset:CIFAR, training... loss:0.68370\n",
      "DPSH[60/150][10:39:22] bit:24, dataset:CIFAR, training... loss:0.68317\n",
      "DPSH[61/150][10:39:22] bit:24, dataset:CIFAR, training... loss:0.68308\n",
      "DPSH[62/150][10:39:22] bit:24, dataset:CIFAR, training... loss:0.68289\n",
      "DPSH[63/150][10:39:22] bit:24, dataset:CIFAR, training... loss:0.68277\n",
      "DPSH[64/150][10:39:23] bit:24, dataset:CIFAR, training... loss:0.68293\n",
      "DPSH[65/150][10:39:23] bit:24, dataset:CIFAR, training... loss:0.68236\n",
      "DPSH[66/150][10:39:23] bit:24, dataset:CIFAR, training... loss:0.68315\n",
      "DPSH[67/150][10:39:23] bit:24, dataset:CIFAR, training... loss:0.68353\n",
      "DPSH[68/150][10:39:23] bit:24, dataset:CIFAR, training... loss:0.68249\n",
      "DPSH[69/150][10:39:24] bit:24, dataset:CIFAR, training... loss:0.68200\n",
      "DPSH[70/150][10:39:24] bit:24, dataset:CIFAR, training... loss:0.68305\n",
      "DPSH[71/150][10:39:24] bit:24, dataset:CIFAR, training... loss:0.68163\n",
      "DPSH[72/150][10:39:24] bit:24, dataset:CIFAR, training... loss:0.68051\n",
      "DPSH[73/150][10:39:24] bit:24, dataset:CIFAR, training... loss:0.68197\n",
      "DPSH[74/150][10:39:24] bit:24, dataset:CIFAR, training... loss:0.68113\n",
      "DPSH[75/150][10:39:25] bit:24, dataset:CIFAR, training... loss:0.67907\n",
      "DPSH[76/150][10:39:25] bit:24, dataset:CIFAR, training... loss:0.68029\n",
      "DPSH[77/150][10:39:25] bit:24, dataset:CIFAR, training... loss:0.68016\n",
      "DPSH[78/150][10:39:25] bit:24, dataset:CIFAR, training... loss:0.67839\n",
      "DPSH[79/150][10:39:25] bit:24, dataset:CIFAR, training... loss:0.67884\n",
      "DPSH[80/150][10:39:26] bit:24, dataset:CIFAR, training... loss:0.67825\n",
      "DPSH[81/150][10:39:26] bit:24, dataset:CIFAR, training... loss:0.67898\n",
      "DPSH[82/150][10:39:26] bit:24, dataset:CIFAR, training... loss:0.67755\n",
      "DPSH[83/150][10:39:26] bit:24, dataset:CIFAR, training... loss:0.67750\n",
      "DPSH[84/150][10:39:26] bit:24, dataset:CIFAR, training... loss:0.67803\n",
      "DPSH[85/150][10:39:27] bit:24, dataset:CIFAR, training... loss:0.67716\n",
      "DPSH[86/150][10:39:27] bit:24, dataset:CIFAR, training... loss:0.67775\n",
      "DPSH[87/150][10:39:27] bit:24, dataset:CIFAR, training... loss:0.67716\n",
      "DPSH[88/150][10:39:27] bit:24, dataset:CIFAR, training... loss:0.67683\n",
      "DPSH[89/150][10:39:27] bit:24, dataset:CIFAR, training... loss:0.67813\n",
      "DPSH[90/150][10:39:27] bit:24, dataset:CIFAR, training... loss:0.67662\n",
      "DPSH[91/150][10:39:28] bit:24, dataset:CIFAR, training... loss:0.67572\n",
      "DPSH[92/150][10:39:28] bit:24, dataset:CIFAR, training... loss:0.67811\n",
      "DPSH[93/150][10:39:28] bit:24, dataset:CIFAR, training... loss:0.67619\n",
      "DPSH[94/150][10:39:28] bit:24, dataset:CIFAR, training... loss:0.67563\n",
      "DPSH[95/150][10:39:28] bit:24, dataset:CIFAR, training... loss:0.67788\n",
      "DPSH[96/150][10:39:29] bit:24, dataset:CIFAR, training... loss:0.67545\n",
      "DPSH[97/150][10:39:29] bit:24, dataset:CIFAR, training... loss:0.67563\n",
      "DPSH[98/150][10:39:29] bit:24, dataset:CIFAR, training... loss:0.67762\n",
      "DPSH[99/150][10:39:29] bit:24, dataset:CIFAR, training... loss:0.67608\n",
      "DPSH[100/150][10:39:29] bit:24, dataset:CIFAR, training... loss:0.67526\n",
      "DPSH[101/150][10:39:30] bit:24, dataset:CIFAR, training... loss:0.67623\n",
      "DPSH[102/150][10:39:30] bit:24, dataset:CIFAR, training... loss:0.67574\n",
      "DPSH[103/150][10:39:30] bit:24, dataset:CIFAR, training... loss:0.67450\n",
      "DPSH[104/150][10:39:30] bit:24, dataset:CIFAR, training... loss:0.67427\n",
      "DPSH[105/150][10:39:30] bit:24, dataset:CIFAR, training... loss:0.67561\n",
      "DPSH[106/150][10:39:30] bit:24, dataset:CIFAR, training... loss:0.67261\n",
      "DPSH[107/150][10:39:31] bit:24, dataset:CIFAR, training... loss:0.67213\n",
      "DPSH[108/150][10:39:31] bit:24, dataset:CIFAR, training... loss:0.67437\n",
      "DPSH[109/150][10:39:31] bit:24, dataset:CIFAR, training... loss:0.67118\n",
      "DPSH[110/150][10:39:31] bit:24, dataset:CIFAR, training... loss:0.67147\n",
      "DPSH[111/150][10:39:31] bit:24, dataset:CIFAR, training... loss:0.67169\n",
      "DPSH[112/150][10:39:32] bit:24, dataset:CIFAR, training... loss:0.67003\n",
      "DPSH[113/150][10:39:32] bit:24, dataset:CIFAR, training... loss:0.67053\n",
      "DPSH[114/150][10:39:32] bit:24, dataset:CIFAR, training... loss:0.67019\n",
      "DPSH[115/150][10:39:32] bit:24, dataset:CIFAR, training... loss:0.66968\n",
      "DPSH[116/150][10:39:32] bit:24, dataset:CIFAR, training... loss:0.67025\n",
      "DPSH[117/150][10:39:32] bit:24, dataset:CIFAR, training... loss:0.66867\n",
      "DPSH[118/150][10:39:33] bit:24, dataset:CIFAR, training... loss:0.66956\n",
      "DPSH[119/150][10:39:33] bit:24, dataset:CIFAR, training... loss:0.67098\n",
      "DPSH[120/150][10:39:33] bit:24, dataset:CIFAR, training... loss:0.66877\n",
      "DPSH[121/150][10:39:33] bit:24, dataset:CIFAR, training... loss:0.66792\n",
      "DPSH[122/150][10:39:33] bit:24, dataset:CIFAR, training... loss:0.67099\n",
      "DPSH[123/150][10:39:34] bit:24, dataset:CIFAR, training... loss:0.66849\n",
      "DPSH[124/150][10:39:34] bit:24, dataset:CIFAR, training... loss:0.66726\n",
      "DPSH[125/150][10:39:34] bit:24, dataset:CIFAR, training... loss:0.67002\n",
      "DPSH[126/150][10:39:34] bit:24, dataset:CIFAR, training... loss:0.66842\n",
      "DPSH[127/150][10:39:34] bit:24, dataset:CIFAR, training... loss:0.66693\n",
      "DPSH[128/150][10:39:34] bit:24, dataset:CIFAR, training... loss:0.66832\n",
      "DPSH[129/150][10:39:35] bit:24, dataset:CIFAR, training... loss:0.66796\n",
      "DPSH[130/150][10:39:35] bit:24, dataset:CIFAR, training... loss:0.66706\n",
      "DPSH[131/150][10:39:35] bit:24, dataset:CIFAR, training... loss:0.66742\n",
      "DPSH[132/150][10:39:35] bit:24, dataset:CIFAR, training... loss:0.66744\n",
      "DPSH[133/150][10:39:35] bit:24, dataset:CIFAR, training... loss:0.66618\n",
      "DPSH[134/150][10:39:36] bit:24, dataset:CIFAR, training... loss:0.66673\n",
      "DPSH[135/150][10:39:36] bit:24, dataset:CIFAR, training... loss:0.66623\n",
      "DPSH[136/150][10:39:36] bit:24, dataset:CIFAR, training... loss:0.66638\n",
      "DPSH[137/150][10:39:36] bit:24, dataset:CIFAR, training... loss:0.66621\n",
      "DPSH[138/150][10:39:36] bit:24, dataset:CIFAR, training... loss:0.66551\n",
      "DPSH[139/150][10:39:36] bit:24, dataset:CIFAR, training... loss:0.66674\n",
      "DPSH[140/150][10:39:37] bit:24, dataset:CIFAR, training... loss:0.66615\n",
      "DPSH[141/150][10:39:37] bit:24, dataset:CIFAR, training... loss:0.66398\n",
      "DPSH[142/150][10:39:37] bit:24, dataset:CIFAR, training... loss:0.66554\n",
      "DPSH[143/150][10:39:37] bit:24, dataset:CIFAR, training... loss:0.66411\n",
      "DPSH[144/150][10:39:37] bit:24, dataset:CIFAR, training... loss:0.66304\n",
      "DPSH[145/150][10:39:38] bit:24, dataset:CIFAR, training... loss:0.66408\n",
      "DPSH[146/150][10:39:38] bit:24, dataset:CIFAR, training... loss:0.66348\n",
      "DPSH[147/150][10:39:38] bit:24, dataset:CIFAR, training... loss:0.66258\n",
      "DPSH[148/150][10:39:38] bit:24, dataset:CIFAR, training... loss:0.66366\n",
      "DPSH[149/150][10:39:38] bit:24, dataset:CIFAR, training... loss:0.66239\n",
      "DPSH[150/150][10:39:38] bit:24, dataset:CIFAR, training... loss:0.66167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 441.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mAP: 0.5282\n",
      "Testing combination: {'eta': 0.01, 'learning_rate': 1e-06, 'weight_decay': 1e-05}\n",
      "DPSH[ 1/150][10:39:40] bit:24, dataset:CIFAR, training... loss:0.75187\n",
      "DPSH[ 2/150][10:39:40] bit:24, dataset:CIFAR, training... loss:0.71573\n",
      "DPSH[ 3/150][10:39:40] bit:24, dataset:CIFAR, training... loss:0.72565\n",
      "DPSH[ 4/150][10:39:40] bit:24, dataset:CIFAR, training... loss:0.71405\n",
      "DPSH[ 5/150][10:39:41] bit:24, dataset:CIFAR, training... loss:0.71624\n",
      "DPSH[ 6/150][10:39:41] bit:24, dataset:CIFAR, training... loss:0.71246\n",
      "DPSH[ 7/150][10:39:41] bit:24, dataset:CIFAR, training... loss:0.71111\n",
      "DPSH[ 8/150][10:39:41] bit:24, dataset:CIFAR, training... loss:0.71095\n",
      "DPSH[ 9/150][10:39:41] bit:24, dataset:CIFAR, training... loss:0.70865\n",
      "DPSH[10/150][10:39:42] bit:24, dataset:CIFAR, training... loss:0.70890\n",
      "DPSH[11/150][10:39:42] bit:24, dataset:CIFAR, training... loss:0.70935\n",
      "DPSH[12/150][10:39:42] bit:24, dataset:CIFAR, training... loss:0.70966\n",
      "DPSH[13/150][10:39:42] bit:24, dataset:CIFAR, training... loss:0.71003\n",
      "DPSH[14/150][10:39:42] bit:24, dataset:CIFAR, training... loss:0.70984\n",
      "DPSH[15/150][10:39:42] bit:24, dataset:CIFAR, training... loss:0.71077\n",
      "DPSH[16/150][10:39:43] bit:24, dataset:CIFAR, training... loss:0.70860\n",
      "DPSH[17/150][10:39:43] bit:24, dataset:CIFAR, training... loss:0.70843\n",
      "DPSH[18/150][10:39:43] bit:24, dataset:CIFAR, training... loss:0.70665\n",
      "DPSH[19/150][10:39:43] bit:24, dataset:CIFAR, training... loss:0.70542\n",
      "DPSH[20/150][10:39:43] bit:24, dataset:CIFAR, training... loss:0.70371\n",
      "DPSH[21/150][10:39:43] bit:24, dataset:CIFAR, training... loss:0.70206\n",
      "DPSH[22/150][10:39:44] bit:24, dataset:CIFAR, training... loss:0.70162\n",
      "DPSH[23/150][10:39:44] bit:24, dataset:CIFAR, training... loss:0.70004\n",
      "DPSH[24/150][10:39:44] bit:24, dataset:CIFAR, training... loss:0.69843\n",
      "DPSH[25/150][10:39:44] bit:24, dataset:CIFAR, training... loss:0.69836\n",
      "DPSH[26/150][10:39:44] bit:24, dataset:CIFAR, training... loss:0.69812\n",
      "DPSH[27/150][10:39:45] bit:24, dataset:CIFAR, training... loss:0.69583\n",
      "DPSH[28/150][10:39:45] bit:24, dataset:CIFAR, training... loss:0.69570\n",
      "DPSH[29/150][10:39:45] bit:24, dataset:CIFAR, training... loss:0.69659\n",
      "DPSH[30/150][10:39:45] bit:24, dataset:CIFAR, training... loss:0.69525\n",
      "DPSH[31/150][10:39:45] bit:24, dataset:CIFAR, training... loss:0.69543\n",
      "DPSH[32/150][10:39:45] bit:24, dataset:CIFAR, training... loss:0.69471\n",
      "DPSH[33/150][10:39:46] bit:24, dataset:CIFAR, training... loss:0.69438\n",
      "DPSH[34/150][10:39:46] bit:24, dataset:CIFAR, training... loss:0.69587\n",
      "DPSH[35/150][10:39:46] bit:24, dataset:CIFAR, training... loss:0.69237\n",
      "DPSH[36/150][10:39:46] bit:24, dataset:CIFAR, training... loss:0.69313\n",
      "DPSH[37/150][10:39:46] bit:24, dataset:CIFAR, training... loss:0.69538\n",
      "DPSH[38/150][10:39:47] bit:24, dataset:CIFAR, training... loss:0.69161\n",
      "DPSH[39/150][10:39:47] bit:24, dataset:CIFAR, training... loss:0.69101\n",
      "DPSH[40/150][10:39:47] bit:24, dataset:CIFAR, training... loss:0.69288\n",
      "DPSH[41/150][10:39:47] bit:24, dataset:CIFAR, training... loss:0.69018\n",
      "DPSH[42/150][10:39:47] bit:24, dataset:CIFAR, training... loss:0.68862\n",
      "DPSH[43/150][10:39:47] bit:24, dataset:CIFAR, training... loss:0.69062\n",
      "DPSH[44/150][10:39:48] bit:24, dataset:CIFAR, training... loss:0.68800\n",
      "DPSH[45/150][10:39:48] bit:24, dataset:CIFAR, training... loss:0.68680\n",
      "DPSH[46/150][10:39:48] bit:24, dataset:CIFAR, training... loss:0.68818\n",
      "DPSH[47/150][10:39:48] bit:24, dataset:CIFAR, training... loss:0.68553\n",
      "DPSH[48/150][10:39:48] bit:24, dataset:CIFAR, training... loss:0.68575\n",
      "DPSH[49/150][10:39:49] bit:24, dataset:CIFAR, training... loss:0.68681\n",
      "DPSH[50/150][10:39:49] bit:24, dataset:CIFAR, training... loss:0.68352\n",
      "DPSH[51/150][10:39:49] bit:24, dataset:CIFAR, training... loss:0.68499\n",
      "DPSH[52/150][10:39:49] bit:24, dataset:CIFAR, training... loss:0.68588\n",
      "DPSH[53/150][10:39:49] bit:24, dataset:CIFAR, training... loss:0.68237\n",
      "DPSH[54/150][10:39:49] bit:24, dataset:CIFAR, training... loss:0.68322\n",
      "DPSH[55/150][10:39:50] bit:24, dataset:CIFAR, training... loss:0.68449\n",
      "DPSH[56/150][10:39:50] bit:24, dataset:CIFAR, training... loss:0.68247\n",
      "DPSH[57/150][10:39:50] bit:24, dataset:CIFAR, training... loss:0.68246\n",
      "DPSH[58/150][10:39:50] bit:24, dataset:CIFAR, training... loss:0.68358\n",
      "DPSH[59/150][10:39:50] bit:24, dataset:CIFAR, training... loss:0.68188\n",
      "DPSH[60/150][10:39:51] bit:24, dataset:CIFAR, training... loss:0.68158\n",
      "DPSH[61/150][10:39:51] bit:24, dataset:CIFAR, training... loss:0.68141\n",
      "DPSH[62/150][10:39:51] bit:24, dataset:CIFAR, training... loss:0.68221\n",
      "DPSH[63/150][10:39:51] bit:24, dataset:CIFAR, training... loss:0.68103\n",
      "DPSH[64/150][10:39:51] bit:24, dataset:CIFAR, training... loss:0.68136\n",
      "DPSH[65/150][10:39:51] bit:24, dataset:CIFAR, training... loss:0.68254\n",
      "DPSH[66/150][10:39:52] bit:24, dataset:CIFAR, training... loss:0.67975\n",
      "DPSH[67/150][10:39:52] bit:24, dataset:CIFAR, training... loss:0.67947\n",
      "DPSH[68/150][10:39:52] bit:24, dataset:CIFAR, training... loss:0.68104\n",
      "DPSH[69/150][10:39:52] bit:24, dataset:CIFAR, training... loss:0.67982\n",
      "DPSH[70/150][10:39:52] bit:24, dataset:CIFAR, training... loss:0.67957\n",
      "DPSH[71/150][10:39:53] bit:24, dataset:CIFAR, training... loss:0.67942\n",
      "DPSH[72/150][10:39:53] bit:24, dataset:CIFAR, training... loss:0.67941\n",
      "DPSH[73/150][10:39:53] bit:24, dataset:CIFAR, training... loss:0.68011\n",
      "DPSH[74/150][10:39:53] bit:24, dataset:CIFAR, training... loss:0.67904\n",
      "DPSH[75/150][10:39:53] bit:24, dataset:CIFAR, training... loss:0.67837\n",
      "DPSH[76/150][10:39:53] bit:24, dataset:CIFAR, training... loss:0.67851\n",
      "DPSH[77/150][10:39:54] bit:24, dataset:CIFAR, training... loss:0.67906\n",
      "DPSH[78/150][10:39:54] bit:24, dataset:CIFAR, training... loss:0.67784\n",
      "DPSH[79/150][10:39:54] bit:24, dataset:CIFAR, training... loss:0.67774\n",
      "DPSH[80/150][10:39:54] bit:24, dataset:CIFAR, training... loss:0.67924\n",
      "DPSH[81/150][10:39:54] bit:24, dataset:CIFAR, training... loss:0.67757\n",
      "DPSH[82/150][10:39:55] bit:24, dataset:CIFAR, training... loss:0.67655\n",
      "DPSH[83/150][10:39:55] bit:24, dataset:CIFAR, training... loss:0.67778\n",
      "DPSH[84/150][10:39:55] bit:24, dataset:CIFAR, training... loss:0.67601\n",
      "DPSH[85/150][10:39:55] bit:24, dataset:CIFAR, training... loss:0.67535\n",
      "DPSH[86/150][10:39:55] bit:24, dataset:CIFAR, training... loss:0.67712\n",
      "DPSH[87/150][10:39:55] bit:24, dataset:CIFAR, training... loss:0.67499\n",
      "DPSH[88/150][10:39:56] bit:24, dataset:CIFAR, training... loss:0.67493\n",
      "DPSH[89/150][10:39:56] bit:24, dataset:CIFAR, training... loss:0.67499\n",
      "DPSH[90/150][10:39:56] bit:24, dataset:CIFAR, training... loss:0.67466\n",
      "DPSH[91/150][10:39:56] bit:24, dataset:CIFAR, training... loss:0.67482\n",
      "DPSH[92/150][10:39:56] bit:24, dataset:CIFAR, training... loss:0.67463\n",
      "DPSH[93/150][10:39:56] bit:24, dataset:CIFAR, training... loss:0.67364\n",
      "DPSH[94/150][10:39:57] bit:24, dataset:CIFAR, training... loss:0.67390\n",
      "DPSH[95/150][10:39:57] bit:24, dataset:CIFAR, training... loss:0.67407\n",
      "DPSH[96/150][10:39:57] bit:24, dataset:CIFAR, training... loss:0.67322\n",
      "DPSH[97/150][10:39:57] bit:24, dataset:CIFAR, training... loss:0.67333\n",
      "DPSH[98/150][10:39:57] bit:24, dataset:CIFAR, training... loss:0.67267\n",
      "DPSH[99/150][10:39:58] bit:24, dataset:CIFAR, training... loss:0.67251\n",
      "DPSH[100/150][10:39:58] bit:24, dataset:CIFAR, training... loss:0.67268\n",
      "DPSH[101/150][10:39:58] bit:24, dataset:CIFAR, training... loss:0.67189\n",
      "DPSH[102/150][10:39:58] bit:24, dataset:CIFAR, training... loss:0.67148\n",
      "DPSH[103/150][10:39:58] bit:24, dataset:CIFAR, training... loss:0.67188\n",
      "DPSH[104/150][10:39:58] bit:24, dataset:CIFAR, training... loss:0.67047\n",
      "DPSH[105/150][10:39:59] bit:24, dataset:CIFAR, training... loss:0.67102\n",
      "DPSH[106/150][10:39:59] bit:24, dataset:CIFAR, training... loss:0.67161\n",
      "DPSH[107/150][10:39:59] bit:24, dataset:CIFAR, training... loss:0.66899\n",
      "DPSH[108/150][10:39:59] bit:24, dataset:CIFAR, training... loss:0.67039\n",
      "DPSH[109/150][10:39:59] bit:24, dataset:CIFAR, training... loss:0.67071\n",
      "DPSH[110/150][10:40:00] bit:24, dataset:CIFAR, training... loss:0.66870\n",
      "DPSH[111/150][10:40:00] bit:24, dataset:CIFAR, training... loss:0.66910\n",
      "DPSH[112/150][10:40:00] bit:24, dataset:CIFAR, training... loss:0.66929\n",
      "DPSH[113/150][10:40:00] bit:24, dataset:CIFAR, training... loss:0.66756\n",
      "DPSH[114/150][10:40:00] bit:24, dataset:CIFAR, training... loss:0.66840\n",
      "DPSH[115/150][10:40:00] bit:24, dataset:CIFAR, training... loss:0.66857\n",
      "DPSH[116/150][10:40:01] bit:24, dataset:CIFAR, training... loss:0.66641\n",
      "DPSH[117/150][10:40:01] bit:24, dataset:CIFAR, training... loss:0.66782\n",
      "DPSH[118/150][10:40:01] bit:24, dataset:CIFAR, training... loss:0.66775\n",
      "DPSH[119/150][10:40:01] bit:24, dataset:CIFAR, training... loss:0.66646\n",
      "DPSH[120/150][10:40:01] bit:24, dataset:CIFAR, training... loss:0.66733\n",
      "DPSH[121/150][10:40:02] bit:24, dataset:CIFAR, training... loss:0.66698\n",
      "DPSH[122/150][10:40:02] bit:24, dataset:CIFAR, training... loss:0.66516\n",
      "DPSH[123/150][10:40:02] bit:24, dataset:CIFAR, training... loss:0.66592\n",
      "DPSH[124/150][10:40:02] bit:24, dataset:CIFAR, training... loss:0.66745\n",
      "DPSH[125/150][10:40:02] bit:24, dataset:CIFAR, training... loss:0.66479\n",
      "DPSH[126/150][10:40:03] bit:24, dataset:CIFAR, training... loss:0.66497\n",
      "DPSH[127/150][10:40:03] bit:24, dataset:CIFAR, training... loss:0.66565\n",
      "DPSH[128/150][10:40:03] bit:24, dataset:CIFAR, training... loss:0.66574\n",
      "DPSH[129/150][10:40:03] bit:24, dataset:CIFAR, training... loss:0.66433\n",
      "DPSH[130/150][10:40:03] bit:24, dataset:CIFAR, training... loss:0.66549\n",
      "DPSH[131/150][10:40:03] bit:24, dataset:CIFAR, training... loss:0.66463\n",
      "DPSH[132/150][10:40:04] bit:24, dataset:CIFAR, training... loss:0.66401\n",
      "DPSH[133/150][10:40:04] bit:24, dataset:CIFAR, training... loss:0.66365\n",
      "DPSH[134/150][10:40:04] bit:24, dataset:CIFAR, training... loss:0.66411\n",
      "DPSH[135/150][10:40:04] bit:24, dataset:CIFAR, training... loss:0.66354\n",
      "DPSH[136/150][10:40:04] bit:24, dataset:CIFAR, training... loss:0.66357\n",
      "DPSH[137/150][10:40:05] bit:24, dataset:CIFAR, training... loss:0.66242\n",
      "DPSH[138/150][10:40:05] bit:24, dataset:CIFAR, training... loss:0.66278\n",
      "DPSH[139/150][10:40:05] bit:24, dataset:CIFAR, training... loss:0.66245\n",
      "DPSH[140/150][10:40:05] bit:24, dataset:CIFAR, training... loss:0.66136\n",
      "DPSH[141/150][10:40:05] bit:24, dataset:CIFAR, training... loss:0.66267\n",
      "DPSH[142/150][10:40:05] bit:24, dataset:CIFAR, training... loss:0.66269\n",
      "DPSH[143/150][10:40:06] bit:24, dataset:CIFAR, training... loss:0.66135\n",
      "DPSH[144/150][10:40:06] bit:24, dataset:CIFAR, training... loss:0.66162\n",
      "DPSH[145/150][10:40:06] bit:24, dataset:CIFAR, training... loss:0.66090\n",
      "DPSH[146/150][10:40:06] bit:24, dataset:CIFAR, training... loss:0.66081\n",
      "DPSH[147/150][10:40:06] bit:24, dataset:CIFAR, training... loss:0.66266\n",
      "DPSH[148/150][10:40:07] bit:24, dataset:CIFAR, training... loss:0.65952\n",
      "DPSH[149/150][10:40:07] bit:24, dataset:CIFAR, training... loss:0.66038\n",
      "DPSH[150/150][10:40:07] bit:24, dataset:CIFAR, training... loss:0.66181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 437.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mAP: 0.5367\n",
      "Testing combination: {'eta': 0.01, 'learning_rate': 1e-06, 'weight_decay': 0.0001}\n",
      "DPSH[ 1/150][10:40:08] bit:24, dataset:CIFAR, training... loss:0.74204\n",
      "DPSH[ 2/150][10:40:09] bit:24, dataset:CIFAR, training... loss:0.71709\n",
      "DPSH[ 3/150][10:40:09] bit:24, dataset:CIFAR, training... loss:0.72391\n",
      "DPSH[ 4/150][10:40:09] bit:24, dataset:CIFAR, training... loss:0.71573\n",
      "DPSH[ 5/150][10:40:09] bit:24, dataset:CIFAR, training... loss:0.71713\n",
      "DPSH[ 6/150][10:40:09] bit:24, dataset:CIFAR, training... loss:0.71432\n",
      "DPSH[ 7/150][10:40:10] bit:24, dataset:CIFAR, training... loss:0.71324\n",
      "DPSH[ 8/150][10:40:10] bit:24, dataset:CIFAR, training... loss:0.71294\n",
      "DPSH[ 9/150][10:40:10] bit:24, dataset:CIFAR, training... loss:0.71059\n",
      "DPSH[10/150][10:40:10] bit:24, dataset:CIFAR, training... loss:0.71107\n",
      "DPSH[11/150][10:40:10] bit:24, dataset:CIFAR, training... loss:0.70961\n",
      "DPSH[12/150][10:40:10] bit:24, dataset:CIFAR, training... loss:0.71073\n",
      "DPSH[13/150][10:40:11] bit:24, dataset:CIFAR, training... loss:0.70954\n",
      "DPSH[14/150][10:40:11] bit:24, dataset:CIFAR, training... loss:0.70759\n",
      "DPSH[15/150][10:40:11] bit:24, dataset:CIFAR, training... loss:0.70992\n",
      "DPSH[16/150][10:40:11] bit:24, dataset:CIFAR, training... loss:0.70671\n",
      "DPSH[17/150][10:40:11] bit:24, dataset:CIFAR, training... loss:0.70651\n",
      "DPSH[18/150][10:40:12] bit:24, dataset:CIFAR, training... loss:0.70625\n",
      "DPSH[19/150][10:40:12] bit:24, dataset:CIFAR, training... loss:0.70440\n",
      "DPSH[20/150][10:40:12] bit:24, dataset:CIFAR, training... loss:0.70415\n",
      "DPSH[21/150][10:40:12] bit:24, dataset:CIFAR, training... loss:0.70206\n",
      "DPSH[22/150][10:40:12] bit:24, dataset:CIFAR, training... loss:0.70103\n",
      "DPSH[23/150][10:40:13] bit:24, dataset:CIFAR, training... loss:0.70095\n",
      "DPSH[24/150][10:40:13] bit:24, dataset:CIFAR, training... loss:0.70014\n",
      "DPSH[25/150][10:40:13] bit:24, dataset:CIFAR, training... loss:0.69900\n",
      "DPSH[26/150][10:40:13] bit:24, dataset:CIFAR, training... loss:0.69889\n",
      "DPSH[27/150][10:40:13] bit:24, dataset:CIFAR, training... loss:0.69897\n",
      "DPSH[28/150][10:40:14] bit:24, dataset:CIFAR, training... loss:0.69788\n",
      "DPSH[29/150][10:40:14] bit:24, dataset:CIFAR, training... loss:0.69666\n",
      "DPSH[30/150][10:40:14] bit:24, dataset:CIFAR, training... loss:0.69699\n",
      "DPSH[31/150][10:40:14] bit:24, dataset:CIFAR, training... loss:0.69656\n",
      "DPSH[32/150][10:40:14] bit:24, dataset:CIFAR, training... loss:0.69414\n",
      "DPSH[33/150][10:40:15] bit:24, dataset:CIFAR, training... loss:0.69492\n",
      "DPSH[34/150][10:40:15] bit:24, dataset:CIFAR, training... loss:0.69464\n",
      "DPSH[35/150][10:40:15] bit:24, dataset:CIFAR, training... loss:0.69180\n",
      "DPSH[36/150][10:40:15] bit:24, dataset:CIFAR, training... loss:0.69327\n",
      "DPSH[37/150][10:40:15] bit:24, dataset:CIFAR, training... loss:0.69239\n",
      "DPSH[38/150][10:40:15] bit:24, dataset:CIFAR, training... loss:0.69001\n",
      "DPSH[39/150][10:40:16] bit:24, dataset:CIFAR, training... loss:0.69073\n",
      "DPSH[40/150][10:40:16] bit:24, dataset:CIFAR, training... loss:0.69004\n",
      "DPSH[41/150][10:40:16] bit:24, dataset:CIFAR, training... loss:0.68919\n",
      "DPSH[42/150][10:40:16] bit:24, dataset:CIFAR, training... loss:0.68922\n",
      "DPSH[43/150][10:40:16] bit:24, dataset:CIFAR, training... loss:0.68990\n",
      "DPSH[44/150][10:40:16] bit:24, dataset:CIFAR, training... loss:0.68835\n",
      "DPSH[45/150][10:40:17] bit:24, dataset:CIFAR, training... loss:0.68734\n",
      "DPSH[46/150][10:40:17] bit:24, dataset:CIFAR, training... loss:0.68903\n",
      "DPSH[47/150][10:40:17] bit:24, dataset:CIFAR, training... loss:0.68774\n",
      "DPSH[48/150][10:40:17] bit:24, dataset:CIFAR, training... loss:0.68707\n",
      "DPSH[49/150][10:40:17] bit:24, dataset:CIFAR, training... loss:0.68755\n",
      "DPSH[50/150][10:40:18] bit:24, dataset:CIFAR, training... loss:0.68629\n",
      "DPSH[51/150][10:40:18] bit:24, dataset:CIFAR, training... loss:0.68707\n",
      "DPSH[52/150][10:40:18] bit:24, dataset:CIFAR, training... loss:0.68620\n",
      "DPSH[53/150][10:40:18] bit:24, dataset:CIFAR, training... loss:0.68535\n",
      "DPSH[54/150][10:40:18] bit:24, dataset:CIFAR, training... loss:0.68737\n",
      "DPSH[55/150][10:40:19] bit:24, dataset:CIFAR, training... loss:0.68475\n",
      "DPSH[56/150][10:40:19] bit:24, dataset:CIFAR, training... loss:0.68492\n",
      "DPSH[57/150][10:40:19] bit:24, dataset:CIFAR, training... loss:0.68541\n",
      "DPSH[58/150][10:40:19] bit:24, dataset:CIFAR, training... loss:0.68394\n",
      "DPSH[59/150][10:40:19] bit:24, dataset:CIFAR, training... loss:0.68458\n",
      "DPSH[60/150][10:40:20] bit:24, dataset:CIFAR, training... loss:0.68309\n",
      "DPSH[61/150][10:40:20] bit:24, dataset:CIFAR, training... loss:0.68392\n",
      "DPSH[62/150][10:40:20] bit:24, dataset:CIFAR, training... loss:0.68311\n",
      "DPSH[63/150][10:40:20] bit:24, dataset:CIFAR, training... loss:0.68172\n",
      "DPSH[64/150][10:40:20] bit:24, dataset:CIFAR, training... loss:0.68272\n",
      "DPSH[65/150][10:40:21] bit:24, dataset:CIFAR, training... loss:0.68215\n",
      "DPSH[66/150][10:40:21] bit:24, dataset:CIFAR, training... loss:0.68171\n",
      "DPSH[67/150][10:40:21] bit:24, dataset:CIFAR, training... loss:0.68193\n",
      "DPSH[68/150][10:40:21] bit:24, dataset:CIFAR, training... loss:0.67952\n",
      "DPSH[69/150][10:40:21] bit:24, dataset:CIFAR, training... loss:0.68101\n",
      "DPSH[70/150][10:40:22] bit:24, dataset:CIFAR, training... loss:0.68176\n",
      "DPSH[71/150][10:40:22] bit:24, dataset:CIFAR, training... loss:0.67958\n",
      "DPSH[72/150][10:40:22] bit:24, dataset:CIFAR, training... loss:0.68081\n",
      "DPSH[73/150][10:40:22] bit:24, dataset:CIFAR, training... loss:0.68049\n",
      "DPSH[74/150][10:40:22] bit:24, dataset:CIFAR, training... loss:0.67921\n",
      "DPSH[75/150][10:40:23] bit:24, dataset:CIFAR, training... loss:0.67924\n",
      "DPSH[76/150][10:40:23] bit:24, dataset:CIFAR, training... loss:0.67987\n",
      "DPSH[77/150][10:40:23] bit:24, dataset:CIFAR, training... loss:0.67869\n",
      "DPSH[78/150][10:40:23] bit:24, dataset:CIFAR, training... loss:0.67803\n",
      "DPSH[79/150][10:40:23] bit:24, dataset:CIFAR, training... loss:0.68008\n",
      "DPSH[80/150][10:40:24] bit:24, dataset:CIFAR, training... loss:0.67891\n",
      "DPSH[81/150][10:40:24] bit:24, dataset:CIFAR, training... loss:0.67664\n",
      "DPSH[82/150][10:40:24] bit:24, dataset:CIFAR, training... loss:0.67915\n",
      "DPSH[83/150][10:40:24] bit:24, dataset:CIFAR, training... loss:0.67755\n",
      "DPSH[84/150][10:40:24] bit:24, dataset:CIFAR, training... loss:0.67584\n",
      "DPSH[85/150][10:40:24] bit:24, dataset:CIFAR, training... loss:0.67742\n",
      "DPSH[86/150][10:40:25] bit:24, dataset:CIFAR, training... loss:0.67760\n",
      "DPSH[87/150][10:40:25] bit:24, dataset:CIFAR, training... loss:0.67531\n",
      "DPSH[88/150][10:40:25] bit:24, dataset:CIFAR, training... loss:0.67744\n",
      "DPSH[89/150][10:40:25] bit:24, dataset:CIFAR, training... loss:0.67632\n",
      "DPSH[90/150][10:40:25] bit:24, dataset:CIFAR, training... loss:0.67585\n",
      "DPSH[91/150][10:40:26] bit:24, dataset:CIFAR, training... loss:0.67683\n",
      "DPSH[92/150][10:40:26] bit:24, dataset:CIFAR, training... loss:0.67551\n",
      "DPSH[93/150][10:40:26] bit:24, dataset:CIFAR, training... loss:0.67641\n",
      "DPSH[94/150][10:40:26] bit:24, dataset:CIFAR, training... loss:0.67623\n",
      "DPSH[95/150][10:40:26] bit:24, dataset:CIFAR, training... loss:0.67432\n",
      "DPSH[96/150][10:40:27] bit:24, dataset:CIFAR, training... loss:0.67561\n",
      "DPSH[97/150][10:40:27] bit:24, dataset:CIFAR, training... loss:0.67560\n",
      "DPSH[98/150][10:40:27] bit:24, dataset:CIFAR, training... loss:0.67451\n",
      "DPSH[99/150][10:40:27] bit:24, dataset:CIFAR, training... loss:0.67422\n",
      "DPSH[100/150][10:40:27] bit:24, dataset:CIFAR, training... loss:0.67364\n",
      "DPSH[101/150][10:40:28] bit:24, dataset:CIFAR, training... loss:0.67408\n",
      "DPSH[102/150][10:40:28] bit:24, dataset:CIFAR, training... loss:0.67298\n",
      "DPSH[103/150][10:40:28] bit:24, dataset:CIFAR, training... loss:0.67271\n",
      "DPSH[104/150][10:40:28] bit:24, dataset:CIFAR, training... loss:0.67389\n",
      "DPSH[105/150][10:40:28] bit:24, dataset:CIFAR, training... loss:0.67132\n",
      "DPSH[106/150][10:40:28] bit:24, dataset:CIFAR, training... loss:0.67173\n",
      "DPSH[107/150][10:40:29] bit:24, dataset:CIFAR, training... loss:0.67284\n",
      "DPSH[108/150][10:40:29] bit:24, dataset:CIFAR, training... loss:0.67054\n",
      "DPSH[109/150][10:40:29] bit:24, dataset:CIFAR, training... loss:0.67318\n",
      "DPSH[110/150][10:40:29] bit:24, dataset:CIFAR, training... loss:0.67265\n",
      "DPSH[111/150][10:40:29] bit:24, dataset:CIFAR, training... loss:0.67007\n",
      "DPSH[112/150][10:40:30] bit:24, dataset:CIFAR, training... loss:0.67305\n",
      "DPSH[113/150][10:40:30] bit:24, dataset:CIFAR, training... loss:0.67288\n",
      "DPSH[114/150][10:40:30] bit:24, dataset:CIFAR, training... loss:0.67073\n",
      "DPSH[115/150][10:40:30] bit:24, dataset:CIFAR, training... loss:0.67138\n",
      "DPSH[116/150][10:40:30] bit:24, dataset:CIFAR, training... loss:0.67136\n",
      "DPSH[117/150][10:40:30] bit:24, dataset:CIFAR, training... loss:0.67138\n",
      "DPSH[118/150][10:40:31] bit:24, dataset:CIFAR, training... loss:0.67047\n",
      "DPSH[119/150][10:40:31] bit:24, dataset:CIFAR, training... loss:0.66941\n",
      "DPSH[120/150][10:40:31] bit:24, dataset:CIFAR, training... loss:0.67059\n",
      "DPSH[121/150][10:40:31] bit:24, dataset:CIFAR, training... loss:0.66952\n",
      "DPSH[122/150][10:40:31] bit:24, dataset:CIFAR, training... loss:0.66737\n",
      "DPSH[123/150][10:40:32] bit:24, dataset:CIFAR, training... loss:0.67007\n",
      "DPSH[124/150][10:40:32] bit:24, dataset:CIFAR, training... loss:0.66836\n",
      "DPSH[125/150][10:40:32] bit:24, dataset:CIFAR, training... loss:0.66751\n",
      "DPSH[126/150][10:40:32] bit:24, dataset:CIFAR, training... loss:0.66930\n",
      "DPSH[127/150][10:40:32] bit:24, dataset:CIFAR, training... loss:0.66692\n",
      "DPSH[128/150][10:40:32] bit:24, dataset:CIFAR, training... loss:0.66609\n",
      "DPSH[129/150][10:40:33] bit:24, dataset:CIFAR, training... loss:0.66782\n",
      "DPSH[130/150][10:40:33] bit:24, dataset:CIFAR, training... loss:0.66689\n",
      "DPSH[131/150][10:40:33] bit:24, dataset:CIFAR, training... loss:0.66538\n",
      "DPSH[132/150][10:40:33] bit:24, dataset:CIFAR, training... loss:0.66645\n",
      "DPSH[133/150][10:40:33] bit:24, dataset:CIFAR, training... loss:0.66612\n",
      "DPSH[134/150][10:40:34] bit:24, dataset:CIFAR, training... loss:0.66497\n",
      "DPSH[135/150][10:40:34] bit:24, dataset:CIFAR, training... loss:0.66632\n",
      "DPSH[136/150][10:40:34] bit:24, dataset:CIFAR, training... loss:0.66557\n",
      "DPSH[137/150][10:40:34] bit:24, dataset:CIFAR, training... loss:0.66510\n",
      "DPSH[138/150][10:40:34] bit:24, dataset:CIFAR, training... loss:0.66539\n",
      "DPSH[139/150][10:40:35] bit:24, dataset:CIFAR, training... loss:0.66494\n",
      "DPSH[140/150][10:40:35] bit:24, dataset:CIFAR, training... loss:0.66337\n",
      "DPSH[141/150][10:40:35] bit:24, dataset:CIFAR, training... loss:0.66447\n",
      "DPSH[142/150][10:40:35] bit:24, dataset:CIFAR, training... loss:0.66421\n",
      "DPSH[143/150][10:40:35] bit:24, dataset:CIFAR, training... loss:0.66238\n",
      "DPSH[144/150][10:40:35] bit:24, dataset:CIFAR, training... loss:0.66347\n",
      "DPSH[145/150][10:40:36] bit:24, dataset:CIFAR, training... loss:0.66361\n",
      "DPSH[146/150][10:40:36] bit:24, dataset:CIFAR, training... loss:0.66174\n",
      "DPSH[147/150][10:40:36] bit:24, dataset:CIFAR, training... loss:0.66335\n",
      "DPSH[148/150][10:40:36] bit:24, dataset:CIFAR, training... loss:0.66307\n",
      "DPSH[149/150][10:40:36] bit:24, dataset:CIFAR, training... loss:0.66232\n",
      "DPSH[150/150][10:40:37] bit:24, dataset:CIFAR, training... loss:0.66178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 438.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mAP: 0.5159\n",
      "Testing combination: {'eta': 0.01, 'learning_rate': 1e-05, 'weight_decay': 1e-06}\n",
      "DPSH[ 1/150][10:40:38] bit:24, dataset:CIFAR, training... loss:0.73307\n",
      "DPSH[ 2/150][10:40:38] bit:24, dataset:CIFAR, training... loss:0.76372\n",
      "DPSH[ 3/150][10:40:38] bit:24, dataset:CIFAR, training... loss:0.75678\n",
      "DPSH[ 4/150][10:40:39] bit:24, dataset:CIFAR, training... loss:0.76779\n",
      "DPSH[ 5/150][10:40:39] bit:24, dataset:CIFAR, training... loss:0.75720\n",
      "DPSH[ 6/150][10:40:39] bit:24, dataset:CIFAR, training... loss:0.74789\n",
      "DPSH[ 7/150][10:40:39] bit:24, dataset:CIFAR, training... loss:0.75822\n",
      "DPSH[ 8/150][10:40:39] bit:24, dataset:CIFAR, training... loss:0.73627\n",
      "DPSH[ 9/150][10:40:40] bit:24, dataset:CIFAR, training... loss:0.73116\n",
      "DPSH[10/150][10:40:40] bit:24, dataset:CIFAR, training... loss:0.72762\n",
      "DPSH[11/150][10:40:40] bit:24, dataset:CIFAR, training... loss:0.71367\n",
      "DPSH[12/150][10:40:40] bit:24, dataset:CIFAR, training... loss:0.70962\n",
      "DPSH[13/150][10:40:40] bit:24, dataset:CIFAR, training... loss:0.70692\n",
      "DPSH[14/150][10:40:41] bit:24, dataset:CIFAR, training... loss:0.70085\n",
      "DPSH[15/150][10:40:41] bit:24, dataset:CIFAR, training... loss:0.69955\n",
      "DPSH[16/150][10:40:41] bit:24, dataset:CIFAR, training... loss:0.69733\n",
      "DPSH[17/150][10:40:41] bit:24, dataset:CIFAR, training... loss:0.69446\n",
      "DPSH[18/150][10:40:41] bit:24, dataset:CIFAR, training... loss:0.69347\n",
      "DPSH[19/150][10:40:42] bit:24, dataset:CIFAR, training... loss:0.69464\n",
      "DPSH[20/150][10:40:42] bit:24, dataset:CIFAR, training... loss:0.69782\n",
      "DPSH[21/150][10:40:42] bit:24, dataset:CIFAR, training... loss:0.70212\n",
      "DPSH[22/150][10:40:42] bit:24, dataset:CIFAR, training... loss:0.70673\n",
      "DPSH[23/150][10:40:42] bit:24, dataset:CIFAR, training... loss:0.69966\n",
      "DPSH[24/150][10:40:43] bit:24, dataset:CIFAR, training... loss:0.69848\n",
      "DPSH[25/150][10:40:43] bit:24, dataset:CIFAR, training... loss:0.69199\n",
      "DPSH[26/150][10:40:43] bit:24, dataset:CIFAR, training... loss:0.68814\n",
      "DPSH[27/150][10:40:43] bit:24, dataset:CIFAR, training... loss:0.68566\n",
      "DPSH[28/150][10:40:43] bit:24, dataset:CIFAR, training... loss:0.68424\n",
      "DPSH[29/150][10:40:44] bit:24, dataset:CIFAR, training... loss:0.68323\n",
      "DPSH[30/150][10:40:44] bit:24, dataset:CIFAR, training... loss:0.68167\n",
      "DPSH[31/150][10:40:44] bit:24, dataset:CIFAR, training... loss:0.68167\n",
      "DPSH[32/150][10:40:44] bit:24, dataset:CIFAR, training... loss:0.68114\n",
      "DPSH[33/150][10:40:44] bit:24, dataset:CIFAR, training... loss:0.68129\n",
      "DPSH[34/150][10:40:45] bit:24, dataset:CIFAR, training... loss:0.68389\n",
      "DPSH[35/150][10:40:45] bit:24, dataset:CIFAR, training... loss:0.68500\n",
      "DPSH[36/150][10:40:45] bit:24, dataset:CIFAR, training... loss:0.68566\n",
      "DPSH[37/150][10:40:45] bit:24, dataset:CIFAR, training... loss:0.69004\n",
      "DPSH[38/150][10:40:45] bit:24, dataset:CIFAR, training... loss:0.68598\n",
      "DPSH[39/150][10:40:46] bit:24, dataset:CIFAR, training... loss:0.68612\n",
      "DPSH[40/150][10:40:46] bit:24, dataset:CIFAR, training... loss:0.68008\n",
      "DPSH[41/150][10:40:46] bit:24, dataset:CIFAR, training... loss:0.68007\n",
      "DPSH[42/150][10:40:46] bit:24, dataset:CIFAR, training... loss:0.67708\n",
      "DPSH[43/150][10:40:47] bit:24, dataset:CIFAR, training... loss:0.67524\n",
      "DPSH[44/150][10:40:47] bit:24, dataset:CIFAR, training... loss:0.67366\n",
      "DPSH[45/150][10:40:47] bit:24, dataset:CIFAR, training... loss:0.67377\n",
      "DPSH[46/150][10:40:47] bit:24, dataset:CIFAR, training... loss:0.67102\n",
      "DPSH[47/150][10:40:47] bit:24, dataset:CIFAR, training... loss:0.67159\n",
      "DPSH[48/150][10:40:48] bit:24, dataset:CIFAR, training... loss:0.66985\n",
      "DPSH[49/150][10:40:48] bit:24, dataset:CIFAR, training... loss:0.66933\n",
      "DPSH[50/150][10:40:48] bit:24, dataset:CIFAR, training... loss:0.66881\n",
      "DPSH[51/150][10:40:48] bit:24, dataset:CIFAR, training... loss:0.66725\n",
      "DPSH[52/150][10:40:48] bit:24, dataset:CIFAR, training... loss:0.66807\n",
      "DPSH[53/150][10:40:49] bit:24, dataset:CIFAR, training... loss:0.66798\n",
      "DPSH[54/150][10:40:49] bit:24, dataset:CIFAR, training... loss:0.66560\n",
      "DPSH[55/150][10:40:49] bit:24, dataset:CIFAR, training... loss:0.66476\n",
      "DPSH[56/150][10:40:49] bit:24, dataset:CIFAR, training... loss:0.66405\n",
      "DPSH[57/150][10:40:49] bit:24, dataset:CIFAR, training... loss:0.66239\n",
      "DPSH[58/150][10:40:50] bit:24, dataset:CIFAR, training... loss:0.66167\n",
      "DPSH[59/150][10:40:50] bit:24, dataset:CIFAR, training... loss:0.66098\n",
      "DPSH[60/150][10:40:50] bit:24, dataset:CIFAR, training... loss:0.66058\n",
      "DPSH[61/150][10:40:50] bit:24, dataset:CIFAR, training... loss:0.66088\n",
      "DPSH[62/150][10:40:50] bit:24, dataset:CIFAR, training... loss:0.66424\n",
      "DPSH[63/150][10:40:51] bit:24, dataset:CIFAR, training... loss:0.66580\n",
      "DPSH[64/150][10:40:51] bit:24, dataset:CIFAR, training... loss:0.66585\n",
      "DPSH[65/150][10:40:51] bit:24, dataset:CIFAR, training... loss:0.66540\n",
      "DPSH[66/150][10:40:51] bit:24, dataset:CIFAR, training... loss:0.66470\n",
      "DPSH[67/150][10:40:51] bit:24, dataset:CIFAR, training... loss:0.66443\n",
      "DPSH[68/150][10:40:51] bit:24, dataset:CIFAR, training... loss:0.66243\n",
      "DPSH[69/150][10:40:52] bit:24, dataset:CIFAR, training... loss:0.66250\n",
      "DPSH[70/150][10:40:52] bit:24, dataset:CIFAR, training... loss:0.65726\n",
      "DPSH[71/150][10:40:52] bit:24, dataset:CIFAR, training... loss:0.65609\n",
      "DPSH[72/150][10:40:52] bit:24, dataset:CIFAR, training... loss:0.65255\n",
      "DPSH[73/150][10:40:52] bit:24, dataset:CIFAR, training... loss:0.65291\n",
      "DPSH[74/150][10:40:53] bit:24, dataset:CIFAR, training... loss:0.65105\n",
      "DPSH[75/150][10:40:53] bit:24, dataset:CIFAR, training... loss:0.65139\n",
      "DPSH[76/150][10:40:53] bit:24, dataset:CIFAR, training... loss:0.64902\n",
      "DPSH[77/150][10:40:53] bit:24, dataset:CIFAR, training... loss:0.64668\n",
      "DPSH[78/150][10:40:53] bit:24, dataset:CIFAR, training... loss:0.64542\n",
      "DPSH[79/150][10:40:54] bit:24, dataset:CIFAR, training... loss:0.64574\n",
      "DPSH[80/150][10:40:54] bit:24, dataset:CIFAR, training... loss:0.64059\n",
      "DPSH[81/150][10:40:54] bit:24, dataset:CIFAR, training... loss:0.63854\n",
      "DPSH[82/150][10:40:54] bit:24, dataset:CIFAR, training... loss:0.63443\n",
      "DPSH[83/150][10:40:54] bit:24, dataset:CIFAR, training... loss:0.63442\n",
      "DPSH[84/150][10:40:55] bit:24, dataset:CIFAR, training... loss:0.63228\n",
      "DPSH[85/150][10:40:55] bit:24, dataset:CIFAR, training... loss:0.62853\n",
      "DPSH[86/150][10:40:55] bit:24, dataset:CIFAR, training... loss:0.63026\n",
      "DPSH[87/150][10:40:55] bit:24, dataset:CIFAR, training... loss:0.62719\n",
      "DPSH[88/150][10:40:55] bit:24, dataset:CIFAR, training... loss:0.61928\n",
      "DPSH[89/150][10:40:56] bit:24, dataset:CIFAR, training... loss:0.61964\n",
      "DPSH[90/150][10:40:56] bit:24, dataset:CIFAR, training... loss:0.61679\n",
      "DPSH[91/150][10:40:56] bit:24, dataset:CIFAR, training... loss:0.61609\n",
      "DPSH[92/150][10:40:56] bit:24, dataset:CIFAR, training... loss:0.61485\n",
      "DPSH[93/150][10:40:56] bit:24, dataset:CIFAR, training... loss:0.61152\n",
      "DPSH[94/150][10:40:57] bit:24, dataset:CIFAR, training... loss:0.60984\n",
      "DPSH[95/150][10:40:57] bit:24, dataset:CIFAR, training... loss:0.60858\n",
      "DPSH[96/150][10:40:57] bit:24, dataset:CIFAR, training... loss:0.60210\n",
      "DPSH[97/150][10:40:57] bit:24, dataset:CIFAR, training... loss:0.60639\n",
      "DPSH[98/150][10:40:57] bit:24, dataset:CIFAR, training... loss:0.60429\n",
      "DPSH[99/150][10:40:58] bit:24, dataset:CIFAR, training... loss:0.59928\n",
      "DPSH[100/150][10:40:58] bit:24, dataset:CIFAR, training... loss:0.59976\n",
      "DPSH[101/150][10:40:58] bit:24, dataset:CIFAR, training... loss:0.59585\n",
      "DPSH[102/150][10:40:58] bit:24, dataset:CIFAR, training... loss:0.58965\n",
      "DPSH[103/150][10:40:58] bit:24, dataset:CIFAR, training... loss:0.58822\n",
      "DPSH[104/150][10:40:58] bit:24, dataset:CIFAR, training... loss:0.58957\n",
      "DPSH[105/150][10:40:59] bit:24, dataset:CIFAR, training... loss:0.58908\n",
      "DPSH[106/150][10:40:59] bit:24, dataset:CIFAR, training... loss:0.58529\n",
      "DPSH[107/150][10:40:59] bit:24, dataset:CIFAR, training... loss:0.58547\n",
      "DPSH[108/150][10:40:59] bit:24, dataset:CIFAR, training... loss:0.58406\n",
      "DPSH[109/150][10:40:59] bit:24, dataset:CIFAR, training... loss:0.58220\n",
      "DPSH[110/150][10:41:00] bit:24, dataset:CIFAR, training... loss:0.57730\n",
      "DPSH[111/150][10:41:00] bit:24, dataset:CIFAR, training... loss:0.57939\n",
      "DPSH[112/150][10:41:00] bit:24, dataset:CIFAR, training... loss:0.57384\n",
      "DPSH[113/150][10:41:00] bit:24, dataset:CIFAR, training... loss:0.57484\n",
      "DPSH[114/150][10:41:00] bit:24, dataset:CIFAR, training... loss:0.57268\n",
      "DPSH[115/150][10:41:01] bit:24, dataset:CIFAR, training... loss:0.57383\n",
      "DPSH[116/150][10:41:01] bit:24, dataset:CIFAR, training... loss:0.57450\n",
      "DPSH[117/150][10:41:01] bit:24, dataset:CIFAR, training... loss:0.56844\n",
      "DPSH[118/150][10:41:01] bit:24, dataset:CIFAR, training... loss:0.56741\n",
      "DPSH[119/150][10:41:01] bit:24, dataset:CIFAR, training... loss:0.56728\n",
      "DPSH[120/150][10:41:01] bit:24, dataset:CIFAR, training... loss:0.56216\n",
      "DPSH[121/150][10:41:02] bit:24, dataset:CIFAR, training... loss:0.55588\n",
      "DPSH[122/150][10:41:02] bit:24, dataset:CIFAR, training... loss:0.55521\n",
      "DPSH[123/150][10:41:02] bit:24, dataset:CIFAR, training... loss:0.54928\n",
      "DPSH[124/150][10:41:02] bit:24, dataset:CIFAR, training... loss:0.54924\n",
      "DPSH[125/150][10:41:02] bit:24, dataset:CIFAR, training... loss:0.54505\n",
      "DPSH[126/150][10:41:03] bit:24, dataset:CIFAR, training... loss:0.54023\n",
      "DPSH[127/150][10:41:03] bit:24, dataset:CIFAR, training... loss:0.54143\n",
      "DPSH[128/150][10:41:03] bit:24, dataset:CIFAR, training... loss:0.54101\n",
      "DPSH[129/150][10:41:03] bit:24, dataset:CIFAR, training... loss:0.54379\n",
      "DPSH[130/150][10:41:03] bit:24, dataset:CIFAR, training... loss:0.54587\n",
      "DPSH[131/150][10:41:04] bit:24, dataset:CIFAR, training... loss:0.54179\n",
      "DPSH[132/150][10:41:04] bit:24, dataset:CIFAR, training... loss:0.54191\n",
      "DPSH[133/150][10:41:04] bit:24, dataset:CIFAR, training... loss:0.53761\n",
      "DPSH[134/150][10:41:04] bit:24, dataset:CIFAR, training... loss:0.53323\n",
      "DPSH[135/150][10:41:04] bit:24, dataset:CIFAR, training... loss:0.53077\n",
      "DPSH[136/150][10:41:05] bit:24, dataset:CIFAR, training... loss:0.52617\n",
      "DPSH[137/150][10:41:05] bit:24, dataset:CIFAR, training... loss:0.52344\n",
      "DPSH[138/150][10:41:05] bit:24, dataset:CIFAR, training... loss:0.52325\n",
      "DPSH[139/150][10:41:05] bit:24, dataset:CIFAR, training... loss:0.52365\n",
      "DPSH[140/150][10:41:05] bit:24, dataset:CIFAR, training... loss:0.52074\n",
      "DPSH[141/150][10:41:06] bit:24, dataset:CIFAR, training... loss:0.52323\n",
      "DPSH[142/150][10:41:06] bit:24, dataset:CIFAR, training... loss:0.52464\n",
      "DPSH[143/150][10:41:06] bit:24, dataset:CIFAR, training... loss:0.52447\n",
      "DPSH[144/150][10:41:06] bit:24, dataset:CIFAR, training... loss:0.52326\n",
      "DPSH[145/150][10:41:06] bit:24, dataset:CIFAR, training... loss:0.52049\n",
      "DPSH[146/150][10:41:07] bit:24, dataset:CIFAR, training... loss:0.51856\n",
      "DPSH[147/150][10:41:07] bit:24, dataset:CIFAR, training... loss:0.51298\n",
      "DPSH[148/150][10:41:07] bit:24, dataset:CIFAR, training... loss:0.50746\n",
      "DPSH[149/150][10:41:07] bit:24, dataset:CIFAR, training... loss:0.50691\n",
      "DPSH[150/150][10:41:07] bit:24, dataset:CIFAR, training... loss:0.50354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 435.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mAP: 0.6807\n",
      "Testing combination: {'eta': 0.01, 'learning_rate': 1e-05, 'weight_decay': 1e-05}\n",
      "DPSH[ 1/150][10:41:09] bit:24, dataset:CIFAR, training... loss:0.72940\n",
      "DPSH[ 2/150][10:41:09] bit:24, dataset:CIFAR, training... loss:0.75028\n",
      "DPSH[ 3/150][10:41:09] bit:24, dataset:CIFAR, training... loss:0.76930\n",
      "DPSH[ 4/150][10:41:09] bit:24, dataset:CIFAR, training... loss:0.77383\n",
      "DPSH[ 5/150][10:41:10] bit:24, dataset:CIFAR, training... loss:0.74705\n",
      "DPSH[ 6/150][10:41:10] bit:24, dataset:CIFAR, training... loss:0.75567\n",
      "DPSH[ 7/150][10:41:10] bit:24, dataset:CIFAR, training... loss:0.74269\n",
      "DPSH[ 8/150][10:41:10] bit:24, dataset:CIFAR, training... loss:0.72964\n",
      "DPSH[ 9/150][10:41:10] bit:24, dataset:CIFAR, training... loss:0.72384\n",
      "DPSH[10/150][10:41:11] bit:24, dataset:CIFAR, training... loss:0.72123\n",
      "DPSH[11/150][10:41:11] bit:24, dataset:CIFAR, training... loss:0.71317\n",
      "DPSH[12/150][10:41:11] bit:24, dataset:CIFAR, training... loss:0.71528\n",
      "DPSH[13/150][10:41:11] bit:24, dataset:CIFAR, training... loss:0.71129\n",
      "DPSH[14/150][10:41:11] bit:24, dataset:CIFAR, training... loss:0.70893\n",
      "DPSH[15/150][10:41:12] bit:24, dataset:CIFAR, training... loss:0.71002\n",
      "DPSH[16/150][10:41:12] bit:24, dataset:CIFAR, training... loss:0.70634\n",
      "DPSH[17/150][10:41:12] bit:24, dataset:CIFAR, training... loss:0.70412\n",
      "DPSH[18/150][10:41:12] bit:24, dataset:CIFAR, training... loss:0.70142\n",
      "DPSH[19/150][10:41:12] bit:24, dataset:CIFAR, training... loss:0.69984\n",
      "DPSH[20/150][10:41:13] bit:24, dataset:CIFAR, training... loss:0.69989\n",
      "DPSH[21/150][10:41:13] bit:24, dataset:CIFAR, training... loss:0.69423\n",
      "DPSH[22/150][10:41:13] bit:24, dataset:CIFAR, training... loss:0.69404\n",
      "DPSH[23/150][10:41:13] bit:24, dataset:CIFAR, training... loss:0.69279\n",
      "DPSH[24/150][10:41:13] bit:24, dataset:CIFAR, training... loss:0.69050\n",
      "DPSH[25/150][10:41:14] bit:24, dataset:CIFAR, training... loss:0.69416\n",
      "DPSH[26/150][10:41:14] bit:24, dataset:CIFAR, training... loss:0.69496\n",
      "DPSH[27/150][10:41:14] bit:24, dataset:CIFAR, training... loss:0.69602\n",
      "DPSH[28/150][10:41:14] bit:24, dataset:CIFAR, training... loss:0.69650\n",
      "DPSH[29/150][10:41:14] bit:24, dataset:CIFAR, training... loss:0.69743\n",
      "DPSH[30/150][10:41:15] bit:24, dataset:CIFAR, training... loss:0.69150\n",
      "DPSH[31/150][10:41:15] bit:24, dataset:CIFAR, training... loss:0.68959\n",
      "DPSH[32/150][10:41:15] bit:24, dataset:CIFAR, training... loss:0.68921\n",
      "DPSH[33/150][10:41:15] bit:24, dataset:CIFAR, training... loss:0.68610\n",
      "DPSH[34/150][10:41:15] bit:24, dataset:CIFAR, training... loss:0.68231\n",
      "DPSH[35/150][10:41:16] bit:24, dataset:CIFAR, training... loss:0.68398\n",
      "DPSH[36/150][10:41:16] bit:24, dataset:CIFAR, training... loss:0.68393\n",
      "DPSH[37/150][10:41:16] bit:24, dataset:CIFAR, training... loss:0.68113\n",
      "DPSH[38/150][10:41:16] bit:24, dataset:CIFAR, training... loss:0.68072\n",
      "DPSH[39/150][10:41:16] bit:24, dataset:CIFAR, training... loss:0.67951\n",
      "DPSH[40/150][10:41:17] bit:24, dataset:CIFAR, training... loss:0.68021\n",
      "DPSH[41/150][10:41:17] bit:24, dataset:CIFAR, training... loss:0.67655\n",
      "DPSH[42/150][10:41:17] bit:24, dataset:CIFAR, training... loss:0.67494\n",
      "DPSH[43/150][10:41:17] bit:24, dataset:CIFAR, training... loss:0.67414\n",
      "DPSH[44/150][10:41:17] bit:24, dataset:CIFAR, training... loss:0.67447\n",
      "DPSH[45/150][10:41:18] bit:24, dataset:CIFAR, training... loss:0.67625\n",
      "DPSH[46/150][10:41:18] bit:24, dataset:CIFAR, training... loss:0.67504\n",
      "DPSH[47/150][10:41:18] bit:24, dataset:CIFAR, training... loss:0.67713\n",
      "DPSH[48/150][10:41:18] bit:24, dataset:CIFAR, training... loss:0.67814\n",
      "DPSH[49/150][10:41:19] bit:24, dataset:CIFAR, training... loss:0.67478\n",
      "DPSH[50/150][10:41:19] bit:24, dataset:CIFAR, training... loss:0.67453\n",
      "DPSH[51/150][10:41:19] bit:24, dataset:CIFAR, training... loss:0.66993\n",
      "DPSH[52/150][10:41:19] bit:24, dataset:CIFAR, training... loss:0.67117\n",
      "DPSH[53/150][10:41:19] bit:24, dataset:CIFAR, training... loss:0.66745\n",
      "DPSH[54/150][10:41:20] bit:24, dataset:CIFAR, training... loss:0.66884\n",
      "DPSH[55/150][10:41:20] bit:24, dataset:CIFAR, training... loss:0.66884\n",
      "DPSH[56/150][10:41:20] bit:24, dataset:CIFAR, training... loss:0.66801\n",
      "DPSH[57/150][10:41:20] bit:24, dataset:CIFAR, training... loss:0.66803\n",
      "DPSH[58/150][10:41:20] bit:24, dataset:CIFAR, training... loss:0.66907\n",
      "DPSH[59/150][10:41:21] bit:24, dataset:CIFAR, training... loss:0.66846\n",
      "DPSH[60/150][10:41:21] bit:24, dataset:CIFAR, training... loss:0.66676\n",
      "DPSH[61/150][10:41:21] bit:24, dataset:CIFAR, training... loss:0.66575\n",
      "DPSH[62/150][10:41:21] bit:24, dataset:CIFAR, training... loss:0.66458\n",
      "DPSH[63/150][10:41:21] bit:24, dataset:CIFAR, training... loss:0.66049\n",
      "DPSH[64/150][10:41:22] bit:24, dataset:CIFAR, training... loss:0.66077\n",
      "DPSH[65/150][10:41:22] bit:24, dataset:CIFAR, training... loss:0.66028\n",
      "DPSH[66/150][10:41:22] bit:24, dataset:CIFAR, training... loss:0.66107\n",
      "DPSH[67/150][10:41:22] bit:24, dataset:CIFAR, training... loss:0.66003\n",
      "DPSH[68/150][10:41:22] bit:24, dataset:CIFAR, training... loss:0.66270\n",
      "DPSH[69/150][10:41:22] bit:24, dataset:CIFAR, training... loss:0.66283\n",
      "DPSH[70/150][10:41:23] bit:24, dataset:CIFAR, training... loss:0.66539\n",
      "DPSH[71/150][10:41:23] bit:24, dataset:CIFAR, training... loss:0.66754\n",
      "DPSH[72/150][10:41:23] bit:24, dataset:CIFAR, training... loss:0.66405\n",
      "DPSH[73/150][10:41:23] bit:24, dataset:CIFAR, training... loss:0.65928\n",
      "DPSH[74/150][10:41:23] bit:24, dataset:CIFAR, training... loss:0.65749\n",
      "DPSH[75/150][10:41:24] bit:24, dataset:CIFAR, training... loss:0.64929\n",
      "DPSH[76/150][10:41:24] bit:24, dataset:CIFAR, training... loss:0.64775\n",
      "DPSH[77/150][10:41:24] bit:24, dataset:CIFAR, training... loss:0.64552\n",
      "DPSH[78/150][10:41:24] bit:24, dataset:CIFAR, training... loss:0.64054\n",
      "DPSH[79/150][10:41:24] bit:24, dataset:CIFAR, training... loss:0.63959\n",
      "DPSH[80/150][10:41:25] bit:24, dataset:CIFAR, training... loss:0.63710\n",
      "DPSH[81/150][10:41:25] bit:24, dataset:CIFAR, training... loss:0.64098\n",
      "DPSH[82/150][10:41:25] bit:24, dataset:CIFAR, training... loss:0.64064\n",
      "DPSH[83/150][10:41:25] bit:24, dataset:CIFAR, training... loss:0.64238\n",
      "DPSH[84/150][10:41:25] bit:24, dataset:CIFAR, training... loss:0.64688\n",
      "DPSH[85/150][10:41:26] bit:24, dataset:CIFAR, training... loss:0.63990\n",
      "DPSH[86/150][10:41:26] bit:24, dataset:CIFAR, training... loss:0.63467\n",
      "DPSH[87/150][10:41:26] bit:24, dataset:CIFAR, training... loss:0.63152\n",
      "DPSH[88/150][10:41:26] bit:24, dataset:CIFAR, training... loss:0.62555\n",
      "DPSH[89/150][10:41:26] bit:24, dataset:CIFAR, training... loss:0.62455\n",
      "DPSH[90/150][10:41:27] bit:24, dataset:CIFAR, training... loss:0.62189\n",
      "DPSH[91/150][10:41:27] bit:24, dataset:CIFAR, training... loss:0.61904\n",
      "DPSH[92/150][10:41:27] bit:24, dataset:CIFAR, training... loss:0.61782\n",
      "DPSH[93/150][10:41:27] bit:24, dataset:CIFAR, training... loss:0.61609\n",
      "DPSH[94/150][10:41:27] bit:24, dataset:CIFAR, training... loss:0.61581\n",
      "DPSH[95/150][10:41:28] bit:24, dataset:CIFAR, training... loss:0.61632\n",
      "DPSH[96/150][10:41:28] bit:24, dataset:CIFAR, training... loss:0.61525\n",
      "DPSH[97/150][10:41:28] bit:24, dataset:CIFAR, training... loss:0.61188\n",
      "DPSH[98/150][10:41:28] bit:24, dataset:CIFAR, training... loss:0.61179\n",
      "DPSH[99/150][10:41:28] bit:24, dataset:CIFAR, training... loss:0.60869\n",
      "DPSH[100/150][10:41:29] bit:24, dataset:CIFAR, training... loss:0.60830\n",
      "DPSH[101/150][10:41:29] bit:24, dataset:CIFAR, training... loss:0.60259\n",
      "DPSH[102/150][10:41:29] bit:24, dataset:CIFAR, training... loss:0.60080\n",
      "DPSH[103/150][10:41:29] bit:24, dataset:CIFAR, training... loss:0.59708\n",
      "DPSH[104/150][10:41:29] bit:24, dataset:CIFAR, training... loss:0.59315\n",
      "DPSH[105/150][10:41:30] bit:24, dataset:CIFAR, training... loss:0.58961\n",
      "DPSH[106/150][10:41:30] bit:24, dataset:CIFAR, training... loss:0.58953\n",
      "DPSH[107/150][10:41:30] bit:24, dataset:CIFAR, training... loss:0.58995\n",
      "DPSH[108/150][10:41:30] bit:24, dataset:CIFAR, training... loss:0.58497\n",
      "DPSH[109/150][10:41:30] bit:24, dataset:CIFAR, training... loss:0.58348\n",
      "DPSH[110/150][10:41:31] bit:24, dataset:CIFAR, training... loss:0.58206\n",
      "DPSH[111/150][10:41:31] bit:24, dataset:CIFAR, training... loss:0.57982\n",
      "DPSH[112/150][10:41:31] bit:24, dataset:CIFAR, training... loss:0.58011\n",
      "DPSH[113/150][10:41:31] bit:24, dataset:CIFAR, training... loss:0.57580\n",
      "DPSH[114/150][10:41:31] bit:24, dataset:CIFAR, training... loss:0.57550\n",
      "DPSH[115/150][10:41:31] bit:24, dataset:CIFAR, training... loss:0.57590\n",
      "DPSH[116/150][10:41:32] bit:24, dataset:CIFAR, training... loss:0.57500\n",
      "DPSH[117/150][10:41:32] bit:24, dataset:CIFAR, training... loss:0.57470\n",
      "DPSH[118/150][10:41:32] bit:24, dataset:CIFAR, training... loss:0.56976\n",
      "DPSH[119/150][10:41:32] bit:24, dataset:CIFAR, training... loss:0.56978\n",
      "DPSH[120/150][10:41:32] bit:24, dataset:CIFAR, training... loss:0.56931\n",
      "DPSH[121/150][10:41:33] bit:24, dataset:CIFAR, training... loss:0.56601\n",
      "DPSH[122/150][10:41:33] bit:24, dataset:CIFAR, training... loss:0.56416\n",
      "DPSH[123/150][10:41:33] bit:24, dataset:CIFAR, training... loss:0.56094\n",
      "DPSH[124/150][10:41:33] bit:24, dataset:CIFAR, training... loss:0.55979\n",
      "DPSH[125/150][10:41:33] bit:24, dataset:CIFAR, training... loss:0.55694\n",
      "DPSH[126/150][10:41:33] bit:24, dataset:CIFAR, training... loss:0.55252\n",
      "DPSH[127/150][10:41:34] bit:24, dataset:CIFAR, training... loss:0.55324\n",
      "DPSH[128/150][10:41:34] bit:24, dataset:CIFAR, training... loss:0.54714\n",
      "DPSH[129/150][10:41:34] bit:24, dataset:CIFAR, training... loss:0.54605\n",
      "DPSH[130/150][10:41:34] bit:24, dataset:CIFAR, training... loss:0.54849\n",
      "DPSH[131/150][10:41:34] bit:24, dataset:CIFAR, training... loss:0.54364\n",
      "DPSH[132/150][10:41:35] bit:24, dataset:CIFAR, training... loss:0.54563\n",
      "DPSH[133/150][10:41:35] bit:24, dataset:CIFAR, training... loss:0.54251\n",
      "DPSH[134/150][10:41:35] bit:24, dataset:CIFAR, training... loss:0.54387\n",
      "DPSH[135/150][10:41:35] bit:24, dataset:CIFAR, training... loss:0.53969\n",
      "DPSH[136/150][10:41:35] bit:24, dataset:CIFAR, training... loss:0.53978\n",
      "DPSH[137/150][10:41:36] bit:24, dataset:CIFAR, training... loss:0.53466\n",
      "DPSH[138/150][10:41:36] bit:24, dataset:CIFAR, training... loss:0.53405\n",
      "DPSH[139/150][10:41:36] bit:24, dataset:CIFAR, training... loss:0.53437\n",
      "DPSH[140/150][10:41:36] bit:24, dataset:CIFAR, training... loss:0.52669\n",
      "DPSH[141/150][10:41:36] bit:24, dataset:CIFAR, training... loss:0.52436\n",
      "DPSH[142/150][10:41:37] bit:24, dataset:CIFAR, training... loss:0.52136\n",
      "DPSH[143/150][10:41:37] bit:24, dataset:CIFAR, training... loss:0.52214\n",
      "DPSH[144/150][10:41:37] bit:24, dataset:CIFAR, training... loss:0.52108\n",
      "DPSH[145/150][10:41:37] bit:24, dataset:CIFAR, training... loss:0.52035\n",
      "DPSH[146/150][10:41:37] bit:24, dataset:CIFAR, training... loss:0.51816\n",
      "DPSH[147/150][10:41:38] bit:24, dataset:CIFAR, training... loss:0.51668\n",
      "DPSH[148/150][10:41:38] bit:24, dataset:CIFAR, training... loss:0.51607\n",
      "DPSH[149/150][10:41:38] bit:24, dataset:CIFAR, training... loss:0.51413\n",
      "DPSH[150/150][10:41:38] bit:24, dataset:CIFAR, training... loss:0.51201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 428.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mAP: 0.6818\n",
      "Testing combination: {'eta': 0.01, 'learning_rate': 1e-05, 'weight_decay': 0.0001}\n",
      "DPSH[ 1/150][10:41:40] bit:24, dataset:CIFAR, training... loss:0.72661\n",
      "DPSH[ 2/150][10:41:40] bit:24, dataset:CIFAR, training... loss:0.75627\n",
      "DPSH[ 3/150][10:41:40] bit:24, dataset:CIFAR, training... loss:0.76741\n",
      "DPSH[ 4/150][10:41:40] bit:24, dataset:CIFAR, training... loss:0.76695\n",
      "DPSH[ 5/150][10:41:41] bit:24, dataset:CIFAR, training... loss:0.75335\n",
      "DPSH[ 6/150][10:41:41] bit:24, dataset:CIFAR, training... loss:0.75525\n",
      "DPSH[ 7/150][10:41:41] bit:24, dataset:CIFAR, training... loss:0.73644\n",
      "DPSH[ 8/150][10:41:41] bit:24, dataset:CIFAR, training... loss:0.73446\n",
      "DPSH[ 9/150][10:41:41] bit:24, dataset:CIFAR, training... loss:0.71660\n",
      "DPSH[10/150][10:41:42] bit:24, dataset:CIFAR, training... loss:0.71310\n",
      "DPSH[11/150][10:41:42] bit:24, dataset:CIFAR, training... loss:0.71283\n",
      "DPSH[12/150][10:41:42] bit:24, dataset:CIFAR, training... loss:0.70569\n",
      "DPSH[13/150][10:41:42] bit:24, dataset:CIFAR, training... loss:0.70841\n",
      "DPSH[14/150][10:41:42] bit:24, dataset:CIFAR, training... loss:0.70777\n",
      "DPSH[15/150][10:41:43] bit:24, dataset:CIFAR, training... loss:0.70478\n",
      "DPSH[16/150][10:41:43] bit:24, dataset:CIFAR, training... loss:0.70547\n",
      "DPSH[17/150][10:41:43] bit:24, dataset:CIFAR, training... loss:0.70042\n",
      "DPSH[18/150][10:41:43] bit:24, dataset:CIFAR, training... loss:0.70196\n",
      "DPSH[19/150][10:41:43] bit:24, dataset:CIFAR, training... loss:0.69699\n",
      "DPSH[20/150][10:41:44] bit:24, dataset:CIFAR, training... loss:0.69577\n",
      "DPSH[21/150][10:41:44] bit:24, dataset:CIFAR, training... loss:0.69620\n",
      "DPSH[22/150][10:41:44] bit:24, dataset:CIFAR, training... loss:0.69181\n",
      "DPSH[23/150][10:41:44] bit:24, dataset:CIFAR, training...."
     ]
    }
   ],
   "source": [
    "DPSH(device, 5000, 10, 24, 150, 128, [0.01, 0.05, 0.1], [1e-6, 1e-5, 1e-4], [1e-6, 1e-5, 1e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DPSH48Cifar = {'eta': 0.05, 'learning_rate': 1e-05, 'weight_decay': 1e-05}\n",
    "DPSH32Cifar = {'eta': 0.1, 'learning_rate': 1e-05, 'weight_decay': 1e-05}\n",
    "DPSH24Cifar = \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
