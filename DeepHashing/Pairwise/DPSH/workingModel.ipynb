{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: '[WinError 127] Den angivne procedure blev ikke fundet'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../../'\n",
    "sys.path.append(root)\n",
    "from HelpfulFunctions.batchCreation import createBatch\n",
    "from HelpfulFunctions.metrics import meanAveragePrecision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(root, num_classes, batch_size, train = 1, HPO = False):\n",
    "    if HPO == False:\n",
    "        if train == 1:\n",
    "            #Create X_train_tensor\n",
    "            X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "\n",
    "            #pca = PCA(n_components=128)\n",
    "            #pca.fit(X_train)\n",
    "            #X_train = torch.tensor(pca.transform(X_train), dtype=torch.float)\n",
    "            #X_test = torch.tensor(pca.transform(X_test), dtype=torch.float)\n",
    "            X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "            #Create Y_train_tensor\n",
    "            y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "            y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return train_loader\n",
    "\n",
    "        elif train == 2:\n",
    "            X_validation = np.load( root + \"Features/val_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "            X_validation_tensor = torch.tensor(X_validation)\n",
    "\n",
    "            y_validation = np.load( root + \"Features/val_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "            y_validation_tensor = torch.tensor(y_validation, dtype=torch.long)\n",
    "            y_validation_tensor = torch.nn.functional.one_hot(y_validation_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_validation = torch.arange(len(X_validation_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_validation_tensor, y_validation_tensor, indices_validation)\n",
    "            validation_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return validation_loader\n",
    "\n",
    "        elif train == 3:\n",
    "            X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "            X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "            y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "            y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "            y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "            test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return test_loader\n",
    "\n",
    "\n",
    "\n",
    "    if HPO == True:\n",
    "        if train == 1:\n",
    "            #Create X_train_tensor\n",
    "            X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "            X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "            #Create Y_train_tensor\n",
    "            y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "            y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return train_loader\n",
    "\n",
    "        elif train == 2:\n",
    "            X_validation = np.load( root + \"Features/val_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "            X_validation_tensor = torch.tensor(X_validation)\n",
    "\n",
    "            y_validation = np.load( root + \"Features/val_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "            y_validation_tensor = torch.tensor(y_validation, dtype=torch.long)\n",
    "            y_validation_tensor = torch.nn.functional.one_hot(y_validation_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_validation = torch.arange(len(X_validation_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_validation_tensor, y_validation_tensor, indices_validation)\n",
    "            validation_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return validation_loader\n",
    "\n",
    "        elif train == 3:\n",
    "            X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "            X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "            y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "            y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "            y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "            #Create indices\n",
    "            indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "            dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "            test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, bits):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, bits),    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "        # Initialize weights and biases from gaussian distribution\n",
    "        for layer in self.fc_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=0.01)  # Initialize weights based on paper\n",
    "                nn.init.normal_(layer.bias, mean=0.0, std=0.01)    # Initialize biases based on paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPSHLoss(torch.nn.Module):\n",
    "    def __init__(self, train_size, n_classes, bit):\n",
    "        super(DPSHLoss, self).__init__()\n",
    "        self.U = torch.zeros(train_size, bit).float().to(device)\n",
    "        self.Y = torch.zeros(train_size, n_classes).float().to(device)\n",
    "\n",
    "    def forward(self, u, y, ind, eta):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        s = (y @ self.Y.t() > 0).float()\n",
    "        inner_product = u @ self.U.t() * 0.5\n",
    "\n",
    "        likelihood_loss = (1 + (-(inner_product.abs())).exp()).log() + inner_product.clamp(min=0) - s * inner_product\n",
    "\n",
    "        likelihood_loss = likelihood_loss.mean()\n",
    "\n",
    "        quantization_loss = eta * (u - u.sign()).pow(2).mean()\n",
    "\n",
    "        return likelihood_loss + quantization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid\n",
    "\n",
    "def DPSH(device: torch.device, train_size: int, n_classes: int, bit: int, num_epoch: int, batch_size: int, eta_values: list, wd_values: list, lr_values: list):\n",
    "\n",
    "    train_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 1)\n",
    "    test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 2)\n",
    "    validation_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 3)\n",
    "\n",
    "    param_grid = {\n",
    "        'eta': eta_values,\n",
    "        'learning_rate': lr_values,\n",
    "        #'batch_size': [16, 32, 64],\n",
    "        'weight_decay': wd_values\n",
    "    }\n",
    "\n",
    "    customLoss = DPSHLoss(train_size, n_classes, bit)\n",
    "\n",
    "\n",
    "    # Get all combinations of parameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    parameter_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # Evaluate each parameter combination\n",
    "    best_params = None\n",
    "    best_map = 0\n",
    "\n",
    "    for params in parameter_combinations:\n",
    "        print(f\"Testing combination: {params}\")\n",
    "    \n",
    "        # Initialize loss function with specific parameters\n",
    "        #loss_fn = customLoss()\n",
    "\n",
    "        # Initialize model and optimizer\n",
    "        model = CustomNN(bits = bit).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay = params['weight_decay'])\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epoch):  # Example epoch count\n",
    "            current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "            print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            \"DPSH\", epoch + 1, num_epoch, current_time, bit, \"CIFAR\"), end=\"\")\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for image, label, ind in train_loader:\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                u = model(image)\n",
    "\n",
    "                loss = customLoss(u, label.float(), ind, eta = params['eta'])\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss = train_loss / (train_size / batch_size)\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for image, label, ind in train_loader:\n",
    "                image = image.to(device)\n",
    "                #label = label.to(device)\n",
    "                hash_train = (model(image)).sign()\n",
    "                hash_train = hash_train.detach().numpy()\n",
    "                label_train = label.numpy()\n",
    "\n",
    "            \n",
    "            for image, label, ind in validation_loader:\n",
    "                image = image.to(device)\n",
    "                #label = label.to(device)\n",
    "                hash_val = (model(image)).sign()\n",
    "                hash_val = hash_val.detach().numpy()\n",
    "                label_val = label.numpy()\n",
    "\n",
    "\n",
    "            map = meanAveragePrecision(training_hashes = hash_train, training_labels = label_train, test_hashes = hash_val, test_labels = label_val)\n",
    "\n",
    "        print(f\"Validation mAP: {map:.4f}\")\n",
    "\n",
    "        # Update best parameters\n",
    "        if map > best_map:\n",
    "            best_map = map\n",
    "            best_params = params\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best mAP:\", map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination: {'eta': 1e-05, 'learning_rate': 1e-05, 'weight_decay': 1e-05}\n",
      "DPSH[ 1/150][13:08:25] bit:48, dataset:CIFAR, training....DPSH[ 2/150][13:09:20] bit:48, dataset:CIFAR, training....DPSH[ 3/150][13:10:16] bit:48, dataset:CIFAR, training...."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mDPSH\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m45000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 55\u001b[0m, in \u001b[0;36mDPSH\u001b[1;34m(device, train_size, n_classes, bit, num_epoch, batch_size, eta_values, wd_values, lr_values)\u001b[0m\n\u001b[0;32m     52\u001b[0m     loss \u001b[38;5;241m=\u001b[39m customLoss(u, label\u001b[38;5;241m.\u001b[39mfloat(), ind, eta \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     53\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 55\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     58\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m (train_size \u001b[38;5;241m/\u001b[39m batch_size)\n",
      "File \u001b[1;32mc:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobop\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DPSH(device, 45000, 10, 48, 150, 128, [1e-5, 1e-3, 1e-1], [1e-5, 1e-3, 1e-1], [1e-5, 1e-3, 1e-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
