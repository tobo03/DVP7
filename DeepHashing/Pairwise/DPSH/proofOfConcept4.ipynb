{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../../'\n",
    "sys.path.append(root)\n",
    "#from HelpfulFunctions.batchCreation import createBatch\n",
    "from HelpfulFunctions.metrics import meanAveragePrecision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(root, num_classes, batch_size, train = True):\n",
    "    if train == True:\n",
    "        #Create X_train_tensor\n",
    "        X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "        pca = PCA(n_components=128)\n",
    "        pca.fit(X_train)\n",
    "        X_train_tensor = torch.tensor(pca.transform(X_train), dtype=torch.float)\n",
    "\n",
    "\n",
    "        #X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "        #Create Y_train_tensor\n",
    "        y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    else:\n",
    "        X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "        pca = PCA(n_components=128)\n",
    "        pca.fit(X_train)\n",
    "        #X_train_tensor = torch.tensor(pca.transform(X_train), dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "        X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "        #X_test_tensor = torch.tensor(X_test)\n",
    "        X_test_tensor = torch.tensor(pca.transform(X_test), dtype=torch.float)\n",
    "        \n",
    "        y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "        y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return test_loader\n",
    "\n",
    "    #Missing implementation for Test and Validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CreateDataset(root, num_classes = 10, batch_size = 128)\n",
    "test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128, 128),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "        # Initialize weights and biases from gaussian distribution\n",
    "        for layer in self.fc_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=0.01)  # Initialize weights based on paper\n",
    "                nn.init.normal_(layer.bias, mean=0.0, std=0.01)    # Initialize biases based on paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPSHLoss(torch.nn.Module):\n",
    "    def __init__(self, train_size, n_classes, bit):\n",
    "        super(DPSHLoss, self).__init__()\n",
    "        self.U = torch.zeros(train_size, bit).float().to(device)\n",
    "        self.Y = torch.zeros(train_size, n_classes).float().to(device)\n",
    "\n",
    "    def forward(self, u, y, ind, eta):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        s = (y @ self.Y.t() > 0).float()\n",
    "        inner_product = u @ self.U.t() * 0.5\n",
    "\n",
    "        likelihood_loss = (1 + (-(inner_product.abs())).exp()).log() + inner_product.clamp(min=0) - s * inner_product\n",
    "\n",
    "        likelihood_loss = likelihood_loss.mean()\n",
    "\n",
    "        quantization_loss = eta * (u - u.sign()).pow(2).mean()\n",
    "\n",
    "        return likelihood_loss + quantization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(device, train_loader, train_size, batch_size, n_classes, bit, num_epoch, eta):\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay = 10 ** -5)\n",
    "    \n",
    "    criterion = DPSHLoss(train_size, n_classes, bit)\n",
    "\n",
    "\n",
    "    #model.train()\n",
    "\n",
    "    #Best_mAP = 0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "        print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            \"DPSH\", epoch + 1, num_epoch, current_time, bit, \"CIFAR\"), end=\"\")\n",
    "\n",
    "        train_loss = 0\n",
    "        for image, label, ind in train_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            u = model(image)\n",
    "\n",
    "\n",
    "            loss = criterion(u, label.float(), ind, eta)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss / (train_size / batch_size)\n",
    "        print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))\n",
    "\n",
    "    return model\n",
    "        #if (epoch + 1) % 3 == 0:\n",
    "            #map = meanAveragePrecision(train_loader, test_loader, model, device)\n",
    "            #print(map)\n",
    "            #Best_mAP = validate(config, Best_mAP, test_loader, dataset_loader, net, bit, epoch, num_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPSH[ 1/150][11:48:27] bit:32, dataset:CIFAR, training... loss:0.78884\n",
      "DPSH[ 2/150][11:49:09] bit:32, dataset:CIFAR, training... loss:0.78431\n",
      "DPSH[ 3/150][11:49:50] bit:32, dataset:CIFAR, training... loss:0.77026\n",
      "DPSH[ 4/150][11:50:31] bit:32, dataset:CIFAR, training... loss:0.76155\n",
      "DPSH[ 5/150][11:51:13] bit:32, dataset:CIFAR, training... loss:0.75415\n",
      "DPSH[ 6/150][11:51:54] bit:32, dataset:CIFAR, training... loss:0.74349\n",
      "DPSH[ 7/150][11:52:35] bit:32, dataset:CIFAR, training... loss:0.73197\n",
      "DPSH[ 8/150][11:53:17] bit:32, dataset:CIFAR, training... loss:0.72378\n",
      "DPSH[ 9/150][11:53:58] bit:32, dataset:CIFAR, training... loss:0.71731\n",
      "DPSH[10/150][11:54:40] bit:32, dataset:CIFAR, training... loss:0.71291\n",
      "DPSH[11/150][11:55:23] bit:32, dataset:CIFAR, training... loss:0.70874\n",
      "DPSH[12/150][11:56:04] bit:32, dataset:CIFAR, training... loss:0.70544\n",
      "DPSH[13/150][11:56:47] bit:32, dataset:CIFAR, training... loss:0.70192\n",
      "DPSH[14/150][11:57:28] bit:32, dataset:CIFAR, training... loss:0.69733\n",
      "DPSH[15/150][11:58:09] bit:32, dataset:CIFAR, training... loss:0.69171\n",
      "DPSH[16/150][11:58:51] bit:32, dataset:CIFAR, training... loss:0.68637\n",
      "DPSH[17/150][11:59:31] bit:32, dataset:CIFAR, training... loss:0.68120\n",
      "DPSH[18/150][12:00:12] bit:32, dataset:CIFAR, training... loss:0.67485\n",
      "DPSH[19/150][12:00:53] bit:32, dataset:CIFAR, training... loss:0.66990\n",
      "DPSH[20/150][12:01:35] bit:32, dataset:CIFAR, training... loss:0.66610\n",
      "DPSH[21/150][12:02:16] bit:32, dataset:CIFAR, training... loss:0.66171\n",
      "DPSH[22/150][12:02:57] bit:32, dataset:CIFAR, training... loss:0.65772\n",
      "DPSH[23/150][12:03:38] bit:32, dataset:CIFAR, training... loss:0.65487\n",
      "DPSH[24/150][12:04:19] bit:32, dataset:CIFAR, training... loss:0.65203\n",
      "DPSH[25/150][12:05:00] bit:32, dataset:CIFAR, training... loss:0.64958\n",
      "DPSH[26/150][12:05:41] bit:32, dataset:CIFAR, training... loss:0.64715\n",
      "DPSH[27/150][12:06:22] bit:32, dataset:CIFAR, training... loss:0.64505\n",
      "DPSH[28/150][12:07:03] bit:32, dataset:CIFAR, training... loss:0.64338\n",
      "DPSH[29/150][12:07:45] bit:32, dataset:CIFAR, training... loss:0.64157\n",
      "DPSH[30/150][12:08:27] bit:32, dataset:CIFAR, training... loss:0.63945\n",
      "DPSH[31/150][12:09:08] bit:32, dataset:CIFAR, training... loss:0.63764\n",
      "DPSH[32/150][12:09:49] bit:32, dataset:CIFAR, training... loss:0.63636\n",
      "DPSH[33/150][12:10:32] bit:32, dataset:CIFAR, training... loss:0.63460\n",
      "DPSH[34/150][12:11:13] bit:32, dataset:CIFAR, training... loss:0.63268\n",
      "DPSH[35/150][12:11:54] bit:32, dataset:CIFAR, training... loss:0.63081\n",
      "DPSH[36/150][12:12:35] bit:32, dataset:CIFAR, training... loss:0.62877\n",
      "DPSH[37/150][12:13:16] bit:32, dataset:CIFAR, training... loss:0.62704\n",
      "DPSH[38/150][12:13:57] bit:32, dataset:CIFAR, training... loss:0.62518\n",
      "DPSH[39/150][12:14:39] bit:32, dataset:CIFAR, training... loss:0.62321\n",
      "DPSH[40/150][12:15:20] bit:32, dataset:CIFAR, training... loss:0.62157\n",
      "DPSH[41/150][12:16:01] bit:32, dataset:CIFAR, training... loss:0.62043\n",
      "DPSH[42/150][12:16:42] bit:32, dataset:CIFAR, training... loss:0.61899\n",
      "DPSH[43/150][12:17:23] bit:32, dataset:CIFAR, training... loss:0.61790\n",
      "DPSH[44/150][12:18:05] bit:32, dataset:CIFAR, training... loss:0.61669\n",
      "DPSH[45/150][12:18:46] bit:32, dataset:CIFAR, training... loss:0.61584\n",
      "DPSH[46/150][12:19:27] bit:32, dataset:CIFAR, training... loss:0.61483\n",
      "DPSH[47/150][12:20:08] bit:32, dataset:CIFAR, training... loss:0.61388\n",
      "DPSH[48/150][12:20:49] bit:32, dataset:CIFAR, training... loss:0.61304\n",
      "DPSH[49/150][12:21:30] bit:32, dataset:CIFAR, training... loss:0.61240\n",
      "DPSH[50/150][12:22:11] bit:32, dataset:CIFAR, training... loss:0.61143\n",
      "DPSH[51/150][12:22:52] bit:32, dataset:CIFAR, training... loss:0.61028\n",
      "DPSH[52/150][12:23:33] bit:32, dataset:CIFAR, training... loss:0.60976\n",
      "DPSH[53/150][12:24:15] bit:32, dataset:CIFAR, training... loss:0.60890\n",
      "DPSH[54/150][12:24:56] bit:32, dataset:CIFAR, training... loss:0.60797\n",
      "DPSH[55/150][12:25:37] bit:32, dataset:CIFAR, training... loss:0.60727\n",
      "DPSH[56/150][12:26:18] bit:32, dataset:CIFAR, training... loss:0.60665\n",
      "DPSH[57/150][12:27:00] bit:32, dataset:CIFAR, training... loss:0.60597\n",
      "DPSH[58/150][12:27:41] bit:32, dataset:CIFAR, training... loss:0.60530\n",
      "DPSH[59/150][12:28:22] bit:32, dataset:CIFAR, training... loss:0.60459\n",
      "DPSH[60/150][12:29:03] bit:32, dataset:CIFAR, training... loss:0.60392\n",
      "DPSH[61/150][12:29:49] bit:32, dataset:CIFAR, training... loss:0.60298\n",
      "DPSH[62/150][12:30:36] bit:32, dataset:CIFAR, training... loss:0.60229\n",
      "DPSH[63/150][12:33:43] bit:32, dataset:CIFAR, training... loss:0.60142\n",
      "DPSH[64/150][12:34:38] bit:32, dataset:CIFAR, training... loss:0.60102\n",
      "DPSH[65/150][12:35:30] bit:32, dataset:CIFAR, training... loss:0.59996\n",
      "DPSH[66/150][12:36:23] bit:32, dataset:CIFAR, training... loss:0.59940\n",
      "DPSH[67/150][12:37:15] bit:32, dataset:CIFAR, training... loss:0.59888\n",
      "DPSH[68/150][12:38:08] bit:32, dataset:CIFAR, training... loss:0.59840\n",
      "DPSH[69/150][12:39:03] bit:32, dataset:CIFAR, training... loss:0.59741\n",
      "DPSH[70/150][12:39:55] bit:32, dataset:CIFAR, training... loss:0.59634\n",
      "DPSH[71/150][12:40:48] bit:32, dataset:CIFAR, training... loss:0.59582\n",
      "DPSH[72/150][12:41:39] bit:32, dataset:CIFAR, training... loss:0.59518\n",
      "DPSH[73/150][12:42:32] bit:32, dataset:CIFAR, training... loss:0.59466\n",
      "DPSH[74/150][12:43:27] bit:32, dataset:CIFAR, training... loss:0.59400\n",
      "DPSH[75/150][12:44:21] bit:32, dataset:CIFAR, training... loss:0.59325\n",
      "DPSH[76/150][12:45:13] bit:32, dataset:CIFAR, training... loss:0.59228\n",
      "DPSH[77/150][12:46:05] bit:32, dataset:CIFAR, training... loss:0.59179\n",
      "DPSH[78/150][12:46:57] bit:32, dataset:CIFAR, training... loss:0.59107\n",
      "DPSH[79/150][12:47:49] bit:32, dataset:CIFAR, training... loss:0.59062\n",
      "DPSH[80/150][12:48:41] bit:32, dataset:CIFAR, training... loss:0.59006\n",
      "DPSH[81/150][12:49:33] bit:32, dataset:CIFAR, training... loss:0.58941\n",
      "DPSH[82/150][12:50:25] bit:32, dataset:CIFAR, training... loss:0.58837\n",
      "DPSH[83/150][12:51:17] bit:32, dataset:CIFAR, training... loss:0.58777\n",
      "DPSH[84/150][12:52:09] bit:32, dataset:CIFAR, training... loss:0.58741\n",
      "DPSH[85/150][12:53:02] bit:32, dataset:CIFAR, training... loss:0.58677\n",
      "DPSH[86/150][12:53:54] bit:32, dataset:CIFAR, training... loss:0.58606\n",
      "DPSH[87/150][12:54:47] bit:32, dataset:CIFAR, training... loss:0.58518\n",
      "DPSH[88/150][12:55:43] bit:32, dataset:CIFAR, training... loss:0.58480\n",
      "DPSH[89/150][12:56:37] bit:32, dataset:CIFAR, training... loss:0.58418\n",
      "DPSH[90/150][12:57:28] bit:32, dataset:CIFAR, training... loss:0.58369\n",
      "DPSH[91/150][12:58:23] bit:32, dataset:CIFAR, training... loss:0.58300\n",
      "DPSH[92/150][12:59:17] bit:32, dataset:CIFAR, training... loss:0.58220\n",
      "DPSH[93/150][13:00:12] bit:32, dataset:CIFAR, training... loss:0.58167\n",
      "DPSH[94/150][13:01:04] bit:32, dataset:CIFAR, training... loss:0.58132\n",
      "DPSH[95/150][13:01:59] bit:32, dataset:CIFAR, training... loss:0.58049\n",
      "DPSH[96/150][13:02:52] bit:32, dataset:CIFAR, training... loss:0.57992\n",
      "DPSH[97/150][13:03:44] bit:32, dataset:CIFAR, training... loss:0.57938\n",
      "DPSH[98/150][13:04:42] bit:32, dataset:CIFAR, training... loss:0.57901\n",
      "DPSH[99/150][13:05:43] bit:32, dataset:CIFAR, training... loss:0.57817\n",
      "DPSH[100/150][13:06:39] bit:32, dataset:CIFAR, training... loss:0.57749\n",
      "DPSH[101/150][13:07:33] bit:32, dataset:CIFAR, training... loss:0.57683\n",
      "DPSH[102/150][13:08:26] bit:32, dataset:CIFAR, training... loss:0.57640\n",
      "DPSH[103/150][13:09:18] bit:32, dataset:CIFAR, training... loss:0.57604\n",
      "DPSH[104/150][13:10:15] bit:32, dataset:CIFAR, training... loss:0.57561\n",
      "DPSH[105/150][13:11:11] bit:32, dataset:CIFAR, training... loss:0.57499\n",
      "DPSH[106/150][13:12:03] bit:32, dataset:CIFAR, training... loss:0.57423\n",
      "DPSH[107/150][13:13:00] bit:32, dataset:CIFAR, training... loss:0.57416\n",
      "DPSH[108/150][13:13:57] bit:32, dataset:CIFAR, training... loss:0.57348\n",
      "DPSH[109/150][13:14:51] bit:32, dataset:CIFAR, training... loss:0.57290\n",
      "DPSH[110/150][13:15:42] bit:32, dataset:CIFAR, training... loss:0.57220\n",
      "DPSH[111/150][13:16:39] bit:32, dataset:CIFAR, training... loss:0.57170\n",
      "DPSH[112/150][13:17:39] bit:32, dataset:CIFAR, training... loss:0.57132\n",
      "DPSH[113/150][13:18:31] bit:32, dataset:CIFAR, training... loss:0.57084\n",
      "DPSH[114/150][13:19:36] bit:32, dataset:CIFAR, training... loss:0.57051\n",
      "DPSH[115/150][13:20:34] bit:32, dataset:CIFAR, training... loss:0.57005\n",
      "DPSH[116/150][13:21:19] bit:32, dataset:CIFAR, training... loss:0.56953\n",
      "DPSH[117/150][13:22:01] bit:32, dataset:CIFAR, training... loss:0.56871\n",
      "DPSH[118/150][13:22:43] bit:32, dataset:CIFAR, training... loss:0.56834\n",
      "DPSH[119/150][13:23:24] bit:32, dataset:CIFAR, training... loss:0.56783\n",
      "DPSH[120/150][13:24:05] bit:32, dataset:CIFAR, training... loss:0.56775\n",
      "DPSH[121/150][13:24:46] bit:32, dataset:CIFAR, training... loss:0.56725\n",
      "DPSH[122/150][13:25:27] bit:32, dataset:CIFAR, training... loss:0.56651\n",
      "DPSH[123/150][13:26:08] bit:32, dataset:CIFAR, training... loss:0.56619\n",
      "DPSH[124/150][13:26:49] bit:32, dataset:CIFAR, training... loss:0.56601\n",
      "DPSH[125/150][13:27:29] bit:32, dataset:CIFAR, training... loss:0.56579\n",
      "DPSH[126/150][13:28:10] bit:32, dataset:CIFAR, training... loss:0.56530\n",
      "DPSH[127/150][13:28:51] bit:32, dataset:CIFAR, training... loss:0.56486\n",
      "DPSH[128/150][13:29:33] bit:32, dataset:CIFAR, training... loss:0.56460\n",
      "DPSH[129/150][13:30:14] bit:32, dataset:CIFAR, training... loss:0.56426\n",
      "DPSH[130/150][13:30:55] bit:32, dataset:CIFAR, training... loss:0.56358\n",
      "DPSH[131/150][13:31:36] bit:32, dataset:CIFAR, training... loss:0.56345\n",
      "DPSH[132/150][13:32:17] bit:32, dataset:CIFAR, training... loss:0.56292\n",
      "DPSH[133/150][13:32:58] bit:32, dataset:CIFAR, training... loss:0.56265\n",
      "DPSH[134/150][13:33:39] bit:32, dataset:CIFAR, training... loss:0.56215\n",
      "DPSH[135/150][13:34:20] bit:32, dataset:CIFAR, training... loss:0.56182\n",
      "DPSH[136/150][13:35:01] bit:32, dataset:CIFAR, training... loss:0.56139\n",
      "DPSH[137/150][13:35:42] bit:32, dataset:CIFAR, training... loss:0.56094\n",
      "DPSH[138/150][13:36:23] bit:32, dataset:CIFAR, training... loss:0.56045\n",
      "DPSH[139/150][13:37:04] bit:32, dataset:CIFAR, training... loss:0.56026\n",
      "DPSH[140/150][13:37:45] bit:32, dataset:CIFAR, training... loss:0.55982\n",
      "DPSH[141/150][13:38:26] bit:32, dataset:CIFAR, training... loss:0.55958\n",
      "DPSH[142/150][13:39:07] bit:32, dataset:CIFAR, training... loss:0.55927\n",
      "DPSH[143/150][13:39:48] bit:32, dataset:CIFAR, training... loss:0.55901\n",
      "DPSH[144/150][13:40:29] bit:32, dataset:CIFAR, training... loss:0.55853\n",
      "DPSH[145/150][13:41:10] bit:32, dataset:CIFAR, training... loss:0.55820\n",
      "DPSH[146/150][13:41:51] bit:32, dataset:CIFAR, training... loss:0.55794\n",
      "DPSH[147/150][13:42:31] bit:32, dataset:CIFAR, training... loss:0.55756\n",
      "DPSH[148/150][13:43:12] bit:32, dataset:CIFAR, training... loss:0.55745\n",
      "DPSH[149/150][13:43:53] bit:32, dataset:CIFAR, training... loss:0.55703\n",
      "DPSH[150/150][13:44:34] bit:32, dataset:CIFAR, training... loss:0.55671\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_val(device, train_loader, train_size = 45000, batch_size = 128, n_classes = 10, bit = 32, num_epoch = 150, eta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trained_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "pca = PCA(n_components=128)\n",
    "pca.fit(X_train)\n",
    "X_train_tensor = torch.tensor(pca.transform(X_train), dtype=torch.float)\n",
    "\n",
    "\n",
    "#X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "#Create Y_train_tensor\n",
    "y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\") # Shape = (45000,)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, 10) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "\n",
    "X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "#X_test_tensor = torch.tensor(X_test)\n",
    "X_test_tensor = torch.tensor(pca.transform(X_test), dtype=torch.float)\n",
    "\n",
    "y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, 10) #One-Hot Encoded -> Shape = (10000, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_train = (trained_model(X_train_tensor)).sign()\n",
    "hash_train = hash_train.detach().numpy()\n",
    "hash_test = (trained_model(X_test_tensor)).sign()\n",
    "hash_test = hash_test.detach().numpy()\n",
    "y_train\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:09<00:00, 52.88it/s]\n"
     ]
    }
   ],
   "source": [
    "map = meanAveragePrecision(training_hashes = hash_train, test_hashes = hash_test, test_labels = y_test, training_labels = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7431643995028396"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash code evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From what we know hash codes should be uncorrelated, and should have mean = 0, for [-1,1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01554375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(hash_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitwise average: [ 0.1702  0.0202  0.0352 -0.0598 -0.1514 -0.0516 -0.0502 -0.1012  0.1544\n",
      "  0.0324  0.1452  0.1864  0.1134 -0.0082 -0.071  -0.0506  0.056   0.1292\n",
      "  0.0364 -0.0018  0.0374 -0.1174  0.0012 -0.0296  0.03   -0.0224 -0.0226\n",
      "  0.1506 -0.0198  0.0068 -0.1624  0.1124]\n"
     ]
    }
   ],
   "source": [
    "# Compute the bitwise average\n",
    "bitwise_avg = np.mean(hash_test, axis=0)\n",
    "\n",
    "print(\"Bitwise average:\", bitwise_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
