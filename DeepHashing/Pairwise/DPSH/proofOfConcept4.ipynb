{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../'\n",
    "sys.path.append(root)\n",
    "#from HelpfulFunctions.batchCreation import createBatch\n",
    "from HelpfulFunctions.metrics import meanAveragePrecision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "\n",
    "\n",
    "X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(root, num_classes, batch_size, train = True):\n",
    "    if train == True:\n",
    "        #Create X_train_tensor\n",
    "        X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "        X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "        #Create Y_train_tensor\n",
    "        y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    else:\n",
    "        X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "        X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "        y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "        y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return test_loader\n",
    "\n",
    "    #Missing implementation for Test and Validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CreateDataset(root, num_classes = 10, batch_size = 128)\n",
    "test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 32),    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "        # Initialize weights and biases from gaussian distribution\n",
    "        for layer in self.fc_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=0.01)  # Initialize weights based on paper\n",
    "                nn.init.normal_(layer.bias, mean=0.0, std=0.01)    # Initialize biases based on paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPSHLoss(torch.nn.Module):\n",
    "    def __init__(self, train_size, n_classes, bit):\n",
    "        super(DPSHLoss, self).__init__()\n",
    "        self.U = torch.zeros(train_size, bit).float().to(device)\n",
    "        self.Y = torch.zeros(train_size, n_classes).float().to(device)\n",
    "\n",
    "    def forward(self, u, y, ind, eta):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        s = (y @ self.Y.t() > 0).float()\n",
    "        inner_product = u @ self.U.t() * 0.5\n",
    "\n",
    "        likelihood_loss = (1 + (-(inner_product.abs())).exp()).log() + inner_product.clamp(min=0) - s * inner_product\n",
    "\n",
    "        likelihood_loss = likelihood_loss.mean()\n",
    "\n",
    "        quantization_loss = eta * (u - u.sign()).pow(2).mean()\n",
    "\n",
    "        return likelihood_loss + quantization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(device, train_loader, test_loader, train_size, test_size, batch_size, n_classes, bit, num_epoch, lr):\n",
    "\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=1e-5, weight_decay = 10 ** -5)\n",
    "\n",
    "    criterion = DPSHLoss(train_size, n_classes, bit)\n",
    "\n",
    "\n",
    "    #model.train()\n",
    "\n",
    "    #Best_mAP = 0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "        print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            \"DPSH\", epoch + 1, num_epoch, current_time, bit, \"CIFAR\"), end=\"\")\n",
    "\n",
    "        train_loss = 0\n",
    "        for image, label, ind in train_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            u = model(image)\n",
    "\n",
    "\n",
    "            loss = criterion(u, label.float(), ind, lr)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss / (train_size / batch_size)\n",
    "\n",
    "        print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))\n",
    "\n",
    "    return model\n",
    "        #if (epoch + 1) % 3 == 0:\n",
    "            #map = meanAveragePrecision(train_loader, test_loader, model, device)\n",
    "            #print(map)\n",
    "            #Best_mAP = validate(config, Best_mAP, test_loader, dataset_loader, net, bit, epoch, num_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPSH[ 1/150][10:19:17] bit:32, dataset:CIFAR, training... loss:0.74281\n",
      "DPSH[ 2/150][10:19:19] bit:32, dataset:CIFAR, training... loss:0.70507\n",
      "DPSH[ 3/150][10:19:20] bit:32, dataset:CIFAR, training... loss:0.66893\n",
      "DPSH[ 4/150][10:19:22] bit:32, dataset:CIFAR, training... loss:0.63625\n",
      "DPSH[ 5/150][10:19:25] bit:32, dataset:CIFAR, training... loss:0.60787\n",
      "DPSH[ 6/150][10:19:27] bit:32, dataset:CIFAR, training... loss:0.58551\n",
      "DPSH[ 7/150][10:19:29] bit:32, dataset:CIFAR, training... loss:0.56877\n",
      "DPSH[ 8/150][10:19:31] bit:32, dataset:CIFAR, training... loss:0.55604\n",
      "DPSH[ 9/150][10:19:32] bit:32, dataset:CIFAR, training... loss:0.54633\n",
      "DPSH[10/150][10:19:34] bit:32, dataset:CIFAR, training... loss:0.53826\n",
      "DPSH[11/150][10:19:37] bit:32, dataset:CIFAR, training... loss:0.53153\n",
      "DPSH[12/150][10:19:40] bit:32, dataset:CIFAR, training... loss:0.52562\n",
      "DPSH[13/150][10:19:43] bit:32, dataset:CIFAR, training... loss:0.52045\n",
      "DPSH[14/150][10:19:45] bit:32, dataset:CIFAR, training... loss:0.51568\n",
      "DPSH[15/150][10:19:47] bit:32, dataset:CIFAR, training... loss:0.51137\n",
      "DPSH[16/150][10:19:49] bit:32, dataset:CIFAR, training... loss:0.50740\n",
      "DPSH[17/150][10:19:51] bit:32, dataset:CIFAR, training... loss:0.50358\n",
      "DPSH[18/150][10:19:54] bit:32, dataset:CIFAR, training... loss:0.49984\n",
      "DPSH[19/150][10:19:57] bit:32, dataset:CIFAR, training... loss:0.49655\n",
      "DPSH[20/150][10:20:00] bit:32, dataset:CIFAR, training... loss:0.49339\n",
      "DPSH[21/150][10:20:02] bit:32, dataset:CIFAR, training... loss:0.49040\n",
      "DPSH[22/150][10:20:05] bit:32, dataset:CIFAR, training... loss:0.48752\n",
      "DPSH[23/150][10:20:08] bit:32, dataset:CIFAR, training... loss:0.48486\n",
      "DPSH[24/150][10:20:11] bit:32, dataset:CIFAR, training... loss:0.48237\n",
      "DPSH[25/150][10:20:13] bit:32, dataset:CIFAR, training... loss:0.47993\n",
      "DPSH[26/150][10:20:16] bit:32, dataset:CIFAR, training... loss:0.47742\n",
      "DPSH[27/150][10:20:19] bit:32, dataset:CIFAR, training... loss:0.47504\n",
      "DPSH[28/150][10:20:22] bit:32, dataset:CIFAR, training... loss:0.47271\n",
      "DPSH[29/150][10:20:24] bit:32, dataset:CIFAR, training... loss:0.47063\n",
      "DPSH[30/150][10:20:27] bit:32, dataset:CIFAR, training... loss:0.46834\n",
      "DPSH[31/150][10:20:29] bit:32, dataset:CIFAR, training... loss:0.46632\n",
      "DPSH[32/150][10:20:31] bit:32, dataset:CIFAR, training... loss:0.46428\n",
      "DPSH[33/150][10:20:33] bit:32, dataset:CIFAR, training... loss:0.46240\n",
      "DPSH[34/150][10:20:34] bit:32, dataset:CIFAR, training... loss:0.46021\n",
      "DPSH[35/150][10:20:36] bit:32, dataset:CIFAR, training... loss:0.45841\n",
      "DPSH[36/150][10:20:38] bit:32, dataset:CIFAR, training... loss:0.45655\n",
      "DPSH[37/150][10:20:39] bit:32, dataset:CIFAR, training... loss:0.45475\n",
      "DPSH[38/150][10:20:41] bit:32, dataset:CIFAR, training... loss:0.45295\n",
      "DPSH[39/150][10:20:44] bit:32, dataset:CIFAR, training... loss:0.45110\n",
      "DPSH[40/150][10:20:46] bit:32, dataset:CIFAR, training... loss:0.44940\n",
      "DPSH[41/150][10:20:47] bit:32, dataset:CIFAR, training... loss:0.44765\n",
      "DPSH[42/150][10:20:49] bit:32, dataset:CIFAR, training... loss:0.44618\n",
      "DPSH[43/150][10:20:50] bit:32, dataset:CIFAR, training... loss:0.44442\n",
      "DPSH[44/150][10:20:52] bit:32, dataset:CIFAR, training... loss:0.44276\n",
      "DPSH[45/150][10:20:54] bit:32, dataset:CIFAR, training... loss:0.44113\n",
      "DPSH[46/150][10:20:56] bit:32, dataset:CIFAR, training... loss:0.43960\n",
      "DPSH[47/150][10:20:59] bit:32, dataset:CIFAR, training... loss:0.43798\n",
      "DPSH[48/150][10:21:00] bit:32, dataset:CIFAR, training... loss:0.43656\n",
      "DPSH[49/150][10:21:02] bit:32, dataset:CIFAR, training... loss:0.43476\n",
      "DPSH[50/150][10:21:03] bit:32, dataset:CIFAR, training... loss:0.43346\n",
      "DPSH[51/150][10:21:05] bit:32, dataset:CIFAR, training... loss:0.43199\n",
      "DPSH[52/150][10:21:07] bit:32, dataset:CIFAR, training... loss:0.43044\n",
      "DPSH[53/150][10:21:09] bit:32, dataset:CIFAR, training... loss:0.42906\n",
      "DPSH[54/150][10:21:12] bit:32, dataset:CIFAR, training... loss:0.42740\n",
      "DPSH[55/150][10:21:13] bit:32, dataset:CIFAR, training... loss:0.42617\n",
      "DPSH[56/150][10:21:15] bit:32, dataset:CIFAR, training... loss:0.42464\n",
      "DPSH[57/150][10:21:16] bit:32, dataset:CIFAR, training... loss:0.42311\n",
      "DPSH[58/150][10:21:18] bit:32, dataset:CIFAR, training... loss:0.42175\n",
      "DPSH[59/150][10:21:20] bit:32, dataset:CIFAR, training... loss:0.42048\n",
      "DPSH[60/150][10:21:22] bit:32, dataset:CIFAR, training... loss:0.41900\n",
      "DPSH[61/150][10:21:25] bit:32, dataset:CIFAR, training... loss:0.41776\n",
      "DPSH[62/150][10:21:26] bit:32, dataset:CIFAR, training... loss:0.41659\n",
      "DPSH[63/150][10:21:28] bit:32, dataset:CIFAR, training... loss:0.41517\n",
      "DPSH[64/150][10:21:30] bit:32, dataset:CIFAR, training... loss:0.41408\n",
      "DPSH[65/150][10:21:31] bit:32, dataset:CIFAR, training... loss:0.41264\n",
      "DPSH[66/150][10:21:33] bit:32, dataset:CIFAR, training... loss:0.41127\n",
      "DPSH[67/150][10:21:34] bit:32, dataset:CIFAR, training... loss:0.40983\n",
      "DPSH[68/150][10:21:37] bit:32, dataset:CIFAR, training... loss:0.40865\n",
      "DPSH[69/150][10:21:39] bit:32, dataset:CIFAR, training... loss:0.40727\n",
      "DPSH[70/150][10:21:41] bit:32, dataset:CIFAR, training... loss:0.40627\n",
      "DPSH[71/150][10:21:42] bit:32, dataset:CIFAR, training... loss:0.40504\n",
      "DPSH[72/150][10:21:44] bit:32, dataset:CIFAR, training... loss:0.40364\n",
      "DPSH[73/150][10:21:46] bit:32, dataset:CIFAR, training... loss:0.40245\n",
      "DPSH[74/150][10:21:47] bit:32, dataset:CIFAR, training... loss:0.40134\n",
      "DPSH[75/150][10:21:49] bit:32, dataset:CIFAR, training... loss:0.40009\n",
      "DPSH[76/150][10:21:51] bit:32, dataset:CIFAR, training... loss:0.39896\n",
      "DPSH[77/150][10:21:52] bit:32, dataset:CIFAR, training... loss:0.39765\n",
      "DPSH[78/150][10:21:54] bit:32, dataset:CIFAR, training... loss:0.39642\n",
      "DPSH[79/150][10:21:55] bit:32, dataset:CIFAR, training... loss:0.39533\n",
      "DPSH[80/150][10:21:57] bit:32, dataset:CIFAR, training... loss:0.39423\n",
      "DPSH[81/150][10:21:58] bit:32, dataset:CIFAR, training... loss:0.39296\n",
      "DPSH[82/150][10:22:00] bit:32, dataset:CIFAR, training... loss:0.39203\n",
      "DPSH[83/150][10:22:02] bit:32, dataset:CIFAR, training... loss:0.39096\n",
      "DPSH[84/150][10:22:05] bit:32, dataset:CIFAR, training... loss:0.38950\n",
      "DPSH[85/150][10:22:08] bit:32, dataset:CIFAR, training... loss:0.38843\n",
      "DPSH[86/150][10:22:11] bit:32, dataset:CIFAR, training... loss:0.38744\n",
      "DPSH[87/150][10:22:14] bit:32, dataset:CIFAR, training... loss:0.38621\n",
      "DPSH[88/150][10:22:16] bit:32, dataset:CIFAR, training... loss:0.38523\n",
      "DPSH[89/150][10:22:19] bit:32, dataset:CIFAR, training... loss:0.38434\n",
      "DPSH[90/150][10:22:22] bit:32, dataset:CIFAR, training... loss:0.38320\n",
      "DPSH[91/150][10:22:24] bit:32, dataset:CIFAR, training... loss:0.38213\n",
      "DPSH[92/150][10:22:27] bit:32, dataset:CIFAR, training... loss:0.38086\n",
      "DPSH[93/150][10:22:30] bit:32, dataset:CIFAR, training... loss:0.37986\n",
      "DPSH[94/150][10:22:33] bit:32, dataset:CIFAR, training... loss:0.37887\n",
      "DPSH[95/150][10:22:35] bit:32, dataset:CIFAR, training... loss:0.37773\n",
      "DPSH[96/150][10:22:38] bit:32, dataset:CIFAR, training... loss:0.37671\n",
      "DPSH[97/150][10:22:40] bit:32, dataset:CIFAR, training... loss:0.37561\n",
      "DPSH[98/150][10:22:41] bit:32, dataset:CIFAR, training... loss:0.37486\n",
      "DPSH[99/150][10:22:43] bit:32, dataset:CIFAR, training... loss:0.37369\n",
      "DPSH[100/150][10:22:45] bit:32, dataset:CIFAR, training... loss:0.37266\n",
      "DPSH[101/150][10:22:46] bit:32, dataset:CIFAR, training... loss:0.37162\n",
      "DPSH[102/150][10:22:48] bit:32, dataset:CIFAR, training... loss:0.37048\n",
      "DPSH[103/150][10:22:49] bit:32, dataset:CIFAR, training... loss:0.36958\n",
      "DPSH[104/150][10:22:51] bit:32, dataset:CIFAR, training... loss:0.36856\n",
      "DPSH[105/150][10:22:54] bit:32, dataset:CIFAR, training... loss:0.36759\n",
      "DPSH[106/150][10:22:56] bit:32, dataset:CIFAR, training... loss:0.36656\n",
      "DPSH[107/150][10:22:58] bit:32, dataset:CIFAR, training... loss:0.36542\n",
      "DPSH[108/150][10:22:59] bit:32, dataset:CIFAR, training... loss:0.36444\n",
      "DPSH[109/150][10:23:01] bit:32, dataset:CIFAR, training... loss:0.36364\n",
      "DPSH[110/150][10:23:02] bit:32, dataset:CIFAR, training... loss:0.36256\n",
      "DPSH[111/150][10:23:04] bit:32, dataset:CIFAR, training... loss:0.36138\n",
      "DPSH[112/150][10:23:07] bit:32, dataset:CIFAR, training... loss:0.36071\n",
      "DPSH[113/150][10:23:09] bit:32, dataset:CIFAR, training... loss:0.35968\n",
      "DPSH[114/150][10:23:11] bit:32, dataset:CIFAR, training... loss:0.35878\n",
      "DPSH[115/150][10:23:12] bit:32, dataset:CIFAR, training... loss:0.35790\n",
      "DPSH[116/150][10:23:14] bit:32, dataset:CIFAR, training... loss:0.35703\n",
      "DPSH[117/150][10:23:15] bit:32, dataset:CIFAR, training... loss:0.35608\n",
      "DPSH[118/150][10:23:17] bit:32, dataset:CIFAR, training... loss:0.35497\n",
      "DPSH[119/150][10:23:20] bit:32, dataset:CIFAR, training... loss:0.35398\n",
      "DPSH[120/150][10:23:22] bit:32, dataset:CIFAR, training... loss:0.35317\n",
      "DPSH[121/150][10:23:24] bit:32, dataset:CIFAR, training... loss:0.35221\n",
      "DPSH[122/150][10:23:25] bit:32, dataset:CIFAR, training... loss:0.35145\n",
      "DPSH[123/150][10:23:27] bit:32, dataset:CIFAR, training... loss:0.35042\n",
      "DPSH[124/150][10:23:29] bit:32, dataset:CIFAR, training... loss:0.34927\n",
      "DPSH[125/150][10:23:30] bit:32, dataset:CIFAR, training... loss:0.34842\n",
      "DPSH[126/150][10:23:33] bit:32, dataset:CIFAR, training... loss:0.34745\n",
      "DPSH[127/150][10:23:35] bit:32, dataset:CIFAR, training... loss:0.34658\n",
      "DPSH[128/150][10:23:37] bit:32, dataset:CIFAR, training... loss:0.34569\n",
      "DPSH[129/150][10:23:38] bit:32, dataset:CIFAR, training... loss:0.34490\n",
      "DPSH[130/150][10:23:40] bit:32, dataset:CIFAR, training... loss:0.34397\n",
      "DPSH[131/150][10:23:42] bit:32, dataset:CIFAR, training... loss:0.34303\n",
      "DPSH[132/150][10:23:43] bit:32, dataset:CIFAR, training... loss:0.34216\n",
      "DPSH[133/150][10:23:45] bit:32, dataset:CIFAR, training... loss:0.34148\n",
      "DPSH[134/150][10:23:48] bit:32, dataset:CIFAR, training... loss:0.34059\n",
      "DPSH[135/150][10:23:50] bit:32, dataset:CIFAR, training... loss:0.33955\n",
      "DPSH[136/150][10:23:51] bit:32, dataset:CIFAR, training... loss:0.33890\n",
      "DPSH[137/150][10:23:53] bit:32, dataset:CIFAR, training... loss:0.33798\n",
      "DPSH[138/150][10:23:55] bit:32, dataset:CIFAR, training... loss:0.33693\n",
      "DPSH[139/150][10:23:56] bit:32, dataset:CIFAR, training... loss:0.33603\n",
      "DPSH[140/150][10:23:58] bit:32, dataset:CIFAR, training... loss:0.33544\n",
      "DPSH[141/150][10:24:00] bit:32, dataset:CIFAR, training... loss:0.33438\n",
      "DPSH[142/150][10:24:01] bit:32, dataset:CIFAR, training... loss:0.33355\n",
      "DPSH[143/150][10:24:03] bit:32, dataset:CIFAR, training... loss:0.33289\n",
      "DPSH[144/150][10:24:04] bit:32, dataset:CIFAR, training... loss:0.33184\n",
      "DPSH[145/150][10:24:06] bit:32, dataset:CIFAR, training... loss:0.33092\n",
      "DPSH[146/150][10:24:08] bit:32, dataset:CIFAR, training... loss:0.33045\n",
      "DPSH[147/150][10:24:09] bit:32, dataset:CIFAR, training... loss:0.32952\n",
      "DPSH[148/150][10:24:11] bit:32, dataset:CIFAR, training... loss:0.32863\n",
      "DPSH[149/150][10:24:14] bit:32, dataset:CIFAR, training... loss:0.32808\n",
      "DPSH[150/150][10:24:16] bit:32, dataset:CIFAR, training... loss:0.32698\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_val(device, train_loader, test_loader, train_size = 45000, test_size = 10000, batch_size = 128, n_classes = 10, bit = 32, num_epoch = 150, lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trained_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_train = (trained_model(X_train_tensor)).sign()\n",
    "hash_train = hash_train.detach().numpy()\n",
    "hash_test = (trained_model(X_test_tensor)).sign()\n",
    "hash_test = hash_test.detach().numpy()\n",
    "y_train\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:25<00:00, 116.32it/s]\n"
     ]
    }
   ],
   "source": [
    "map = meanAveragePrecision(training_hashes = hash_train, test_hashes = hash_test, test_labels = y_test, training_labels = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854891885459112"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
