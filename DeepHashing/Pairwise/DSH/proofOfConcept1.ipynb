{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../../'\n",
    "sys.path.append(root)\n",
    "from HelpfulFunctions.batchCreation import createBatch\n",
    "from HelpfulFunctions.metrics import meanAveragePrecision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "\n",
    "\n",
    "X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(root, num_classes, batch_size, train = True):\n",
    "    if train == True:\n",
    "        #Create X_train_tensor\n",
    "        X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "        X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "        #Create Y_train_tensor\n",
    "        y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    else:\n",
    "        X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "        X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "        y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "        y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return test_loader\n",
    "\n",
    "    #Missing implementation for Test and Validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CreateDataset(root, num_classes = 10, batch_size = 128)\n",
    "test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 48),    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSHLoss(torch.nn.Module):\n",
    "    def __init__(self, train_size, n_classes, bit):\n",
    "        super(DSHLoss, self).__init__()\n",
    "        self.m = 2 * bit\n",
    "        self.U = torch.zeros(train_size, bit).float().to(device)\n",
    "        self.Y = torch.zeros(train_size, n_classes).float().to(device)\n",
    "\n",
    "    def forward(self, u, y, ind, eta):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        dist = (u.unsqueeze(1) - self.U.unsqueeze(0)).pow(2).sum(dim=2)\n",
    "        y = (y @ self.Y.t() == 0).float()\n",
    "\n",
    "        loss = (1 - y) / 2 * dist + y / 2 * (self.m - dist).clamp(min=0)\n",
    "        loss1 = loss.mean()\n",
    "        loss2 = eta * (1 - u.abs()).abs().mean()\n",
    "\n",
    "        return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(device, train_loader, train_size, batch_size, n_classes, bit, num_epoch, eta):\n",
    "\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr = 1e-5, weight_decay = 10 ** -5)\n",
    "\n",
    "    criterion = DSHLoss(train_size, n_classes, bit)\n",
    "\n",
    "\n",
    "    #model.train()\n",
    "\n",
    "    #Best_mAP = 0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "        print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            \"DSH\", epoch + 1, num_epoch, current_time, bit, \"CIFAR\"), end=\"\")\n",
    "\n",
    "        train_loss = 0\n",
    "        for image, label, ind in train_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            u = model(image)\n",
    "\n",
    "\n",
    "            loss = criterion(u, label.float(), ind, eta)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss / (train_size / batch_size)\n",
    "\n",
    "        print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))\n",
    "\n",
    "        #if (epoch + 1) % config[\"test_map\"] == 0:\n",
    "            #Best_mAP = validate(config, Best_mAP, test_loader, dataset_loader, net, bit, epoch, num_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSH[ 1/150][10:25:16] bit:48, dataset:CIFAR, training... loss:4.33965\n",
      "DSH[ 2/150][10:25:32] bit:48, dataset:CIFAR, training... loss:6.10590\n",
      "DSH[ 3/150][10:25:48] bit:48, dataset:CIFAR, training... loss:5.43099\n",
      "DSH[ 4/150][10:26:03] bit:48, dataset:CIFAR, training... loss:5.13345\n",
      "DSH[ 5/150][10:26:17] bit:48, dataset:CIFAR, training... loss:4.93343\n",
      "DSH[ 6/150][10:26:31] bit:48, dataset:CIFAR, training... loss:4.77908\n",
      "DSH[ 7/150][10:26:46] bit:48, dataset:CIFAR, training... loss:4.65768\n",
      "DSH[ 8/150][10:27:00] bit:48, dataset:CIFAR, training... loss:4.55718\n",
      "DSH[ 9/150][10:27:14] bit:48, dataset:CIFAR, training... loss:4.47224\n",
      "DSH[10/150][10:27:28] bit:48, dataset:CIFAR, training... loss:4.39560\n",
      "DSH[11/150][10:27:42] bit:48, dataset:CIFAR, training... loss:4.32832\n",
      "DSH[12/150][10:27:57] bit:48, dataset:CIFAR, training... loss:4.26781\n",
      "DSH[13/150][10:28:12] bit:48, dataset:CIFAR, training... loss:4.21239\n",
      "DSH[14/150][10:28:27] bit:48, dataset:CIFAR, training... loss:4.16178\n",
      "DSH[15/150][10:28:41] bit:48, dataset:CIFAR, training... loss:4.11396\n",
      "DSH[16/150][10:28:56] bit:48, dataset:CIFAR, training... loss:4.07016\n",
      "DSH[17/150][10:29:10] bit:48, dataset:CIFAR, training... loss:4.02883\n",
      "DSH[18/150][10:29:24] bit:48, dataset:CIFAR, training... loss:3.98944\n",
      "DSH[19/150][10:29:38] bit:48, dataset:CIFAR, training... loss:3.95274\n",
      "DSH[20/150][10:29:54] bit:48, dataset:CIFAR, training... loss:3.91855\n",
      "DSH[21/150][10:30:10] bit:48, dataset:CIFAR, training... loss:3.88407\n",
      "DSH[22/150][10:30:25] bit:48, dataset:CIFAR, training... loss:3.85302\n",
      "DSH[23/150][10:30:41] bit:48, dataset:CIFAR, training... loss:3.82216\n",
      "DSH[24/150][10:30:57] bit:48, dataset:CIFAR, training... loss:3.79258\n",
      "DSH[25/150][10:31:14] bit:48, dataset:CIFAR, training... loss:3.76400\n",
      "DSH[26/150][10:31:30] bit:48, dataset:CIFAR, training... loss:3.73777\n",
      "DSH[27/150][10:31:46] bit:48, dataset:CIFAR, training... loss:3.71202\n",
      "DSH[28/150][10:32:02] bit:48, dataset:CIFAR, training... loss:3.68708\n",
      "DSH[29/150][10:32:18] bit:48, dataset:CIFAR, training... loss:3.66225\n",
      "DSH[30/150][10:32:35] bit:48, dataset:CIFAR, training... loss:3.63723\n",
      "DSH[31/150][10:32:51] bit:48, dataset:CIFAR, training... loss:3.61527\n",
      "DSH[32/150][10:33:07] bit:48, dataset:CIFAR, training... loss:3.59371\n",
      "DSH[33/150][10:33:24] bit:48, dataset:CIFAR, training... loss:3.57235\n",
      "DSH[34/150][10:33:46] bit:48, dataset:CIFAR, training... loss:3.55207\n",
      "DSH[35/150][10:34:06] bit:48, dataset:CIFAR, training... loss:3.53020\n",
      "DSH[36/150][10:34:26] bit:48, dataset:CIFAR, training... loss:3.50999\n",
      "DSH[37/150][10:34:47] bit:48, dataset:CIFAR, training... loss:3.49032\n",
      "DSH[38/150][10:35:05] bit:48, dataset:CIFAR, training... loss:3.47091\n",
      "DSH[39/150][10:35:22] bit:48, dataset:CIFAR, training... loss:3.45360\n",
      "DSH[40/150][10:35:40] bit:48, dataset:CIFAR, training... loss:3.43493\n",
      "DSH[41/150][10:35:57] bit:48, dataset:CIFAR, training... loss:3.41747\n",
      "DSH[42/150][10:36:14] bit:48, dataset:CIFAR, training... loss:3.40029\n",
      "DSH[43/150][10:36:46] bit:48, dataset:CIFAR, training... loss:3.38467\n",
      "DSH[44/150][10:37:29] bit:48, dataset:CIFAR, training... loss:3.36637\n",
      "DSH[45/150][10:37:46] bit:48, dataset:CIFAR, training... loss:3.35016\n",
      "DSH[46/150][10:38:03] bit:48, dataset:CIFAR, training... loss:3.33468\n",
      "DSH[47/150][10:38:21] bit:48, dataset:CIFAR, training... loss:3.31861\n",
      "DSH[48/150][10:38:39] bit:48, dataset:CIFAR, training... loss:3.30259\n",
      "DSH[49/150][10:38:58] bit:48, dataset:CIFAR, training... loss:3.28746\n",
      "DSH[50/150][10:39:16] bit:48, dataset:CIFAR, training... loss:3.27323\n",
      "DSH[51/150][10:39:34] bit:48, dataset:CIFAR, training... loss:3.25857\n",
      "DSH[52/150][10:39:52] bit:48, dataset:CIFAR, training... loss:3.24333\n",
      "DSH[53/150][10:40:10] bit:48, dataset:CIFAR, training... loss:3.23136\n",
      "DSH[54/150][10:40:27] bit:48, dataset:CIFAR, training... loss:3.21670\n",
      "DSH[55/150][10:40:45] bit:48, dataset:CIFAR, training... loss:3.20221\n",
      "DSH[56/150][10:41:04] bit:48, dataset:CIFAR, training... loss:3.19058\n",
      "DSH[57/150][10:41:21] bit:48, dataset:CIFAR, training... loss:3.17733\n",
      "DSH[58/150][10:41:41] bit:48, dataset:CIFAR, training... loss:3.16198\n",
      "DSH[59/150][10:42:01] bit:48, dataset:CIFAR, training... loss:3.15029\n",
      "DSH[60/150][10:42:20] bit:48, dataset:CIFAR, training... loss:3.13889\n",
      "DSH[61/150][10:42:38] bit:48, dataset:CIFAR, training... loss:3.12482\n",
      "DSH[62/150][10:42:55] bit:48, dataset:CIFAR, training... loss:3.11347\n",
      "DSH[63/150][10:43:13] bit:48, dataset:CIFAR, training... loss:3.10232\n",
      "DSH[64/150][10:43:31] bit:48, dataset:CIFAR, training... loss:3.08986\n",
      "DSH[65/150][10:43:49] bit:48, dataset:CIFAR, training... loss:3.07682\n",
      "DSH[66/150][10:44:07] bit:48, dataset:CIFAR, training... loss:3.06785\n",
      "DSH[67/150][10:44:25] bit:48, dataset:CIFAR, training... loss:3.05390\n",
      "DSH[68/150][10:44:43] bit:48, dataset:CIFAR, training... loss:3.04331\n",
      "DSH[69/150][10:45:02] bit:48, dataset:CIFAR, training... loss:3.03229\n",
      "DSH[70/150][10:45:20] bit:48, dataset:CIFAR, training... loss:3.02196\n",
      "DSH[71/150][10:45:38] bit:48, dataset:CIFAR, training... loss:3.01053\n",
      "DSH[72/150][10:45:56] bit:48, dataset:CIFAR, training... loss:3.00114\n",
      "DSH[73/150][10:46:14] bit:48, dataset:CIFAR, training... loss:2.98873\n",
      "DSH[74/150][10:46:32] bit:48, dataset:CIFAR, training... loss:2.97814\n",
      "DSH[75/150][10:46:50] bit:48, dataset:CIFAR, training... loss:2.96789\n",
      "DSH[76/150][10:47:07] bit:48, dataset:CIFAR, training... loss:2.95978\n",
      "DSH[77/150][10:47:24] bit:48, dataset:CIFAR, training... loss:2.94746\n",
      "DSH[78/150][10:47:43] bit:48, dataset:CIFAR, training... loss:2.93814\n",
      "DSH[79/150][10:48:01] bit:48, dataset:CIFAR, training... loss:2.92875\n",
      "DSH[80/150][10:48:19] bit:48, dataset:CIFAR, training... loss:2.91791\n",
      "DSH[81/150][10:48:37] bit:48, dataset:CIFAR, training... loss:2.90781\n",
      "DSH[82/150][10:48:55] bit:48, dataset:CIFAR, training... loss:2.89850\n",
      "DSH[83/150][10:49:13] bit:48, dataset:CIFAR, training... loss:2.88930\n",
      "DSH[84/150][10:53:45] bit:48, dataset:CIFAR, training... loss:2.87869\n",
      "DSH[85/150][10:54:33] bit:48, dataset:CIFAR, training... loss:2.86990\n",
      "DSH[86/150][10:55:39] bit:48, dataset:CIFAR, training... loss:2.86082\n",
      "DSH[87/150][10:56:41] bit:48, dataset:CIFAR, training... loss:2.85027\n",
      "DSH[88/150][10:57:17] bit:48, dataset:CIFAR, training... loss:2.84182\n",
      "DSH[89/150][10:57:36] bit:48, dataset:CIFAR, training... loss:2.83367\n",
      "DSH[90/150][10:57:54] bit:48, dataset:CIFAR, training... loss:2.82387\n",
      "DSH[91/150][10:58:11] bit:48, dataset:CIFAR, training... loss:2.81552\n",
      "DSH[92/150][10:58:28] bit:48, dataset:CIFAR, training... loss:2.80555\n",
      "DSH[93/150][10:58:46] bit:48, dataset:CIFAR, training... loss:2.79908\n",
      "DSH[94/150][10:59:04] bit:48, dataset:CIFAR, training... loss:2.78806\n",
      "DSH[95/150][10:59:22] bit:48, dataset:CIFAR, training... loss:2.78127\n",
      "DSH[96/150][10:59:41] bit:48, dataset:CIFAR, training... loss:2.77110\n",
      "DSH[97/150][10:59:58] bit:48, dataset:CIFAR, training... loss:2.76272\n",
      "DSH[98/150][11:00:16] bit:48, dataset:CIFAR, training... loss:2.75511\n",
      "DSH[99/150][11:00:33] bit:48, dataset:CIFAR, training... loss:2.74726\n",
      "DSH[100/150][11:00:51] bit:48, dataset:CIFAR, training... loss:2.73742\n",
      "DSH[101/150][11:01:08] bit:48, dataset:CIFAR, training... loss:2.73014\n",
      "DSH[102/150][11:01:26] bit:48, dataset:CIFAR, training... loss:2.71941\n",
      "DSH[103/150][11:01:43] bit:48, dataset:CIFAR, training... loss:2.70993\n",
      "DSH[104/150][11:02:01] bit:48, dataset:CIFAR, training... loss:2.70403\n",
      "DSH[105/150][11:02:20] bit:48, dataset:CIFAR, training... loss:2.69516\n",
      "DSH[106/150][11:02:38] bit:48, dataset:CIFAR, training... loss:2.68707\n",
      "DSH[107/150][11:02:56] bit:48, dataset:CIFAR, training... loss:2.68054\n",
      "DSH[108/150][11:03:14] bit:48, dataset:CIFAR, training... loss:2.67226\n",
      "DSH[109/150][11:03:33] bit:48, dataset:CIFAR, training... loss:2.66374\n",
      "DSH[110/150][11:03:51] bit:48, dataset:CIFAR, training... loss:2.65736\n",
      "DSH[111/150][11:04:09] bit:48, dataset:CIFAR, training... loss:2.65024\n",
      "DSH[112/150][11:04:27] bit:48, dataset:CIFAR, training... loss:2.64085\n",
      "DSH[113/150][11:04:44] bit:48, dataset:CIFAR, training... loss:2.63223\n",
      "DSH[114/150][11:05:00] bit:48, dataset:CIFAR, training... loss:2.62693\n",
      "DSH[115/150][11:05:15] bit:48, dataset:CIFAR, training... loss:2.61886\n",
      "DSH[116/150][11:05:29] bit:48, dataset:CIFAR, training... loss:2.61075\n",
      "DSH[117/150][11:05:43] bit:48, dataset:CIFAR, training... loss:2.60220\n",
      "DSH[118/150][11:05:57] bit:48, dataset:CIFAR, training... loss:2.59592\n",
      "DSH[119/150][11:06:11] bit:48, dataset:CIFAR, training... loss:2.59043\n",
      "DSH[120/150][11:06:25] bit:48, dataset:CIFAR, training... loss:2.57997\n",
      "DSH[121/150][11:06:39] bit:48, dataset:CIFAR, training... loss:2.57329\n",
      "DSH[122/150][11:06:53] bit:48, dataset:CIFAR, training... loss:2.56706\n",
      "DSH[123/150][11:07:07] bit:48, dataset:CIFAR, training... loss:2.55888\n",
      "DSH[124/150][11:07:21] bit:48, dataset:CIFAR, training... loss:2.55323\n",
      "DSH[125/150][11:07:35] bit:48, dataset:CIFAR, training... loss:2.54639\n",
      "DSH[126/150][11:07:50] bit:48, dataset:CIFAR, training... loss:2.53958\n",
      "DSH[127/150][11:08:05] bit:48, dataset:CIFAR, training... loss:2.53152\n",
      "DSH[128/150][11:08:21] bit:48, dataset:CIFAR, training... loss:2.52179\n",
      "DSH[129/150][11:08:36] bit:48, dataset:CIFAR, training... loss:2.51742\n",
      "DSH[130/150][11:08:52] bit:48, dataset:CIFAR, training... loss:2.50896\n",
      "DSH[131/150][11:09:07] bit:48, dataset:CIFAR, training... loss:2.50364\n",
      "DSH[132/150][11:09:22] bit:48, dataset:CIFAR, training... loss:2.49480\n",
      "DSH[133/150][11:09:37] bit:48, dataset:CIFAR, training... loss:2.48878\n",
      "DSH[134/150][11:09:53] bit:48, dataset:CIFAR, training... loss:2.48210\n",
      "DSH[135/150][11:10:08] bit:48, dataset:CIFAR, training... loss:2.47584\n",
      "DSH[136/150][11:10:23] bit:48, dataset:CIFAR, training... loss:2.46985\n",
      "DSH[137/150][11:10:38] bit:48, dataset:CIFAR, training... loss:2.46158\n",
      "DSH[138/150][11:10:53] bit:48, dataset:CIFAR, training... loss:2.45597\n",
      "DSH[139/150][11:11:08] bit:48, dataset:CIFAR, training... loss:2.45032\n",
      "DSH[140/150][11:11:23] bit:48, dataset:CIFAR, training... loss:2.44501\n",
      "DSH[141/150][11:11:38] bit:48, dataset:CIFAR, training... loss:2.43601\n",
      "DSH[142/150][11:11:53] bit:48, dataset:CIFAR, training... loss:2.42806\n",
      "DSH[143/150][11:12:08] bit:48, dataset:CIFAR, training... loss:2.42221\n",
      "DSH[144/150][11:12:24] bit:48, dataset:CIFAR, training... loss:2.41611\n",
      "DSH[145/150][11:12:39] bit:48, dataset:CIFAR, training... loss:2.40930\n",
      "DSH[146/150][11:12:55] bit:48, dataset:CIFAR, training... loss:2.40478\n",
      "DSH[147/150][11:13:10] bit:48, dataset:CIFAR, training... loss:2.39723\n",
      "DSH[148/150][11:13:26] bit:48, dataset:CIFAR, training... loss:2.39088\n",
      "DSH[149/150][11:13:41] bit:48, dataset:CIFAR, training... loss:2.38654\n",
      "DSH[150/150][11:13:56] bit:48, dataset:CIFAR, training... loss:2.37958\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_val(device, train_loader, train_size = 45000, batch_size = 128, n_classes = 10, bit = 48, num_epoch = 150, eta = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = model.cpu()\n",
    "hash_train = (trained_model(X_train_tensor)).sign()\n",
    "hash_train = hash_train.detach().numpy()\n",
    "hash_test = (trained_model(X_test_tensor)).sign()\n",
    "hash_test = hash_test.detach().numpy()\n",
    "y_train\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:00<00:00, 82.65it/s]\n"
     ]
    }
   ],
   "source": [
    "map = meanAveragePrecision(training_hashes = hash_train, test_hashes = hash_test, test_labels = y_test, training_labels = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8482320072978633"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
