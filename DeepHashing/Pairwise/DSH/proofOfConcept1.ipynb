{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../../'\n",
    "sys.path.append(root)\n",
    "from HelpfulFunctions.batchCreation import createBatch\n",
    "from HelpfulFunctions.metrics_final import meanAveragePrecisionOptimized\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(root, num_classes, batch_size, train = True):\n",
    "    if train == True:\n",
    "        #Create X_train_tensor\n",
    "        X_train = np.load( root + \"Features/X_hpo_Img.npy\" ) # Shape = (45000, 4096)\n",
    "        X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "        #Create Y_train_tensor\n",
    "        y_train = np.load( root + \"Features/y_hpo_Img.npy\" ) # Shape = (45000,)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_train_tensor = torch.nn.functional.one_hot(y_train_tensor)#, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    else:\n",
    "        X_test = np.load( root + \"Features/X_val_Img.npy\" ) # Shape = (10000, 4096)\n",
    "        X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "        y_test = np.load( root + \"Features/y_val_Img.npy\" ) # Shape = (10000,)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "        y_test_tensor = torch.nn.functional.one_hot(y_test_tensor)#, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return test_loader\n",
    "\n",
    "    #Missing implementation for Test and Validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CreateDataset(root, num_classes = 10, batch_size = 128)\n",
    "test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, bits):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, bits),    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSHLoss(torch.nn.Module):\n",
    "    def __init__(self, train_size, n_classes, bit):\n",
    "        super(DSHLoss, self).__init__()\n",
    "        self.m = 2 * bit\n",
    "        self.U = torch.zeros(train_size, bit).float().to(device)\n",
    "        self.Y = torch.zeros(train_size, n_classes).float().to(device)\n",
    "\n",
    "    def forward(self, u, y, ind, eta):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        dist = (u.unsqueeze(1) - self.U.unsqueeze(0)).pow(2).sum(dim=2)\n",
    "        y = (y @ self.Y.t() == 0).float()\n",
    "\n",
    "        loss = (1 - y) / 2 * dist + y / 2 * (self.m - dist).clamp(min=0)\n",
    "        loss1 = loss.mean()\n",
    "        loss2 = eta * (1 - u.abs()).abs().mean()\n",
    "\n",
    "        return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid\n",
    "\n",
    "def DSH(device: torch.device, train_size: int, n_classes: int, bit: int, num_epoch: int, batch_size: int, eta_values: list, wd_values: list, lr_values: list):\n",
    "\n",
    "    train_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 1)\n",
    "    #test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 2)\n",
    "    validation_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "        'eta': eta_values,\n",
    "        'learning_rate': lr_values,\n",
    "        #'batch_size': [16, 32, 64],\n",
    "        'weight_decay': wd_values\n",
    "    }\n",
    "\n",
    "    customLoss = DSHLoss(train_size, n_classes, bit)\n",
    "\n",
    "\n",
    "    # Get all combinations of parameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    parameter_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # Evaluate each parameter combination\n",
    "    best_params = None\n",
    "    best_map = 0\n",
    "\n",
    "    for params in parameter_combinations:\n",
    "        print(f\"Testing combination: {params}\")\n",
    "    \n",
    "        # Initialize loss function with specific parameters\n",
    "        #loss_fn = customLoss()\n",
    "\n",
    "        # Initialize model and optimizer\n",
    "        model = CustomNN(bits = bit).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay = params['weight_decay'])\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epoch):  # Example epoch count\n",
    "            current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "            print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            \"DPSH\", epoch + 1, num_epoch, current_time, bit, \"CIFAR\"), end=\"\")\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for image, label, ind in train_loader:\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                u = model(image)\n",
    "\n",
    "                loss = customLoss(u, label.float(), ind, eta = params['eta'])\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss = train_loss / (train_size / batch_size)\n",
    "            print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        hash_train = []\n",
    "        label_train = []\n",
    "\n",
    "        hash_val = []\n",
    "        label_val = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, label, ind in train_loader:\n",
    "                image = image.to(device)\n",
    "                label_batch = label.to(device)\n",
    "                hash_train_batch = (model(image)).sign()\n",
    "                hash_train.append(hash_train_batch)\n",
    "                label_train.append(label_batch)\n",
    "                #hash_codes_matrix = np.vstack(hash_code_batches)\n",
    "                #print(hash_codes_matrix.shape) \n",
    "\n",
    "            hash_train = torch.cat(hash_train, dim = 0).cpu().numpy()\n",
    "            label_train = torch.cat(label_train, dim = 0).cpu().numpy()\n",
    "\n",
    "            \n",
    "            for image, label, ind in validation_loader:\n",
    "                image = image.to(device)\n",
    "                label_batch = label.to(device)\n",
    "                hash_val_batch = (model(image)).sign()\n",
    "                hash_val.append(hash_val_batch)\n",
    "                label_val.append(label_batch)\n",
    "            hash_val = torch.cat(hash_val, dim = 0).cpu().numpy()\n",
    "            label_val = torch.cat(label_val, dim = 0).cpu().numpy()\n",
    "\n",
    "            map = meanAveragePrecisionOptimized(training_hashes = hash_train, training_labels = label_train, test_hashes = hash_val, test_labels = label_val)\n",
    "\n",
    "            print(f\"Validation mAP: {map:.4f}\")\n",
    "\n",
    "            # Update best parameters\n",
    "            if map > best_map:\n",
    "                best_map = map\n",
    "                best_params = params\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best mAP:\", best_map)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination: {'eta': 0.01, 'learning_rate': 1e-06, 'weight_decay': 1e-06}\n",
      "DPSH[ 1/150][08:27:37] bit:32, dataset:CIFAR, training... loss:12.15274\n",
      "DPSH[ 2/150][08:27:52] bit:32, dataset:CIFAR, training...."
     ]
    }
   ],
   "source": [
    "#DSH12Img = DSH(device, 50000, 1001, 12, 150, 128, [0.01, 0.05, 0.1], [1e-6, 1e-5, 1e-4], [1e-6, 1e-5, 1e-4])\n",
    "#DSH24Img = DSH(device, 50000, 1001, 24, 150, 128, [0.01, 0.05, 0.1], [1e-6, 1e-5, 1e-4], [1e-6, 1e-5, 1e-4])\n",
    "DSH32Img = DSH(device, 50000, 1001, 32, 150, 128, [0.01, 0.05, 0.1], [1e-6, 1e-5, 1e-4], [1e-6, 1e-5, 1e-4])\n",
    "#DSH48Img = DSH(device, 50000, 1001, 48, 150, 128, [0.01, 0.05, 0.1], [1e-6, 1e-5, 1e-4], [1e-6, 1e-5, 1e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSH12Cifar = {'eta': 0.05, 'learning_rate': 0.0001, 'weight_decay': 0.0001}\n",
    "DSH24Cifar = {'eta': 0.1, 'learning_rate': 0.0001, 'weight_decay': 1e-05}\n",
    "DSH32Cifar = {'eta': 0.1, 'learning_rate': 0.0001, 'weight_decay': 0.0001}\n",
    "DSH48Cifar = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSH12Nus = {'eta': 0.05, 'learning_rate': 0.0001, 'weight_decay': 1e-05}\n",
    "DSH24Nus = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}\n",
    "DSH32Nus = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}\n",
    "DSH48Nus = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSH12Img = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}\n",
    "DSH24Img = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
