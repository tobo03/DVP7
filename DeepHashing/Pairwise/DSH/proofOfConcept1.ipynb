{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../../'\n",
    "sys.path.append(root)\n",
    "from HelpfulFunctions.batchCreation import createBatch\n",
    "from HelpfulFunctions.metrics import meanAveragePrecision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(root, num_classes, batch_size, train = True):\n",
    "    if train == True:\n",
    "        #Create X_train_tensor\n",
    "        X_train = np.load( root + \"Features/X_hpo_Img.npy\" ) # Shape = (45000, 4096)\n",
    "        X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "        #Create Y_train_tensor\n",
    "        y_train = np.load( root + \"Features/y_hpo_Img.npy\" ) # Shape = (45000,)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_train_tensor = torch.nn.functional.one_hot(y_train_tensor)#, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    else:\n",
    "        X_test = np.load( root + \"Features/X_val_Img.npy\" ) # Shape = (10000, 4096)\n",
    "        X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "        y_test = np.load( root + \"Features/y_val_Img.npy\" ) # Shape = (10000,)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "        y_test_tensor = torch.nn.functional.one_hot(y_test_tensor)#, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return test_loader\n",
    "\n",
    "    #Missing implementation for Test and Validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CreateDataset(root, num_classes = 10, batch_size = 128)\n",
    "test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self, bits):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, bits),    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSHLoss(torch.nn.Module):\n",
    "    def __init__(self, train_size, n_classes, bit):\n",
    "        super(DSHLoss, self).__init__()\n",
    "        self.m = 2 * bit\n",
    "        self.U = torch.zeros(train_size, bit).float().to(device)\n",
    "        self.Y = torch.zeros(train_size, n_classes).float().to(device)\n",
    "\n",
    "    def forward(self, u, y, ind, eta):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        dist = (u.unsqueeze(1) - self.U.unsqueeze(0)).pow(2).sum(dim=2)\n",
    "        y = (y @ self.Y.t() == 0).float()\n",
    "\n",
    "        loss = (1 - y) / 2 * dist + y / 2 * (self.m - dist).clamp(min=0)\n",
    "        loss1 = loss.mean()\n",
    "        loss2 = eta * (1 - u.abs()).abs().mean()\n",
    "\n",
    "        return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the grid\n",
    "\n",
    "def DSH(device: torch.device, train_size: int, n_classes: int, bit: int, num_epoch: int, batch_size: int, eta_values: list, wd_values: list, lr_values: list):\n",
    "\n",
    "    train_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 1)\n",
    "    #test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 2)\n",
    "    validation_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    param_grid = {\n",
    "        'eta': eta_values,\n",
    "        'learning_rate': lr_values,\n",
    "        #'batch_size': [16, 32, 64],\n",
    "        'weight_decay': wd_values\n",
    "    }\n",
    "\n",
    "    customLoss = DSHLoss(train_size, n_classes, bit)\n",
    "\n",
    "\n",
    "    # Get all combinations of parameters\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    parameter_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    # Evaluate each parameter combination\n",
    "    best_params = None\n",
    "    best_map = 0\n",
    "\n",
    "    for params in parameter_combinations:\n",
    "        print(f\"Testing combination: {params}\")\n",
    "    \n",
    "        # Initialize loss function with specific parameters\n",
    "        #loss_fn = customLoss()\n",
    "\n",
    "        # Initialize model and optimizer\n",
    "        model = CustomNN(bits = bit).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay = params['weight_decay'])\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epoch):  # Example epoch count\n",
    "            current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "            print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            \"DPSH\", epoch + 1, num_epoch, current_time, bit, \"CIFAR\"), end=\"\")\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for image, label, ind in train_loader:\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                u = model(image)\n",
    "\n",
    "                loss = customLoss(u, label.float(), ind, eta = params['eta'])\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss = train_loss / (train_size / batch_size)\n",
    "            print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        hash_train = []\n",
    "        label_train = []\n",
    "\n",
    "        hash_val = []\n",
    "        label_val = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for image, label, ind in train_loader:\n",
    "                image = image.to(device)\n",
    "                label_batch = label.to(device)\n",
    "                hash_train_batch = (model(image)).sign()\n",
    "                hash_train.append(hash_train_batch)\n",
    "                label_train.append(label_batch)\n",
    "                #hash_codes_matrix = np.vstack(hash_code_batches)\n",
    "                #print(hash_codes_matrix.shape) \n",
    "\n",
    "            hash_train = torch.cat(hash_train, dim = 0).cpu().numpy()\n",
    "            label_train = torch.cat(label_train, dim = 0).cpu().numpy()\n",
    "\n",
    "            \n",
    "            for image, label, ind in validation_loader:\n",
    "                image = image.to(device)\n",
    "                label_batch = label.to(device)\n",
    "                hash_val_batch = (model(image)).sign()\n",
    "                hash_val.append(hash_val_batch)\n",
    "                label_val.append(label_batch)\n",
    "            hash_val = torch.cat(hash_val, dim = 0).cpu().numpy()\n",
    "            label_val = torch.cat(label_val, dim = 0).cpu().numpy()\n",
    "\n",
    "            map = meanAveragePrecision(training_hashes = hash_train, training_labels = label_train, test_hashes = hash_val, test_labels = label_val)\n",
    "\n",
    "            print(f\"Validation mAP: {map:.4f}\")\n",
    "\n",
    "            # Update best parameters\n",
    "            if map > best_map:\n",
    "                best_map = map\n",
    "                best_params = params\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best mAP:\", best_map)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination: {'eta': 0.01, 'learning_rate': 1e-06, 'weight_decay': 1e-06}\n",
      "DPSH[ 1/150][01:07:54] bit:24, dataset:CIFAR, training... loss:8.17268\n",
      "DPSH[ 2/150][01:08:05] bit:24, dataset:CIFAR, training... loss:2.61797\n",
      "DPSH[ 3/150][01:08:16] bit:24, dataset:CIFAR, training... loss:1.61480\n",
      "DPSH[ 4/150][01:08:27] bit:24, dataset:CIFAR, training... loss:1.15319\n",
      "DPSH[ 5/150][01:08:37] bit:24, dataset:CIFAR, training... loss:0.90195\n",
      "DPSH[ 6/150][01:08:48] bit:24, dataset:CIFAR, training... loss:0.67892\n",
      "DPSH[ 7/150][01:08:59] bit:24, dataset:CIFAR, training... loss:0.53926\n",
      "DPSH[ 8/150][01:09:09] bit:24, dataset:CIFAR, training... loss:0.44943\n",
      "DPSH[ 9/150][01:09:20] bit:24, dataset:CIFAR, training... loss:0.38534\n",
      "DPSH[10/150][01:09:31] bit:24, dataset:CIFAR, training... loss:0.33407\n",
      "DPSH[11/150][01:09:42] bit:24, dataset:CIFAR, training... loss:0.28914\n",
      "DPSH[12/150][01:09:52] bit:24, dataset:CIFAR, training... loss:0.24749\n",
      "DPSH[13/150][01:10:03] bit:24, dataset:CIFAR, training... loss:0.21053\n",
      "DPSH[14/150][01:10:14] bit:24, dataset:CIFAR, training... loss:0.17781\n",
      "DPSH[15/150][01:10:25] bit:24, dataset:CIFAR, training... loss:0.15078\n",
      "DPSH[16/150][01:10:35] bit:24, dataset:CIFAR, training... loss:0.12984\n",
      "DPSH[17/150][01:10:46] bit:24, dataset:CIFAR, training... loss:0.11467\n",
      "DPSH[18/150][01:10:57] bit:24, dataset:CIFAR, training... loss:0.10371\n",
      "DPSH[19/150][01:11:07] bit:24, dataset:CIFAR, training... loss:0.09594\n",
      "DPSH[20/150][01:11:18] bit:24, dataset:CIFAR, training... loss:0.09052\n",
      "DPSH[21/150][01:11:29] bit:24, dataset:CIFAR, training... loss:0.08677\n",
      "DPSH[22/150][01:11:40] bit:24, dataset:CIFAR, training... loss:0.08397\n",
      "DPSH[23/150][01:11:51] bit:24, dataset:CIFAR, training... loss:0.08169\n",
      "DPSH[24/150][01:12:01] bit:24, dataset:CIFAR, training... loss:0.07957\n",
      "DPSH[25/150][01:12:12] bit:24, dataset:CIFAR, training... loss:0.07753\n",
      "DPSH[26/150][01:12:23] bit:24, dataset:CIFAR, training... loss:0.07557\n",
      "DPSH[27/150][01:12:34] bit:24, dataset:CIFAR, training... loss:0.07368\n",
      "DPSH[28/150][01:12:44] bit:24, dataset:CIFAR, training... loss:0.07194\n",
      "DPSH[29/150][01:12:55] bit:24, dataset:CIFAR, training... loss:0.07027\n",
      "DPSH[30/150][01:13:06] bit:24, dataset:CIFAR, training... loss:0.06874\n",
      "DPSH[31/150][01:13:16] bit:24, dataset:CIFAR, training... loss:0.06733\n",
      "DPSH[32/150][01:13:27] bit:24, dataset:CIFAR, training... loss:0.06604\n",
      "DPSH[33/150][01:13:38] bit:24, dataset:CIFAR, training... loss:0.06485\n",
      "DPSH[34/150][01:13:49] bit:24, dataset:CIFAR, training... loss:0.06377\n",
      "DPSH[35/150][01:13:59] bit:24, dataset:CIFAR, training... loss:0.06280\n",
      "DPSH[36/150][01:14:10] bit:24, dataset:CIFAR, training... loss:0.06192\n",
      "DPSH[37/150][01:14:21] bit:24, dataset:CIFAR, training... loss:0.06115\n",
      "DPSH[38/150][01:14:32] bit:24, dataset:CIFAR, training... loss:0.06048\n",
      "DPSH[39/150][01:14:42] bit:24, dataset:CIFAR, training... loss:0.05985\n",
      "DPSH[40/150][01:14:53] bit:24, dataset:CIFAR, training... loss:0.05931\n",
      "DPSH[41/150][01:15:04] bit:24, dataset:CIFAR, training... loss:0.05882\n",
      "DPSH[42/150][01:15:14] bit:24, dataset:CIFAR, training... loss:0.05836\n",
      "DPSH[43/150][01:15:25] bit:24, dataset:CIFAR, training...."
     ]
    }
   ],
   "source": [
    "#DSH12Img = DSH(device, 50000, 1001, 12, 150, 128, [0.01, 0.05, 0.1], [1e-6, 1e-5, 1e-4], [1e-6, 1e-5, 1e-4])\n",
    "DSH24Img = DSH(device, 50000, 1001, 24, 150, 128, [0.01, 0.05, 0.1], [1e-6, 1e-5, 1e-4], [1e-6, 1e-5, 1e-4])\n",
    "#DSH32Img = DSH(device, 50000, 1001, 32, 150, 128, [0.01, 0.05, 0.1], [1e-6, 1e-5, 1e-4], [1e-6, 1e-5, 1e-4])\n",
    "#DSH48Img = DSH(device, 50000, 1001, 48, 150, 128, [0.01, 0.05, 0.1], [1e-6, 1e-5, 1e-4], [1e-6, 1e-5, 1e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSH12Cifar = {'eta': 0.05, 'learning_rate': 0.0001, 'weight_decay': 0.0001}\n",
    "DSH24Cifar = {'eta': 0.1, 'learning_rate': 0.0001, 'weight_decay': 1e-05}\n",
    "DSH32Cifar = {'eta': 0.1, 'learning_rate': 0.0001, 'weight_decay': 0.0001}\n",
    "DSH48Cifar = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSH12Nus = {'eta': 0.05, 'learning_rate': 0.0001, 'weight_decay': 1e-05}\n",
    "DSH24Nus = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}\n",
    "DSH32Nus = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}\n",
    "DSH48Nus = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSH12Img = {'eta': 0.01, 'learning_rate': 0.0001, 'weight_decay': 1e-06}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
