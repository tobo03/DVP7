{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../'\n",
    "sys.path.append(root)\n",
    "#from HelpfulFunctions.batchCreation import createBatch\n",
    "from HelpfulFunctions.metrics import meanAveragePrecision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "\n",
    "\n",
    "X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(root, num_classes, batch_size, train = True):\n",
    "    if train == True:\n",
    "        #Create X_train_tensor\n",
    "        X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "        X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "        #Create Y_train_tensor\n",
    "        y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    else:\n",
    "        X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "        X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "        y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "        y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return test_loader\n",
    "\n",
    "    #Missing implementation for Test and Validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CreateDataset(root, num_classes = 10, batch_size = 128)\n",
    "test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 32),    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "        # Initialize weights and biases from gaussian distribution\n",
    "        for layer in self.fc_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=0.01)  # Initialize weights based on paper\n",
    "                nn.init.normal_(layer.bias, mean=0.0, std=0.01)    # Initialize biases based on paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPSHLoss(torch.nn.Module):\n",
    "    def __init__(self, train_size, n_classes, bit):\n",
    "        super(DPSHLoss, self).__init__()\n",
    "        self.U = torch.zeros(train_size, bit).float().to(device)\n",
    "        self.Y = torch.zeros(train_size, n_classes).float().to(device)\n",
    "\n",
    "    def forward(self, u, y, ind, eta):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        s = (y @ self.Y.t() > 0).float()\n",
    "        inner_product = u @ self.U.t() * 0.5\n",
    "\n",
    "        likelihood_loss = (1 + (-(inner_product.abs())).exp()).log() + inner_product.clamp(min=0) - s * inner_product\n",
    "\n",
    "        likelihood_loss = likelihood_loss.mean()\n",
    "\n",
    "        quantization_loss = eta * (u - u.sign()).pow(2).mean()\n",
    "\n",
    "        return likelihood_loss + quantization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(device, train_loader, test_loader, train_size, test_size, batch_size, n_classes, bit, num_epoch, lr):\n",
    "\n",
    "\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=1e-5, weight_decay = 10 ** -5)\n",
    "    \n",
    "    criterion = DPSHLoss(train_size, n_classes, bit)\n",
    "\n",
    "\n",
    "    #model.train()\n",
    "\n",
    "    #Best_mAP = 0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "        print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            \"DPSH\", epoch + 1, num_epoch, current_time, bit, \"CIFAR\"), end=\"\")\n",
    "\n",
    "        train_loss = 0\n",
    "        for image, label, ind in train_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            u = model(image)\n",
    "\n",
    "\n",
    "            loss = criterion(u, label.float(), ind, lr)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss / (train_size / batch_size)\n",
    "        print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))\n",
    "\n",
    "    return model\n",
    "        #if (epoch + 1) % 3 == 0:\n",
    "            #map = meanAveragePrecision(train_loader, test_loader, model, device)\n",
    "            #print(map)\n",
    "            #Best_mAP = validate(config, Best_mAP, test_loader, dataset_loader, net, bit, epoch, num_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPSH[ 1/150][10:46:13] bit:32, dataset:CIFAR, training... loss:0.74397\n",
      "DPSH[ 2/150][10:46:15] bit:32, dataset:CIFAR, training... loss:0.70944\n",
      "DPSH[ 3/150][10:46:17] bit:32, dataset:CIFAR, training... loss:0.67670\n",
      "DPSH[ 4/150][10:46:18] bit:32, dataset:CIFAR, training... loss:0.64323\n",
      "DPSH[ 5/150][10:46:20] bit:32, dataset:CIFAR, training... loss:0.61358\n",
      "DPSH[ 6/150][10:46:22] bit:32, dataset:CIFAR, training... loss:0.59110\n",
      "DPSH[ 7/150][10:46:25] bit:32, dataset:CIFAR, training... loss:0.57384\n",
      "DPSH[ 8/150][10:46:27] bit:32, dataset:CIFAR, training... loss:0.56044\n",
      "DPSH[ 9/150][10:46:30] bit:32, dataset:CIFAR, training... loss:0.54972\n",
      "DPSH[10/150][10:46:33] bit:32, dataset:CIFAR, training... loss:0.54105\n",
      "DPSH[11/150][10:46:35] bit:32, dataset:CIFAR, training... loss:0.53371\n",
      "DPSH[12/150][10:46:38] bit:32, dataset:CIFAR, training... loss:0.52746\n",
      "DPSH[13/150][10:46:40] bit:32, dataset:CIFAR, training... loss:0.52186\n",
      "DPSH[14/150][10:46:42] bit:32, dataset:CIFAR, training... loss:0.51673\n",
      "DPSH[15/150][10:46:43] bit:32, dataset:CIFAR, training... loss:0.51213\n",
      "DPSH[16/150][10:46:45] bit:32, dataset:CIFAR, training... loss:0.50797\n",
      "DPSH[17/150][10:46:46] bit:32, dataset:CIFAR, training... loss:0.50433\n",
      "DPSH[18/150][10:46:48] bit:32, dataset:CIFAR, training... loss:0.50087\n",
      "DPSH[19/150][10:46:49] bit:32, dataset:CIFAR, training... loss:0.49735\n",
      "DPSH[20/150][10:46:51] bit:32, dataset:CIFAR, training... loss:0.49426\n",
      "DPSH[21/150][10:46:52] bit:32, dataset:CIFAR, training... loss:0.49124\n",
      "DPSH[22/150][10:46:54] bit:32, dataset:CIFAR, training... loss:0.48845\n",
      "DPSH[23/150][10:46:55] bit:32, dataset:CIFAR, training... loss:0.48574\n",
      "DPSH[24/150][10:46:57] bit:32, dataset:CIFAR, training... loss:0.48329\n",
      "DPSH[25/150][10:46:58] bit:32, dataset:CIFAR, training... loss:0.48067\n",
      "DPSH[26/150][10:47:00] bit:32, dataset:CIFAR, training... loss:0.47835\n",
      "DPSH[27/150][10:47:02] bit:32, dataset:CIFAR, training... loss:0.47586\n",
      "DPSH[28/150][10:47:03] bit:32, dataset:CIFAR, training... loss:0.47357\n",
      "DPSH[29/150][10:47:05] bit:32, dataset:CIFAR, training... loss:0.47123\n",
      "DPSH[30/150][10:47:06] bit:32, dataset:CIFAR, training... loss:0.46920\n",
      "DPSH[31/150][10:47:08] bit:32, dataset:CIFAR, training... loss:0.46707\n",
      "DPSH[32/150][10:47:09] bit:32, dataset:CIFAR, training... loss:0.46490\n",
      "DPSH[33/150][10:47:11] bit:32, dataset:CIFAR, training... loss:0.46305\n",
      "DPSH[34/150][10:47:12] bit:32, dataset:CIFAR, training... loss:0.46117\n",
      "DPSH[35/150][10:47:14] bit:32, dataset:CIFAR, training... loss:0.45910\n",
      "DPSH[36/150][10:47:15] bit:32, dataset:CIFAR, training... loss:0.45732\n",
      "DPSH[37/150][10:47:17] bit:32, dataset:CIFAR, training... loss:0.45556\n",
      "DPSH[38/150][10:47:18] bit:32, dataset:CIFAR, training... loss:0.45343\n",
      "DPSH[39/150][10:47:20] bit:32, dataset:CIFAR, training... loss:0.45180\n",
      "DPSH[40/150][10:47:21] bit:32, dataset:CIFAR, training... loss:0.45032\n",
      "DPSH[41/150][10:47:23] bit:32, dataset:CIFAR, training... loss:0.44843\n",
      "DPSH[42/150][10:47:24] bit:32, dataset:CIFAR, training... loss:0.44670\n",
      "DPSH[43/150][10:47:26] bit:32, dataset:CIFAR, training... loss:0.44510\n",
      "DPSH[44/150][10:47:27] bit:32, dataset:CIFAR, training... loss:0.44356\n",
      "DPSH[45/150][10:47:29] bit:32, dataset:CIFAR, training... loss:0.44186\n",
      "DPSH[46/150][10:47:30] bit:32, dataset:CIFAR, training... loss:0.44020\n",
      "DPSH[47/150][10:47:32] bit:32, dataset:CIFAR, training... loss:0.43863\n",
      "DPSH[48/150][10:47:33] bit:32, dataset:CIFAR, training... loss:0.43715\n",
      "DPSH[49/150][10:47:35] bit:32, dataset:CIFAR, training... loss:0.43557\n",
      "DPSH[50/150][10:47:37] bit:32, dataset:CIFAR, training... loss:0.43396\n",
      "DPSH[51/150][10:47:38] bit:32, dataset:CIFAR, training... loss:0.43266\n",
      "DPSH[52/150][10:47:40] bit:32, dataset:CIFAR, training... loss:0.43094\n",
      "DPSH[53/150][10:47:41] bit:32, dataset:CIFAR, training... loss:0.42967\n",
      "DPSH[54/150][10:47:43] bit:32, dataset:CIFAR, training... loss:0.42811\n",
      "DPSH[55/150][10:47:44] bit:32, dataset:CIFAR, training... loss:0.42678\n",
      "DPSH[56/150][10:47:46] bit:32, dataset:CIFAR, training... loss:0.42544\n",
      "DPSH[57/150][10:47:47] bit:32, dataset:CIFAR, training... loss:0.42399\n",
      "DPSH[58/150][10:47:49] bit:32, dataset:CIFAR, training... loss:0.42258\n",
      "DPSH[59/150][10:47:51] bit:32, dataset:CIFAR, training... loss:0.42109\n",
      "DPSH[60/150][10:47:52] bit:32, dataset:CIFAR, training... loss:0.41970\n",
      "DPSH[61/150][10:47:54] bit:32, dataset:CIFAR, training... loss:0.41853\n",
      "DPSH[62/150][10:47:55] bit:32, dataset:CIFAR, training... loss:0.41727\n",
      "DPSH[63/150][10:47:57] bit:32, dataset:CIFAR, training... loss:0.41581\n",
      "DPSH[64/150][10:47:58] bit:32, dataset:CIFAR, training... loss:0.41455\n",
      "DPSH[65/150][10:48:00] bit:32, dataset:CIFAR, training... loss:0.41304\n",
      "DPSH[66/150][10:48:01] bit:32, dataset:CIFAR, training... loss:0.41171\n",
      "DPSH[67/150][10:48:03] bit:32, dataset:CIFAR, training... loss:0.41019\n",
      "DPSH[68/150][10:48:04] bit:32, dataset:CIFAR, training... loss:0.40909\n",
      "DPSH[69/150][10:48:06] bit:32, dataset:CIFAR, training... loss:0.40784\n",
      "DPSH[70/150][10:48:07] bit:32, dataset:CIFAR, training... loss:0.40650\n",
      "DPSH[71/150][10:48:09] bit:32, dataset:CIFAR, training... loss:0.40532\n",
      "DPSH[72/150][10:48:10] bit:32, dataset:CIFAR, training... loss:0.40420\n",
      "DPSH[73/150][10:48:12] bit:32, dataset:CIFAR, training... loss:0.40298\n",
      "DPSH[74/150][10:48:13] bit:32, dataset:CIFAR, training... loss:0.40169\n",
      "DPSH[75/150][10:48:15] bit:32, dataset:CIFAR, training... loss:0.40054\n",
      "DPSH[76/150][10:48:16] bit:32, dataset:CIFAR, training... loss:0.39940\n",
      "DPSH[77/150][10:48:18] bit:32, dataset:CIFAR, training... loss:0.39811\n",
      "DPSH[78/150][10:48:19] bit:32, dataset:CIFAR, training... loss:0.39692\n",
      "DPSH[79/150][10:48:21] bit:32, dataset:CIFAR, training... loss:0.39584\n",
      "DPSH[80/150][10:48:23] bit:32, dataset:CIFAR, training... loss:0.39454\n",
      "DPSH[81/150][10:48:24] bit:32, dataset:CIFAR, training... loss:0.39352\n",
      "DPSH[82/150][10:48:26] bit:32, dataset:CIFAR, training... loss:0.39230\n",
      "DPSH[83/150][10:48:27] bit:32, dataset:CIFAR, training... loss:0.39122\n",
      "DPSH[84/150][10:48:29] bit:32, dataset:CIFAR, training... loss:0.39011\n",
      "DPSH[85/150][10:48:30] bit:32, dataset:CIFAR, training... loss:0.38874\n",
      "DPSH[86/150][10:48:32] bit:32, dataset:CIFAR, training... loss:0.38758\n",
      "DPSH[87/150][10:48:33] bit:32, dataset:CIFAR, training... loss:0.38673\n",
      "DPSH[88/150][10:48:35] bit:32, dataset:CIFAR, training... loss:0.38552\n",
      "DPSH[89/150][10:48:37] bit:32, dataset:CIFAR, training... loss:0.38448\n",
      "DPSH[90/150][10:48:38] bit:32, dataset:CIFAR, training... loss:0.38340\n",
      "DPSH[91/150][10:48:40] bit:32, dataset:CIFAR, training... loss:0.38233\n",
      "DPSH[92/150][10:48:41] bit:32, dataset:CIFAR, training... loss:0.38141\n",
      "DPSH[93/150][10:48:43] bit:32, dataset:CIFAR, training... loss:0.38016\n",
      "DPSH[94/150][10:48:44] bit:32, dataset:CIFAR, training... loss:0.37906\n",
      "DPSH[95/150][10:48:46] bit:32, dataset:CIFAR, training... loss:0.37817\n",
      "DPSH[96/150][10:48:47] bit:32, dataset:CIFAR, training... loss:0.37709\n",
      "DPSH[97/150][10:48:49] bit:32, dataset:CIFAR, training... loss:0.37597\n",
      "DPSH[98/150][10:48:50] bit:32, dataset:CIFAR, training... loss:0.37508\n",
      "DPSH[99/150][10:48:52] bit:32, dataset:CIFAR, training... loss:0.37385\n",
      "DPSH[100/150][10:48:54] bit:32, dataset:CIFAR, training... loss:0.37297\n",
      "DPSH[101/150][10:48:55] bit:32, dataset:CIFAR, training... loss:0.37198\n",
      "DPSH[102/150][10:48:57] bit:32, dataset:CIFAR, training... loss:0.37081\n",
      "DPSH[103/150][10:48:58] bit:32, dataset:CIFAR, training... loss:0.36987\n",
      "DPSH[104/150][10:49:00] bit:32, dataset:CIFAR, training... loss:0.36871\n",
      "DPSH[105/150][10:49:01] bit:32, dataset:CIFAR, training... loss:0.36809\n",
      "DPSH[106/150][10:49:03] bit:32, dataset:CIFAR, training... loss:0.36685\n",
      "DPSH[107/150][10:49:04] bit:32, dataset:CIFAR, training... loss:0.36592\n",
      "DPSH[108/150][10:49:06] bit:32, dataset:CIFAR, training... loss:0.36502\n",
      "DPSH[109/150][10:49:07] bit:32, dataset:CIFAR, training... loss:0.36389\n",
      "DPSH[110/150][10:49:09] bit:32, dataset:CIFAR, training... loss:0.36287\n",
      "DPSH[111/150][10:49:11] bit:32, dataset:CIFAR, training... loss:0.36202\n",
      "DPSH[112/150][10:49:12] bit:32, dataset:CIFAR, training... loss:0.36103\n",
      "DPSH[113/150][10:49:14] bit:32, dataset:CIFAR, training... loss:0.36007\n",
      "DPSH[114/150][10:49:15] bit:32, dataset:CIFAR, training... loss:0.35913\n",
      "DPSH[115/150][10:49:17] bit:32, dataset:CIFAR, training... loss:0.35828\n",
      "DPSH[116/150][10:49:18] bit:32, dataset:CIFAR, training... loss:0.35752\n",
      "DPSH[117/150][10:49:20] bit:32, dataset:CIFAR, training... loss:0.35639\n",
      "DPSH[118/150][10:49:21] bit:32, dataset:CIFAR, training... loss:0.35555\n",
      "DPSH[119/150][10:49:23] bit:32, dataset:CIFAR, training... loss:0.35447\n",
      "DPSH[120/150][10:49:25] bit:32, dataset:CIFAR, training... loss:0.35354\n",
      "DPSH[121/150][10:49:26] bit:32, dataset:CIFAR, training... loss:0.35266\n",
      "DPSH[122/150][10:49:28] bit:32, dataset:CIFAR, training... loss:0.35196\n",
      "DPSH[123/150][10:49:29] bit:32, dataset:CIFAR, training... loss:0.35076\n",
      "DPSH[124/150][10:49:31] bit:32, dataset:CIFAR, training... loss:0.34979\n",
      "DPSH[125/150][10:49:32] bit:32, dataset:CIFAR, training... loss:0.34888\n",
      "DPSH[126/150][10:49:34] bit:32, dataset:CIFAR, training... loss:0.34786\n",
      "DPSH[127/150][10:49:35] bit:32, dataset:CIFAR, training... loss:0.34729\n",
      "DPSH[128/150][10:49:37] bit:32, dataset:CIFAR, training... loss:0.34636\n",
      "DPSH[129/150][10:49:39] bit:32, dataset:CIFAR, training... loss:0.34523\n",
      "DPSH[130/150][10:49:40] bit:32, dataset:CIFAR, training... loss:0.34460\n",
      "DPSH[131/150][10:49:42] bit:32, dataset:CIFAR, training... loss:0.34360\n",
      "DPSH[132/150][10:49:43] bit:32, dataset:CIFAR, training... loss:0.34273\n",
      "DPSH[133/150][10:49:45] bit:32, dataset:CIFAR, training... loss:0.34202\n",
      "DPSH[134/150][10:49:46] bit:32, dataset:CIFAR, training... loss:0.34114\n",
      "DPSH[135/150][10:49:48] bit:32, dataset:CIFAR, training... loss:0.34004\n",
      "DPSH[136/150][10:49:49] bit:32, dataset:CIFAR, training... loss:0.33925\n",
      "DPSH[137/150][10:49:51] bit:32, dataset:CIFAR, training... loss:0.33835\n",
      "DPSH[138/150][10:49:52] bit:32, dataset:CIFAR, training... loss:0.33753\n",
      "DPSH[139/150][10:49:54] bit:32, dataset:CIFAR, training... loss:0.33668\n",
      "DPSH[140/150][10:49:55] bit:32, dataset:CIFAR, training... loss:0.33573\n",
      "DPSH[141/150][10:49:57] bit:32, dataset:CIFAR, training... loss:0.33493\n",
      "DPSH[142/150][10:49:58] bit:32, dataset:CIFAR, training... loss:0.33452\n",
      "DPSH[143/150][10:50:00] bit:32, dataset:CIFAR, training... loss:0.33341\n",
      "DPSH[144/150][10:50:01] bit:32, dataset:CIFAR, training... loss:0.33267\n",
      "DPSH[145/150][10:50:03] bit:32, dataset:CIFAR, training... loss:0.33183\n",
      "DPSH[146/150][10:50:04] bit:32, dataset:CIFAR, training... loss:0.33064\n",
      "DPSH[147/150][10:50:06] bit:32, dataset:CIFAR, training... loss:0.32988\n",
      "DPSH[148/150][10:50:07] bit:32, dataset:CIFAR, training... loss:0.32915\n",
      "DPSH[149/150][10:50:09] bit:32, dataset:CIFAR, training... loss:0.32848\n",
      "DPSH[150/150][10:50:10] bit:32, dataset:CIFAR, training... loss:0.32760\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_val(device, train_loader, test_loader, train_size = 45000, test_size = 10000, batch_size = 128, n_classes = 10, bit = 32, num_epoch = 150, lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trained_model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_train = (trained_model(X_train_tensor)).sign()\n",
    "hash_train = hash_train.detach().numpy()\n",
    "hash_test = (trained_model(X_test_tensor)).sign()\n",
    "hash_test = hash_test.detach().numpy()\n",
    "y_train\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:09<00:00, 143.08it/s]\n"
     ]
    }
   ],
   "source": [
    "map = meanAveragePrecision(training_hashes = hash_train, test_hashes = hash_test, test_labels = y_test, training_labels = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573545010386268"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash code evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From what we know hash codes should be uncorrelated, and should have mean = 0, for [-1,1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01554375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(hash_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitwise average: [ 0.1702  0.0202  0.0352 -0.0598 -0.1514 -0.0516 -0.0502 -0.1012  0.1544\n",
      "  0.0324  0.1452  0.1864  0.1134 -0.0082 -0.071  -0.0506  0.056   0.1292\n",
      "  0.0364 -0.0018  0.0374 -0.1174  0.0012 -0.0296  0.03   -0.0224 -0.0226\n",
      "  0.1506 -0.0198  0.0068 -0.1624  0.1124]\n"
     ]
    }
   ],
   "source": [
    "# Compute the bitwise average\n",
    "bitwise_avg = np.mean(hash_test, axis=0)\n",
    "\n",
    "print(\"Bitwise average:\", bitwise_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
