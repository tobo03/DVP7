{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../'\n",
    "sys.path.append(root)\n",
    "from HelpfulFunctions.batchCreation import createBatch\n",
    "#from HelpfulFunctions.metrics import meanAveragePrecision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAveragePrecision(train_loader, test_loader, model, device='cuda') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates mean average precision based on hashes from train_loader and test_loader.\n",
    "\n",
    "    Args:\n",
    "    ---\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data.\n",
    "    test_loader : DataLoader\n",
    "        DataLoader for test data.\n",
    "    model : torch.nn.Module\n",
    "        Model used to compute hashes. Assumes model outputs a hash for each input.\n",
    "    device : str\n",
    "        The device on which to run computations ('cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    mean_average_precision : torch.Tensor\n",
    "        The mean average precision value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store hashes and labels for training and testing data\n",
    "    train_hashes, train_labels = [], []\n",
    "    test_hashes, test_labels = [], []\n",
    "\n",
    "    # Generate hashes and labels for the training set\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        for images, labels, ind in tqdm(train_loader, desc=\"Processing Train Loader\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Get the hash from the model (assuming model outputs hash directly)\n",
    "            hashes = model(images).sign()  # Apply sign if needed for binary hash\n",
    "            train_hashes.append(hashes)\n",
    "            train_labels.append(labels)\n",
    "\n",
    "        # Concatenate all hashes and labels along the batch dimension\n",
    "        train_hashes = torch.cat(train_hashes, dim=0)\n",
    "        train_labels = torch.cat(train_labels, dim=0)\n",
    "\n",
    "    # Generate hashes and labels for the test set\n",
    "    with torch.no_grad():\n",
    "        for images, labels, ind in tqdm(test_loader, desc=\"Processing Test Loader\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            hashes = model(images).sign()\n",
    "            test_hashes.append(hashes)\n",
    "            test_labels.append(labels)\n",
    "\n",
    "        test_hashes = torch.cat(test_hashes, dim=0)\n",
    "        test_labels = torch.cat(test_labels, dim=0)\n",
    "\n",
    "    # Now calculate mean average precision\n",
    "    aps = []\n",
    "    for i in tqdm(range(test_hashes.size(0)), desc=\"Calculating Mean Average Precision\"):\n",
    "        test_hash = test_hashes[i]  # Get a single test hash\n",
    "        label = test_labels[i]\n",
    "\n",
    "        # Calculate distances between the single test hash and all train hashes\n",
    "        distances = (train_hashes - test_hash).abs().sum(dim=1)\n",
    "\n",
    "        # Create true positive indicators\n",
    "        tp = (train_labels == label).float()\n",
    "\n",
    "        # Sort by distances (ascending order)\n",
    "        sorted_indices = distances.argsort()\n",
    "        sorted_tp = tp[sorted_indices]\n",
    "\n",
    "        # Cumulative sum of true positives\n",
    "        cumsum_tp = torch.cumsum(sorted_tp, dim=0)\n",
    "\n",
    "        # Precision calculation\n",
    "        precision = cumsum_tp / (torch.arange(1, len(cumsum_tp) + 1, device=device).float())\n",
    "\n",
    "        # Average precision calculation\n",
    "        if sorted_tp.sum() > 0:  # Avoid division by zero\n",
    "            ap = (precision * sorted_tp).sum() / sorted_tp.sum()\n",
    "        else:\n",
    "            ap = torch.tensor(0.0, device=device)\n",
    "        aps.append(ap)\n",
    "    \n",
    "    # Calculate mean average precision\n",
    "    mean_average_precision = torch.stack(aps).mean()\n",
    "    \n",
    "    return mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "\n",
    "\n",
    "X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(root, num_classes, batch_size, train = True):\n",
    "    if train == True:\n",
    "        #Create X_train_tensor\n",
    "        X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (45000, 4096)\n",
    "        X_train_tensor = torch.tensor(X_train)\n",
    "\n",
    "        #Create Y_train_tensor\n",
    "        y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (45000,)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes) #One-Hot Encoded -> Shape = (45000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_train = torch.arange(len(X_train_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_train_tensor, y_train_tensor, indices_train)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return train_loader\n",
    "\n",
    "    else:\n",
    "        X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "        X_test_tensor = torch.tensor(X_test)\n",
    "\n",
    "        y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "        y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes) #One-Hot Encoded -> Shape = (10000, num_classes)\n",
    "\n",
    "        #Create indices\n",
    "        indices_test = torch.arange(len(X_test_tensor))\n",
    "\n",
    "        dataset = TensorDataset(X_test_tensor, y_test_tensor, indices_test)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return test_loader\n",
    "\n",
    "    #Missing implementation for Test and Validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CreateDataset(root, num_classes = 10, batch_size = 128)\n",
    "test_loader = CreateDataset(root, num_classes = 10, batch_size = 128, train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4096, 1024),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 48),    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "        # Initialize weights and biases from gaussian distribution\n",
    "        for layer in self.fc_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=0.01)  # Initialize weights based on paper\n",
    "                nn.init.normal_(layer.bias, mean=0.0, std=0.01)    # Initialize biases based on paper\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPSHLoss(torch.nn.Module):\n",
    "    def __init__(self, train_size, n_classes, bit):\n",
    "        super(DPSHLoss, self).__init__()\n",
    "        self.U = torch.zeros(train_size, bit).float().to(device)\n",
    "        self.Y = torch.zeros(train_size, n_classes).float().to(device)\n",
    "\n",
    "    def forward(self, u, y, ind, eta):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        s = (y @ self.Y.t() > 0).float()\n",
    "        inner_product = u @ self.U.t() * 0.5\n",
    "\n",
    "        likelihood_loss = (1 + (-(inner_product.abs())).exp()).log() + inner_product.clamp(min=0) - s * inner_product\n",
    "\n",
    "        likelihood_loss = likelihood_loss.mean()\n",
    "\n",
    "        quantization_loss = eta * (u - u.sign()).pow(2).mean()\n",
    "\n",
    "        return likelihood_loss + quantization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(device, train_loader, test_loader, train_size, test_size, batch_size, n_classes, bit, num_epoch, lr):\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    criterion = DPSHLoss(train_size, n_classes, bit)\n",
    "\n",
    "\n",
    "    #model.train()\n",
    "\n",
    "    #Best_mAP = 0\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "\n",
    "        print(\"%s[%2d/%2d][%s] bit:%d, dataset:%s, training....\" % (\n",
    "            \"DPSH\", epoch + 1, num_epoch, current_time, bit, \"CIFAR\"), end=\"\")\n",
    "\n",
    "        train_loss = 0\n",
    "        for image, label, ind in train_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            u = model(image)\n",
    "\n",
    "\n",
    "            loss = criterion(u, label.float(), ind, lr)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss / (train_size / batch_size)\n",
    "\n",
    "        print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.5f\" % (train_loss))\n",
    "\n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            map = meanAveragePrecision(train_loader, test_loader, model, device)\n",
    "            print(map)\n",
    "            #Best_mAP = validate(config, Best_mAP, test_loader, dataset_loader, net, bit, epoch, num_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPSH[ 1/150][13:15:43] bit:48, dataset:CIFAR, training... loss:0.79593\n",
      "DPSH[ 2/150][13:15:45] bit:48, dataset:CIFAR, training... loss:0.78111\n",
      "DPSH[ 3/150][13:15:48] bit:48, dataset:CIFAR, training... loss:0.78417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train Loader: 100%|██████████| 352/352 [00:00<00:00, 411.65it/s]\n",
      "Processing Test Loader: 100%|██████████| 79/79 [00:00<00:00, 407.26it/s]\n",
      "Calculating Mean Average Precision:   0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (45000) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m45000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[84], line 40\u001b[0m, in \u001b[0;36mtrain_val\u001b[1;34m(device, train_loader, test_loader, train_size, test_size, batch_size, n_classes, bit, num_epoch, lr)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\b\u001b[39;00m\u001b[38;5;130;01m\\b\u001b[39;00m\u001b[38;5;130;01m\\b\u001b[39;00m\u001b[38;5;130;01m\\b\u001b[39;00m\u001b[38;5;130;01m\\b\u001b[39;00m\u001b[38;5;130;01m\\b\u001b[39;00m\u001b[38;5;130;01m\\b\u001b[39;00m\u001b[38;5;124m loss:\u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (train_loss))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28mmap\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmeanAveragePrecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mmap\u001b[39m)\n",
      "Cell \u001b[1;32mIn[77], line 76\u001b[0m, in \u001b[0;36mmeanAveragePrecision\u001b[1;34m(train_loader, test_loader, model, device)\u001b[0m\n\u001b[0;32m     73\u001b[0m cumsum_tp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcumsum(sorted_tp, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Precision calculation\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mcumsum_tp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcumsum_tp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Average precision calculation\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sorted_tp\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Avoid division by zero\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (45000) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "train_val(device, train_loader, test_loader, train_size = 45000, test_size = 10000, batch_size = 128, n_classes = 10, bit = 48, num_epoch = 150, lr = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
