{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../../'\n",
    "\n",
    "X_train = np.load( root + \"Features/train_features_vgg16_cifar10.npy\" ) # Shape = (50000, 4096)\n",
    "y_train = np.load( root + \"Features/train_labels_vgg16_cifar10.npy\" ) # Shape = (50000,)\n",
    "\n",
    "X_test = np.load( root + \"Features/test_features_vgg16_cifar10.npy\" ) # Shape = (10000, 4096)\n",
    "y_test = np.load( root + \"Features/test_labels_vgg16_cifar10.npy\" ) # Shape = (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'HelpfulFunctions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mHelpfulFunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBatchCreation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CreateBatch\n\u001b[0;32m      3\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m CreateBatch(X_tensor \u001b[38;5;241m=\u001b[39m X_tensor, y_train \u001b[38;5;241m=\u001b[39m y_train, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'HelpfulFunctions'"
     ]
    }
   ],
   "source": [
    "from HelpfulFunctions.BatchCreation import CreateBatch\n",
    "\n",
    "X_batch, y_batch = CreateBatch(X_tensor = X_tensor, y_train = y_train, batch_size = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(4096, 256),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 32)    # Second fully connected layer to reduce to 4000\n",
    "        )\n",
    "\n",
    "        # Initialize weights and biases from gaussian distribution\n",
    "        for layer in self.fc_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, mean=0.0, std=0.01)  # Initialize weights\n",
    "                nn.init.normal_(layer.bias, mean=0.0, std=0.01)    # Initialize biases\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, eta = 0.25):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.eta = eta\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        #S calculation\n",
    "        print(\"S Start\")\n",
    "        targets = torch.tensor(targets)\n",
    "        S = (targets[:, None] == targets).float() # Shape = (sample_size, sample_size)\n",
    "        print(\"S Slut\")\n",
    "\n",
    "\n",
    "        #U calculation\n",
    "        U = outputs\n",
    "\n",
    "        #Calculate Theta\n",
    "        print(\"Theta Start\")\n",
    "        dot_product_matrix = torch.matmul(U, U.T)\n",
    "        dot_product_matrix # (sample_size, sample_size) Shape\n",
    "        Theta = 1/2 * dot_product_matrix\n",
    "        print(\"Theta Slut\")\n",
    "\n",
    "\n",
    "        #Calculate hash codes\n",
    "        B = torch.sign(U) # Shape = (sample_size, hash_length)\n",
    "\n",
    "\n",
    "        loss = - torch.sum(S * Theta - torch.log(1 + torch.exp(Theta))) + self.eta * torch.sum(torch.norm(B - U, dim = 1).pow(2))\n",
    "        print(loss) # Should give tensor(loss, grad_fn) NO CLUE HVAD GRAD_FN ER, NOK HVORDAN GRADIENT SKAL UDREGNES\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Custom_loss And Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loss = CustomLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S Start\n",
      "S Slut\n",
      "Theta Start\n",
      "Theta Slut\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero gradients from the previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(torch.tensor(X_train))\n",
    "\n",
    "    # Forward pass\n",
    "    loss = custom_loss(outputs, y_train)\n",
    "    \n",
    "    \n",
    "    # Backward pass to compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters using optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Optionally, print loss for monitoring\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HelpfulFunctions.Metrics import mean_average_precision\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
