{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaoy\\AppData\\Local\\Temp\\ipykernel_20992\\4150589690.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_hpo_tensor = torch.tensor(X_hpo, dtype=torch.long)\n",
      "C:\\Users\\xiaoy\\AppData\\Local\\Temp\\ipykernel_20992\\4150589690.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_valid_tensor = torch.tensor(X_valid, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "root = \"/Users/xiaoy/OneDrive/Desktop/P7/p7 project/data/HPO_Validering_3/CIFAR/\"\n",
    "# load data\n",
    "X_hpo = torch.tensor(np.load(root + \"X_hpo_Cifar.npy\" ))                       \n",
    "y_hpo = np.load(root + \"y_hpo_CIfar.npy\" )\n",
    "\n",
    "X_valid = torch.tensor( np.load(root + \"X_val_Cifar.npy\" ) )\n",
    "y_valid = np.load(root + \"y_val_Cifar.npy\")\n",
    "\n",
    "X_hpo_tensor = torch.tensor(X_hpo, dtype=torch.long)\n",
    "y_hpo_tensor = torch.tensor(y_hpo, dtype=torch.long)\n",
    "\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.long)\n",
    "y_valid_tensor = torch.tensor(y_valid, dtype=torch.long)\n",
    "\n",
    "hpo_dataset = TensorDataset(X_hpo_tensor, y_hpo_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "\n",
    "hpo_loader = DataLoader(hpo_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "losses = []\n",
    "maps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SUBIC_encoder(nn.Module): \n",
    "    def __init__(self, input_size=4096, bits=48, num_classes=10, num_blocks=8, block_size=6):\n",
    "        super(SUBIC_encoder, self).__init__()\n",
    "       \n",
    "        assert bits % num_blocks == 0, \"Bits must be divisible by num_blocks\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.bits = bits \n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Define the encoder structure\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, bits)\n",
    "        )  # Outputs binary feature vectors\n",
    "        \n",
    "        self.fc3 = nn.Linear(bits, num_classes)  # Logits for num_classes\n",
    "    \n",
    "    def block_softmax(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        block_size = x.shape[1] // self.num_blocks\n",
    "        \n",
    "        # Ensure that x has the expected shape\n",
    "        assert x.shape[1] == self.bits, f\"Expected shape [batch_size, {self.bits}], got {x.shape}\"\n",
    "        \n",
    "        # Reshape and apply softmax\n",
    "        x = x.view(batch_size, self.num_blocks, block_size)\n",
    "        x = F.softmax(x, dim=-1) \n",
    "        return x.view(batch_size, -1) #-1 refers to the value that will match the original elements \n",
    "    \n",
    "    def block_one_hot(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = x.view(batch_size, self.num_blocks, self.block_size)\n",
    "        max_indices = x.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Create one-hot encoding\n",
    "        one_hot = torch.zeros_like(x).scatter_(-1, max_indices, 1)\n",
    "\n",
    "        return one_hot.view(batch_size, self.bits)\n",
    "    \n",
    "    def forward(self, x, use_one_hot=False):\n",
    "        # Ensure x is a flat tensor before passing to encoder\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)  # Flatten if necessary\n",
    "\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        if use_one_hot:\n",
    "            binary_codes = self.block_one_hot(z)\n",
    "        else:\n",
    "            binary_codes = self.block_softmax(z)\n",
    "\n",
    "        class_probs = F.softmax(self.fc3(binary_codes), dim=-1) \n",
    "\n",
    "        return class_probs, binary_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product # create cartesian products for all params\n",
    "\n",
    "bits_12_param_grid = {\n",
    "'bits':12,\n",
    "'num_blocks':\n",
    "[3, 4],\n",
    "'gamma':[0.2, 0.6, 0.8],\n",
    "'mu':[0.2, 0.6, 0.8],\n",
    "'learning_rate': [0.005, 0.01]\n",
    "}\n",
    "\n",
    "bits_24_param_grid = {\n",
    "'bits': 24,\n",
    "'num_blocks':\n",
    "[3, 6, 12],\n",
    "'gamma':[0.2, 0.6, 0.8],\n",
    "'mu':[0.2, 0.6, 0.8],\n",
    "'learning_rate': [0.005, 0.01]\n",
    "}\n",
    "\n",
    "bits_32_param_grid = {\n",
    "'bits':32,\n",
    "'num_blocks':\n",
    "[4, 8, 16],\n",
    "'gamma':[0.2, 0.6, 0.8],\n",
    "'mu':[0.2, 0.6, 0.8],\n",
    "'learning_rate': [0.005, 0.01]\n",
    "}\n",
    "\n",
    "bits_48_param_grid = {\n",
    "'bits':48,\n",
    "'num_blocks':\n",
    "[6, 12, 24],\n",
    "'gamma':[0.2, 0.6, 0.8],\n",
    "'mu':[0.2, 0.6, 0.8],\n",
    "'learning_rate': [0.005, 0.01]\n",
    "}\n",
    "\n",
    "def generate_combinations(param_grid):\n",
    "    keys = list(param_grid.keys())\n",
    "    values = [v if isinstance(v, list) else [v] for v in param_grid.values()] # make list of dicts, if value is a list then take value from the list if not then take that scalar value\n",
    "    gamma_idx = keys.index(\"gamma\")\n",
    "    mu_idx = keys.index(\"mu\")\n",
    "    gamma_mu_pairs = list(zip(values[gamma_idx], values[mu_idx]))\n",
    "    combined_values = values[:gamma_idx] + [gamma_mu_pairs] + values[mu_idx+1:]\n",
    "    combined_keys = keys[:gamma_idx] + [\"gamma_mu\"] + keys[mu_idx+1:]\n",
    "    comb = list(product(*combined_values))\n",
    "    param_combinations_dicts = [\n",
    "        {\n",
    "            **dict(zip(combined_keys, comb_item)),\n",
    "            'gamma': comb_item[combined_keys.index('gamma_mu')][0],\n",
    "            'mu': comb_item[combined_keys.index('gamma_mu')][1]\n",
    "        } for comb_item in comb\n",
    "    ]\n",
    "    for comb_dict in param_combinations_dicts:\n",
    "        del comb_dict['gamma_mu']\n",
    "    \n",
    "    return param_combinations_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(a):\n",
    "    \n",
    "    if isinstance(a, torch.Tensor):\n",
    "         a = a.cpu().numpy()\n",
    "    b = np.zeros((a.size, a.max() + 1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    \n",
    "    return b\n",
    "def meanAveragePrecision(test_hashes, training_hashes, test_labels, training_labels):\n",
    "    aps = []\n",
    "    if len(training_labels.shape) == 1:\n",
    "        training_labels = one_hot_encode(training_labels)\n",
    "        test_labels = one_hot_encode(test_labels)\n",
    "    for i, test_hash in enumerate(tqdm(test_hashes)):\n",
    "        label = test_labels[i]\n",
    "        distances = np.abs(training_hashes - test_hashes[i]).sum(axis=1)\n",
    "        tp = np.where((training_labels*label).sum(axis=1)>0, 1, 0)\n",
    "        hash_df = pd.DataFrame({\"distances\":distances, \"tp\":tp}).reset_index()\n",
    "        hash_df = hash_df.sort_values([\"distances\", \"index\"]).reset_index(drop=True)\n",
    "        hash_df = hash_df.drop([\"index\", \"distances\"], axis=1).reset_index()\n",
    "        hash_df = hash_df[hash_df[\"tp\"]==1]\n",
    "        hash_df[\"tp\"] = hash_df[\"tp\"].cumsum()\n",
    "        hash_df[\"index\"] = hash_df[\"index\"] +1 \n",
    "        precision = np.array(hash_df[\"tp\"]) / np.array(hash_df[\"index\"])\n",
    "        ap = precision.mean()\n",
    "        aps.append(ap)\n",
    "    \n",
    "    return np.array(aps).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    entropy_result = -torch.sum(p * torch.log2(p + 1e-30), dim=-1)\n",
    "    return entropy_result\n",
    "\n",
    "def cross_entropy(class_prob, target):\n",
    "    if len(target.shape) >1:\n",
    "        s = (class_prob*target).mean(axis=1)\n",
    "    else:\n",
    "        s = class_prob[torch.arange(len(target)), target]\n",
    "    return -torch.log2(s)/torch.log2(torch.FloatTensor([class_prob.shape[1]]))\n",
    "\n",
    "def compute_total_loss(class_probs, target, binary_codes, num_blocks, block_size, gamma=0.5, mu=0.5):\n",
    "    \"\"\"\n",
    "    Computes the total loss, which includes:\n",
    "    - Cross-entropy classification loss\n",
    "    - Mean entropy loss (encouraging one-hot encoding within each block)\n",
    "    - Batch entropy loss (encouraging uniform distribution across blocks)\n",
    "    \n",
    "    Parameters:\n",
    "    - class_probs: The class probabilities from the classification layer.\n",
    "    - target: The true labels.\n",
    "    - binary_codes: The binary codes generated by the encoder.\n",
    "    - num_blocks: The number of blocks in the binary codes.\n",
    "    - block_size: The size of each block in the binary codes.\n",
    "    - gamma: Weight for the mean entropy loss.\n",
    "    - mu: Weight for the batch entropy loss.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    classification_loss = cross_entropy(class_probs, target)\n",
    "\n",
    "    batch_size = binary_codes.shape[0]\n",
    "    binary_codes = binary_codes.view(batch_size, num_blocks, block_size) #used in structure encoding\n",
    "\n",
    "    #Mean Entropy Loss (encourages each block to resemble a one-hot vector) using softmax binary code\n",
    "    mean_entropy_loss = entropy(binary_codes).mean(dim=1)\n",
    "\n",
    "    #Batch Entropy Loss (encourages uniform distribution across blocks)\n",
    "    average_support = binary_codes.mean(dim=0)  \n",
    "    batch_entropy_loss = entropy(average_support).mean(dim=0)\n",
    "\n",
    "    #Combine losses with weights gamma and mu\n",
    "    entropy_loss = (gamma * mean_entropy_loss - mu * batch_entropy_loss)/torch.log2(torch.FloatTensor([block_size]))\n",
    "    total_loss = (classification_loss + entropy_loss).mean()\n",
    "    \n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "#logits, binary_codes = model(X_train, use_one_hot=False)\n",
    "#loss = compute_total_loss(logits, y_train_tensor, binary_codes, num_blocks=8, block_size=4, gamma=0.5, mu=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(epochs, bits, num_blocks, block_size, gamma, mu, learning_rate):\n",
    "    # Initialize model\n",
    "    model = SUBIC_encoder(\n",
    "        bits=bits,\n",
    "        input_size=X_hpo.shape[1],\n",
    "        num_classes=10,\n",
    "        num_blocks=num_blocks,\n",
    "        block_size=block_size\n",
    "    )\n",
    "    model.to(device)  # Move model to the specified device\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Track losses\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images, labels in hpo_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images, labels = images.to(torch.float), labels.to(torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            class_probs, binary_codes = model(images, use_one_hot=False)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = compute_total_loss(\n",
    "                class_probs, labels, binary_codes, num_blocks, block_size, gamma, mu\n",
    "            )\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Compute average loss for the epoch\n",
    "        avg_loss = total_loss / len(hpo_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model, losses\n",
    "\n",
    "def map_on_call(model):\n",
    "    \n",
    "    model.eval()\n",
    "    all_query_codes, all_query_labels = [], []\n",
    "    all_db_codes, all_db_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images, labels = images.to(torch.float), labels.to(torch.long)\n",
    "\n",
    "            _, binary_codes = model(images, use_one_hot=True)\n",
    "\n",
    "            # Ensure binary_codes is a tensor\n",
    "            if not isinstance(binary_codes, torch.Tensor):\n",
    "                raise TypeError(\"Expected binary_codes to be a tensor.\")\n",
    "            \n",
    "            all_db_codes.append(binary_codes)\n",
    "            all_db_labels.append(labels)\n",
    "            if len(all_query_codes) == 0:  \n",
    "                all_query_codes.append(binary_codes.clone())  \n",
    "                all_query_labels.append(labels.clone())\n",
    "\n",
    "    # Concatenate all tensors\n",
    "    all_query_codes = torch.cat(all_query_codes, dim=0)\n",
    "    all_query_labels = torch.cat(all_query_labels, dim=0)\n",
    "    all_db_codes = torch.cat(all_db_codes, dim=0)\n",
    "    all_db_labels = torch.cat(all_db_labels, dim=0)\n",
    "\n",
    "    # Calculate MAP Score\n",
    "    map_score = meanAveragePrecision(\n",
    "        all_query_codes,\n",
    "        all_db_codes,\n",
    "        all_query_labels,\n",
    "        all_db_labels\n",
    "        )\n",
    "\n",
    "    return map_score \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_12_param_dicts = generate_combinations(bits_12_param_grid) \n",
    "bits_24_param_dicts = generate_combinations(bits_24_param_grid) \n",
    "bits_32_param_dicts = generate_combinations(bits_32_param_grid) \n",
    "bits_48_param_dicts = generate_combinations(bits_48_param_grid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "epochs = 100\n",
    "map_results = {}\n",
    "\n",
    "# iterate over each combination, train the model and evaluate its performance \n",
    "# initailize model with current hyperparameters\n",
    "\n",
    "def hyper_tuning(param_grid, epochs, hpo_train, hpo_loader, device):\n",
    "    results = {}  # To store MAP scores for each hyperparameter set\n",
    "    best_map_score = 0  # Initialize with a very low value\n",
    "    best_params = None  # To store the best parameter set\n",
    "\n",
    "    for params in param_grid:\n",
    "        # Extract parameters from the current set\n",
    "        bits = params['bits']\n",
    "        num_blocks = params['num_blocks']\n",
    "        block_size = bits // num_blocks\n",
    "        gamma = params['gamma']\n",
    "        mu = params['mu']\n",
    "        learning_rate = params['learning_rate']\n",
    "\n",
    "        print(f\"Testing hyperparameters: {params}\")\n",
    "\n",
    "        # Initialize model with the current hyperparameters\n",
    "        model = SUBIC_encoder(bits=bits, input_size= hpo_train.shape[1], num_classes=10, num_blocks=num_blocks, block_size=block_size)\n",
    "        model.to(device) \n",
    "\n",
    "        # Train the model with the current parameter set\n",
    "        trained_model, _ = loss_function(epochs, bits, num_blocks, block_size, gamma, mu, learning_rate)\n",
    "\n",
    "        # Evaluate the model using MAP score\n",
    "        map_score = map_on_call(trained_model)\n",
    "        print(f\"MAP score: {map_score:.4f}\")\n",
    "\n",
    "        # Store the MAP score for the current parameter set\n",
    "        results[tuple(params.items())] = map_score\n",
    "\n",
    "        if map_score > best_map_score:\n",
    "            best_map_score = map_score\n",
    "            best_params = params\n",
    "            \n",
    "    return best_params, best_map_score, results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing hyperparameters: {'bits': 12, 'num_blocks': 3, 'learning_rate': 0.005, 'gamma': 0.2, 'mu': 0.2}\n",
      "Epoch 1/100, Loss: 0.8455\n",
      "Epoch 2/100, Loss: 0.7225\n",
      "Epoch 3/100, Loss: 0.6825\n",
      "Epoch 4/100, Loss: 0.6424\n",
      "Epoch 5/100, Loss: 0.6512\n",
      "Epoch 6/100, Loss: 0.6363\n",
      "Epoch 7/100, Loss: 0.6138\n",
      "Epoch 8/100, Loss: 0.6132\n",
      "Epoch 9/100, Loss: 0.5881\n",
      "Epoch 10/100, Loss: 0.6076\n",
      "Epoch 11/100, Loss: 0.6635\n",
      "Epoch 12/100, Loss: 0.6004\n",
      "Epoch 13/100, Loss: 0.6803\n",
      "Epoch 14/100, Loss: 0.5850\n",
      "Epoch 15/100, Loss: 0.5699\n",
      "Epoch 16/100, Loss: 0.5828\n",
      "Epoch 17/100, Loss: 0.5598\n",
      "Epoch 18/100, Loss: 0.5740\n",
      "Epoch 19/100, Loss: 0.5706\n",
      "Epoch 20/100, Loss: 0.5604\n",
      "Epoch 21/100, Loss: 0.5802\n",
      "Epoch 22/100, Loss: 0.5657\n",
      "Epoch 23/100, Loss: 0.5478\n",
      "Epoch 24/100, Loss: 0.5577\n",
      "Epoch 25/100, Loss: 0.5642\n",
      "Epoch 26/100, Loss: 0.5351\n",
      "Epoch 27/100, Loss: 0.5377\n",
      "Epoch 28/100, Loss: 0.5669\n",
      "Epoch 29/100, Loss: 0.5009\n",
      "Epoch 30/100, Loss: 0.5226\n",
      "Epoch 31/100, Loss: 0.5095\n",
      "Epoch 32/100, Loss: 0.5163\n",
      "Epoch 33/100, Loss: 0.4610\n",
      "Epoch 34/100, Loss: 0.4908\n",
      "Epoch 35/100, Loss: 0.5226\n",
      "Epoch 36/100, Loss: 0.5280\n",
      "Epoch 37/100, Loss: 0.5198\n",
      "Epoch 38/100, Loss: 0.4498\n",
      "Epoch 39/100, Loss: 0.4523\n",
      "Epoch 40/100, Loss: 0.4612\n",
      "Epoch 41/100, Loss: 0.4947\n",
      "Epoch 42/100, Loss: 0.4432\n",
      "Epoch 43/100, Loss: 0.4850\n",
      "Epoch 44/100, Loss: 0.4316\n",
      "Epoch 45/100, Loss: 0.4713\n",
      "Epoch 46/100, Loss: 0.4755\n",
      "Epoch 47/100, Loss: 0.4394\n",
      "Epoch 48/100, Loss: 0.4315\n",
      "Epoch 49/100, Loss: 0.4908\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mhyper_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhpo_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_hpo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhpo_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhpo_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m, in \u001b[0;36mhyper_tuning\u001b[1;34m(param_grid, epochs, hpo_train, hpo_loader, device)\u001b[0m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train the model with the current parameter set\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m trained_model, _ \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the model using MAP score\u001b[39;00m\n\u001b[0;32m     32\u001b[0m map_score \u001b[38;5;241m=\u001b[39m map_on_call(trained_model)\n",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36mloss_function\u001b[1;34m(epochs, bits, num_blocks, block_size, gamma, mu, learning_rate)\u001b[0m\n\u001b[0;32m     24\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat), labels\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m class_probs, binary_codes \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_one_hot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_total_loss(\n\u001b[0;32m     31\u001b[0m     class_probs, labels, binary_codes, num_blocks, block_size, gamma, mu\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xiaoy\\OneDrive\\Desktop\\P7\\p7 project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xiaoy\\OneDrive\\Desktop\\P7\\p7 project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m, in \u001b[0;36mSUBIC_encoder.forward\u001b[1;34m(self, x, use_one_hot)\u001b[0m\n\u001b[0;32m     51\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten if necessary\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_one_hot:\n\u001b[0;32m     57\u001b[0m     binary_codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_one_hot(z)\n",
      "File \u001b[1;32mc:\\Users\\xiaoy\\OneDrive\\Desktop\\P7\\p7 project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xiaoy\\OneDrive\\Desktop\\P7\\p7 project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xiaoy\\OneDrive\\Desktop\\P7\\p7 project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\xiaoy\\OneDrive\\Desktop\\P7\\p7 project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xiaoy\\OneDrive\\Desktop\\P7\\p7 project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xiaoy\\OneDrive\\Desktop\\P7\\p7 project\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper_tuning(param_grid=bits_32_param_dicts, epochs=100, hpo_train=X_hpo, hpo_loader=hpo_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_tuning(param_grid=bits_48_param_dicts, epochs=100, hpo_train=X_hpo, hpo_loader=hpo_loader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
