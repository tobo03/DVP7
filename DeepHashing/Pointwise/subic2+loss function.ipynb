{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/Users/kristiansjorslevnielsen/Documents/DVP7/\"\n",
    "# load data\n",
    "X_train = torch.tensor( np.load(root + \"Features/cifar10/train_features_vgg16_cifar10.npy\" ) )\n",
    "y_train = np.load(root + \"Features/cifar10/train_labels_vgg16_cifar10.npy\" )\n",
    "\n",
    "X_test = torch.tensor( np.load(root + \"Features/cifar10/test_features_vgg16_cifar10.npy\" ) )\n",
    "y_test = np.load(root + \"Features/cifar10/test_labels_vgg16_cifar10.npy\")\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nroot = \"/Users/kristiansjorslevnielsen/Documents/DVP7/\"\\n# load data\\nX_train = torch.tensor( np.load(root + \"Features/NusWidePCA/X_train_nus_wide.npy\" ) )\\ny_train = np.load(root + \"Features/NusWidePCA/y_train_nus_wide.npy\" )\\n\\nX_test = torch.tensor( np.load(root + \"Features/NusWidePCA/X_test_nus_wide.npy\" ) )\\ny_test = np.load(root + \"Features/NusWidePCA/y_test_nus_wide.npy\")\\n\\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "root = \"/Users/kristiansjorslevnielsen/Documents/DVP7/\"\n",
    "# load data\n",
    "X_train = torch.tensor( np.load(root + \"Features/NusWidePCA/X_train_nus_wide.npy\" ) )\n",
    "y_train = np.load(root + \"Features/NusWidePCA/y_train_nus_wide.npy\" )\n",
    "\n",
    "X_test = torch.tensor( np.load(root + \"Features/NusWidePCA/X_test_nus_wide.npy\" ) )\n",
    "y_test = np.load(root + \"Features/NusWidePCA/y_test_nus_wide.npy\")\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(n_components=128)\n",
    "#pca.fit(X_train)\n",
    "#X_train = torch.tensor(pca.transform(X_train), dtype=torch.float)\n",
    "#X_test = torch.tensor(pca.transform(X_test), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAveragePrecision1(test_hashes, training_hashes, test_labels, training_labels):\n",
    "    aps = []\n",
    "    num_queries = len(test_hashes)\n",
    "    for i in tqdm(range(num_queries)):\n",
    "        label = test_labels[i]\n",
    "        distances = (training_hashes != test_hashes[i]).sum(axis=1)  # Hamming distance\n",
    "        tp = (training_labels == label)  # True positive indicator\n",
    "        hash_df = pd.DataFrame({\"distances\": distances, \"tp\": tp.cpu().numpy()})\n",
    "        hash_df = hash_df.sort_values(by=\"distances\").reset_index(drop=True)\n",
    "        hash_df[\"tp_cumsum\"] = hash_df[\"tp\"].cumsum()\n",
    "        hash_df[\"precision\"] = hash_df[\"tp_cumsum\"] / (np.arange(len(hash_df)) + 1)\n",
    "        ap = hash_df[\"precision\"].where(hash_df[\"tp\"] == 1).mean() if hash_df[\"tp\"].sum() > 0 else 0\n",
    "        aps.append(ap)\n",
    "\n",
    "    return np.mean(aps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(a):\n",
    "    b = np.zeros((a.size, a.max() + 1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAveragePrecision(test_hashes, training_hashes, test_labels, training_labels):\n",
    "    aps = []\n",
    "    if len(training_labels.shape) == 1:\n",
    "        training_labels = one_hot_encode(training_labels)\n",
    "        test_labels = one_hot_encode(test_labels)\n",
    "    for i, test_hash in enumerate(tqdm(test_hashes)):\n",
    "        label = test_labels[i]\n",
    "        distances = np.abs(training_hashes - test_hashes[i]).sum(axis=1)\n",
    "        tp = np.where((training_labels*label).sum(axis=1)>0, 1, 0)\n",
    "        hash_df = pd.DataFrame({\"distances\":distances, \"tp\":tp}).reset_index()\n",
    "        hash_df = hash_df.sort_values([\"distances\", \"index\"]).reset_index(drop=True)\n",
    "        hash_df = hash_df.drop([\"index\", \"distances\"], axis=1).reset_index()\n",
    "        hash_df = hash_df[hash_df[\"tp\"]==1]\n",
    "        hash_df[\"tp\"] = hash_df[\"tp\"].cumsum()\n",
    "        hash_df[\"index\"] = hash_df[\"index\"] +1 \n",
    "        precision = np.array(hash_df[\"tp\"]) / np.array(hash_df[\"index\"])\n",
    "        ap = precision.mean()\n",
    "        aps.append(ap)\n",
    "    \n",
    "    return np.array(aps).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SUBIC_encoder(nn.Module): \n",
    "    def __init__(self, input_size=4096, bits=48, num_classes=10, num_blocks=6, block_size=8):\n",
    "        super(SUBIC_encoder, self).__init__()\n",
    "       \n",
    "        assert bits % num_blocks == 0, \"Bits must be divisible by num_blocks\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.bits = bits \n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = block_size\n",
    "        \n",
    "\n",
    "        # Define the encoder structure\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, bits)\n",
    "        )  # Outputs binary feature vectors\n",
    "        \n",
    "        self.fc3 = nn.Linear(bits, num_classes)  # Logits for num_classes\n",
    "    \n",
    "    def block_softmax(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        block_size = x.shape[1] // self.num_blocks\n",
    "        \n",
    "        # Ensure that x has the expected shape\n",
    "        assert x.shape[1] == self.bits, f\"Expected shape [batch_size, {self.bits}], got {x.shape}\"\n",
    "        \n",
    "        # Reshape and apply softmax\n",
    "        x = x.view(batch_size, self.num_blocks, block_size)\n",
    "        x = F.softmax(x, dim=-1) \n",
    "        return x.view(batch_size, -1) #-1 refers to the value that will match the original elements \n",
    "    \n",
    "    def block_one_hot(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = x.view(batch_size, self.num_blocks, self.block_size)\n",
    "        max_indices = x.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Create one-hot encoding\n",
    "        one_hot = torch.zeros_like(x).scatter_(-1, max_indices, 1)\n",
    "\n",
    "        return one_hot.view(batch_size, self.bits)\n",
    "    \n",
    "    def forward(self, x, use_one_hot=False):\n",
    "        # Ensure x is a flat tensor before passing to encoder\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)  # Flatten if necessary\n",
    "\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        if use_one_hot:\n",
    "            binary_codes = self.block_one_hot(z)\n",
    "        else:\n",
    "            binary_codes = self.block_softmax(z)\n",
    "\n",
    "        class_probs = F.softmax(self.fc3(binary_codes), dim=-1) \n",
    "\n",
    "        return class_probs, binary_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    entropy_result = -torch.sum(p * torch.log2(p + 1e-30), dim=-1)\n",
    "    return entropy_result\n",
    "\n",
    "def cross_entropy(class_prob, target):\n",
    "    if len(target.shape) >1:\n",
    "        s = (class_prob*target).sum(axis=1)\n",
    "    else:\n",
    "        s = class_prob[torch.arange(len(target)), target]\n",
    "    return -torch.log2(s)/torch.log2(torch.FloatTensor([class_prob.shape[1]]))\n",
    "\n",
    "def compute_total_loss(class_probs, target, binary_codes, num_blocks, block_size, gamma=0.5, mu=0.5):\n",
    "    \"\"\"\n",
    "    Computes the total loss, which includes:\n",
    "    - Cross-entropy classification loss\n",
    "    - Mean entropy loss (encouraging one-hot encoding within each block)\n",
    "    - Batch entropy loss (encouraging uniform distribution across blocks)\n",
    "    \n",
    "    Parameters:\n",
    "    - class_probs: The class probabilities from the classification layer.\n",
    "    - target: The true labels.\n",
    "    - binary_codes: The binary codes generated by the encoder.\n",
    "    - num_blocks: The number of blocks in the binary codes.\n",
    "    - block_size: The size of each block in the binary codes.\n",
    "    - gamma: Weight for the mean entropy loss.\n",
    "    - mu: Weight for the batch entropy loss.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    classification_loss = cross_entropy(class_probs, target)\n",
    "\n",
    "    batch_size = binary_codes.shape[0]\n",
    "    binary_codes = binary_codes.view(batch_size, num_blocks, block_size) #used in structure encoding\n",
    "\n",
    "\n",
    "\n",
    "    #Mean Entropy Loss (encourages each block to resemble a one-hot vector) using softmax binary code\n",
    "    mean_entropy_loss = entropy(binary_codes).mean(dim=1)\n",
    "\n",
    "    #Batch Entropy Loss (encourages uniform distribution across blocks)\n",
    "    average_support = binary_codes.mean(dim=0)  \n",
    "    batch_entropy_loss = entropy(average_support).mean(dim=0)\n",
    "\n",
    "    #Combine losses with weights gamma and mu\n",
    "    entropy_loss = (gamma * mean_entropy_loss - mu * batch_entropy_loss)/torch.log2(torch.FloatTensor([block_size]))\n",
    "    total_loss = (classification_loss + entropy_loss).mean()\n",
    "    \n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "#logits, binary_codes = model(X_train, use_one_hot=False)\n",
    "#loss = compute_total_loss(logits, y_train_tensor, binary_codes, num_blocks=8, block_size=4, gamma=0.5, mu=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SUBIC_encoder(bits = 48, input_size=X_train.shape[1], num_classes = y_train.shape[-1], num_blocks = 16, block_size = 3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "class_probs, binary_codes = model.forward(X_train, use_one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: -0.1481\n",
      "Epoch [2/100], Loss: -0.4006\n",
      "Epoch [3/100], Loss: -0.4158\n",
      "Epoch [4/100], Loss: -0.4215\n",
      "Epoch [5/100], Loss: -0.4245\n",
      "Epoch [6/100], Loss: -0.4259\n",
      "Epoch [7/100], Loss: -0.4269\n",
      "Epoch [8/100], Loss: -0.4280\n",
      "Epoch [9/100], Loss: -0.4285\n",
      "Epoch [10/100], Loss: -0.4301\n",
      "Epoch [11/100], Loss: -0.4315\n",
      "Epoch [12/100], Loss: -0.4317\n",
      "Epoch [13/100], Loss: -0.4322\n",
      "Epoch [14/100], Loss: -0.4326\n",
      "Epoch [15/100], Loss: -0.4330\n",
      "Epoch [16/100], Loss: -0.4329\n",
      "Epoch [17/100], Loss: -0.4354\n",
      "Epoch [18/100], Loss: -0.4347\n",
      "Epoch [19/100], Loss: -0.4363\n",
      "Epoch [20/100], Loss: -0.4350\n",
      "Epoch [21/100], Loss: -0.4356\n",
      "Epoch [22/100], Loss: -0.4345\n",
      "Epoch [23/100], Loss: -0.4357\n",
      "Epoch [24/100], Loss: -0.4361\n",
      "Epoch [25/100], Loss: -0.4354\n",
      "Epoch [26/100], Loss: -0.4355\n",
      "Epoch [27/100], Loss: -0.4362\n",
      "Epoch [28/100], Loss: -0.4366\n",
      "Epoch [29/100], Loss: -0.4368\n",
      "Epoch [30/100], Loss: -0.4376\n",
      "Epoch [31/100], Loss: -0.4370\n",
      "Epoch [32/100], Loss: -0.4378\n",
      "Epoch [33/100], Loss: -0.4377\n",
      "Epoch [34/100], Loss: -0.4370\n",
      "Epoch [35/100], Loss: -0.4385\n",
      "Epoch [36/100], Loss: -0.4368\n",
      "Epoch [37/100], Loss: -0.4384\n",
      "Epoch [38/100], Loss: -0.4392\n",
      "Epoch [39/100], Loss: -0.4387\n",
      "Epoch [40/100], Loss: -0.4372\n",
      "Epoch [41/100], Loss: -0.4378\n",
      "Epoch [42/100], Loss: -0.4400\n",
      "Epoch [43/100], Loss: -0.4393\n",
      "Epoch [44/100], Loss: -0.4394\n",
      "Epoch [45/100], Loss: -0.4392\n",
      "Epoch [46/100], Loss: -0.4384\n",
      "Epoch [47/100], Loss: -0.4396\n",
      "Epoch [48/100], Loss: -0.4394\n",
      "Epoch [49/100], Loss: -0.4410\n",
      "Epoch [50/100], Loss: -0.4388\n",
      "Epoch [51/100], Loss: -0.4403\n",
      "Epoch [52/100], Loss: -0.4404\n",
      "Epoch [53/100], Loss: -0.4389\n",
      "Epoch [54/100], Loss: -0.4414\n",
      "Epoch [55/100], Loss: -0.4403\n",
      "Epoch [56/100], Loss: -0.4389\n",
      "Epoch [57/100], Loss: -0.4408\n",
      "Epoch [58/100], Loss: -0.4413\n",
      "Epoch [59/100], Loss: -0.4416\n",
      "Epoch [60/100], Loss: -0.4415\n",
      "Epoch [61/100], Loss: -0.4410\n",
      "Epoch [62/100], Loss: -0.4403\n",
      "Epoch [63/100], Loss: -0.4410\n",
      "Epoch [64/100], Loss: -0.4412\n",
      "Epoch [65/100], Loss: -0.4382\n",
      "Epoch [66/100], Loss: -0.4404\n",
      "Epoch [67/100], Loss: -0.4404\n",
      "Epoch [68/100], Loss: -0.4413\n",
      "Epoch [69/100], Loss: -0.4399\n",
      "Epoch [70/100], Loss: -0.4385\n",
      "Epoch [71/100], Loss: -0.4388\n",
      "Epoch [72/100], Loss: -0.4410\n",
      "Epoch [73/100], Loss: -0.4407\n",
      "Epoch [74/100], Loss: -0.4404\n",
      "Epoch [75/100], Loss: -0.4400\n",
      "Epoch [76/100], Loss: -0.4415\n",
      "Epoch [77/100], Loss: -0.4415\n",
      "Epoch [78/100], Loss: -0.4422\n",
      "Epoch [79/100], Loss: -0.4417\n",
      "Epoch [80/100], Loss: -0.4417\n",
      "Epoch [81/100], Loss: -0.4420\n",
      "Epoch [82/100], Loss: -0.4396\n",
      "Epoch [83/100], Loss: -0.4409\n",
      "Epoch [84/100], Loss: -0.4404\n",
      "Epoch [85/100], Loss: -0.4400\n",
      "Epoch [86/100], Loss: -0.4423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     27\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    392\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     adam(\n\u001b[1;32m    169\u001b[0m         params_with_grad,\n\u001b[1;32m    170\u001b[0m         grads,\n\u001b[1;32m    171\u001b[0m         exp_avgs,\n\u001b[1;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    174\u001b[0m         state_steps,\n\u001b[1;32m    175\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m         has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    177\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    178\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    179\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    180\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    181\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    186\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    187\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    188\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m func(params,\n\u001b[1;32m    319\u001b[0m      grads,\n\u001b[1;32m    320\u001b[0m      exp_avgs,\n\u001b[1;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    323\u001b[0m      state_steps,\n\u001b[1;32m    324\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    325\u001b[0m      has_complex\u001b[39m=\u001b[39;49mhas_complex,\n\u001b[1;32m    326\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    327\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    328\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    329\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    330\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    331\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    332\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    333\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    334\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    335\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    439\u001b[0m         denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    440\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    443\u001b[0m     param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[1;32m    445\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        class_probs, binary_codes = model.forward(images, use_one_hot=False)\n",
    "\n",
    "\n",
    "        # Compute loss and update model\n",
    "        loss = compute_total_loss(class_probs, labels, binary_codes, num_blocks=16, block_size=3, gamma=0.5, mu=0.5)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss / len(train_loader))\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probs, hash = model.forward(X_test, use_one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0351e-03, 1.4728e-03, 7.1268e-03,  ..., 1.5334e-10, 1.7459e-10,\n",
       "        2.3274e-10], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 48)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, train_binary_codes = model(X_train, use_one_hot=True)\n",
    "_, test_binary_codes = model(X_test, use_one_hot=True)\n",
    "\n",
    "train_binary_codes = train_binary_codes.detach().numpy()\n",
    "test_binary_codes = test_binary_codes.detach().numpy()\n",
    "test_binary_codes[9000:,].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:03<00:00, 81.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6837244289367512"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, train_binary_codes = model(X_train, use_one_hot=True)\n",
    "_, test_binary_codes = model(X_test, use_one_hot=True)\n",
    "\n",
    "train_binary_codes = train_binary_codes.detach().numpy()\n",
    "test_binary_codes = test_binary_codes.detach().numpy()\n",
    "\n",
    "database = train_binary_codes\n",
    "database_label = y_train\n",
    "query_set = test_binary_codes\n",
    "query_set_label = y_test\n",
    "\n",
    "\n",
    "meanAveragePrecision(query_set, database, query_set_label, database_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m all_db_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_db_labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate MAP Score\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m map_score \u001b[38;5;241m=\u001b[39m \u001b[43mmeanAveragePrecision\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_query_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_db_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_query_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_db_labels\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmap_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[103], line 4\u001b[0m, in \u001b[0;36mmeanAveragePrecision\u001b[0;34m(test_hashes, training_hashes, test_labels, training_labels)\u001b[0m\n\u001b[1;32m      2\u001b[0m aps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(training_labels\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     training_labels \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     test_labels \u001b[38;5;241m=\u001b[39m one_hot_encode(test_labels)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, test_hash \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(test_hashes)):\n",
      "Cell \u001b[0;32mIn[102], line 2\u001b[0m, in \u001b[0;36mone_hot_encode\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot_encode\u001b[39m(a):\n\u001b[0;32m----> 2\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     b[np\u001b[38;5;241m.\u001b[39marange(a\u001b[38;5;241m.\u001b[39msize), a] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m b\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_query_codes, all_query_labels = [], []\n",
    "all_db_codes, all_db_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        _, binary_codes = model(images, use_one_hot=True)\n",
    "\n",
    "        # Ensure binary_codes is a tensor\n",
    "        if not isinstance(binary_codes, torch.Tensor):\n",
    "            raise TypeError(\"Expected binary_codes to be a tensor.\")\n",
    "        \n",
    "        all_db_codes.append(binary_codes)\n",
    "        all_db_labels.append(labels)\n",
    "\n",
    "        if len(all_query_codes) == 0:  \n",
    "            all_query_codes.append(binary_codes.clone())  \n",
    "            all_query_labels.append(labels.clone())\n",
    "\n",
    "# Concatenate all tensors\n",
    "all_query_codes = torch.cat(all_query_codes, dim=0)\n",
    "all_query_labels = torch.cat(all_query_labels, dim=0)\n",
    "all_db_codes = torch.cat(all_db_codes, dim=0)\n",
    "all_db_labels = torch.cat(all_db_labels, dim=0)\n",
    "\n",
    "# Calculate MAP Score\n",
    "map_score = meanAveragePrecision(\n",
    "    all_query_codes,\n",
    "    all_db_codes,\n",
    "    all_query_labels,\n",
    "    all_db_labels\n",
    "    )\n",
    "\n",
    "print(f\"MAP Score: {map_score:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
