{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import distance_matrix\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../../'\n",
    "sys.path.append(root)\n",
    "#from HelpfulFunctions.batchCreation import createBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root = \"C:/Users/xiaoy/OneDrive/Desktop/P7/p7 project/DVP7/\"\n",
    "# load data\n",
    "root = '../../'\n",
    "sys.path.append(root)\n",
    "X_train = torch.tensor( np.load(root + \"Features/train_features_vgg16_cifar10.npy\" ) )\n",
    "y_train = np.load(root + \"Features/train_labels_vgg16_cifar10.npy\" )\n",
    "\n",
    "X_test = torch.tensor( np.load(root + \"Features/test_features_vgg16_cifar10.npy\" ) )\n",
    "y_test = np.load(root + \"Features/test_labels_vgg16_cifar10.npy\")\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45000, 4096])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanAveragePrecision(test_hashes, training_hashes, test_labels, training_labels):\n",
    "    aps = []\n",
    "    num_queries = len(test_hashes)\n",
    "    for i in tqdm(range(num_queries)):\n",
    "        label = test_labels[i]\n",
    "        distances = (training_hashes != test_hashes[i]).sum(axis=1)  # Hamming distance\n",
    "        tp = (training_labels == label).astype(int)  # True positive indicator\n",
    "        hash_df = pd.DataFrame({\"distances\": distances, \"tp\": tp})\n",
    "        hash_df = hash_df.sort_values(by=\"distances\").reset_index(drop=True)\n",
    "        hash_df[\"tp_cumsum\"] = hash_df[\"tp\"].cumsum()\n",
    "        hash_df[\"precision\"] = hash_df[\"tp_cumsum\"] / (np.arange(len(hash_df)) + 1)\n",
    "        ap = hash_df[\"precision\"].where(hash_df[\"tp\"] == 1).mean() if hash_df[\"tp\"].sum() > 0 else 0\n",
    "        aps.append(ap)\n",
    "\n",
    "    return np.mean(aps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SUBIC_encoder(nn.Module): \n",
    "    def __init__(self, bits=48, num_classes=10, num_blocks=8, block_size=8):\n",
    "        super(SUBIC_encoder, self).__init__()\n",
    "       \n",
    "        assert bits % num_blocks == 0, \"Bits must be divisible by num_blocks\"\n",
    "\n",
    "        self.bits = bits \n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Define the encoder structure\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(4096, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, bits)\n",
    "        )  # Outputs binary feature vectors\n",
    "        \n",
    "        self.fc3 = nn.Linear(bits, num_classes)  # Logits for num_classes\n",
    "    \n",
    "    def block_softmax(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        block_size = x.shape[1] // self.num_blocks\n",
    "        \n",
    "        # Ensure that x has the expected shape\n",
    "        assert x.shape[1] == self.bits, f\"Expected shape [batch_size, {self.bits}], got {x.shape}\"\n",
    "        \n",
    "        # Reshape and apply softmax\n",
    "        x = x.view(batch_size, self.num_blocks, block_size)\n",
    "        x = F.softmax(x, dim=-1) \n",
    "        return x.view(batch_size, -1)\n",
    "    \n",
    "    def block_one_hot(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = x.view(batch_size, self.num_blocks, self.block_size)\n",
    "        max_indices = x.argmax(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Create one-hot encoding\n",
    "        one_hot = torch.zeros_like(x).scatter_(-1, max_indices, 1)\n",
    "\n",
    "        return one_hot.view(batch_size, self.bits)\n",
    "    \n",
    "    def forward(self, x, use_one_hot=False):\n",
    "        # Ensure x is a flat tensor before passing to encoder\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)  # Flatten if necessary\n",
    "\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        if use_one_hot:\n",
    "            binary_codes = self.block_one_hot(z)\n",
    "        else:\n",
    "            binary_codes = self.block_softmax(z)\n",
    "\n",
    "        logits = self.fc3(binary_codes)\n",
    "\n",
    "        return logits, binary_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SUBIC_encoder(bits = 48, num_classes = 10, num_blocks = 8, block_size = 6)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "gamma, mu = 0.5, 0.1  # Adjust based on experimentation\n",
    "epochs = 10\n",
    "logits, binary_codes = model(X_train, use_one_hot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45000, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(logits, target, binary_codes, num_blocks, block_size, gamma=0.05, mu=0.05):\n",
    "    \"\"\"\n",
    "    Computes the total loss, which includes:\n",
    "    - Cross-entropy classification loss\n",
    "    - Mean entropy loss (encouraging one-hot encoding within each block)\n",
    "    - Batch entropy loss (encouraging uniform distribution across blocks)\n",
    "    \n",
    "    Parameters:\n",
    "    - logits: The output logits from the classification layer.\n",
    "    - target: The true labels.\n",
    "    - binary_codes: The binary codes generated by the encoder.\n",
    "    - num_blocks: The number of blocks in the binary codes.\n",
    "    - block_size: The size of each block in the binary codes.\n",
    "    - gamma: Weight for the mean entropy loss.\n",
    "    - mu: Weight for the batch entropy loss.\n",
    "    \n",
    "    \"\"\"\n",
    "    classification_loss = F.cross_entropy(logits, target)\n",
    "\n",
    "    batch_size = binary_codes.shape[0]\n",
    "    binary_codes = binary_codes.view(batch_size, num_blocks, block_size) #used in structure encoding\n",
    "\n",
    "    #Mean Entropy Loss (encourages each block to resemble a one-hot vector) using softmax binary codes\n",
    "    mean_entropy_loss = -torch.sum(binary_codes * torch.log2(binary_codes + 1e-10), dim=-1).mean()\n",
    "\n",
    "    #Batch Entropy Loss (encourages uniform distribution across blocks)\n",
    "    average_support = binary_codes.mean(dim=0)  \n",
    "    batch_entropy_loss = torch.sum(-average_support * torch.log2(average_support + 1e-10)).mean()\n",
    "\n",
    "    #Combine losses with weights gamma and mu\n",
    "    entropy_loss = gamma * mean_entropy_loss - mu * batch_entropy_loss\n",
    "    total_loss = classification_loss + entropy_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "#logits, binary_codes = model(X_train, use_one_hot=False)\n",
    "#loss = compute_total_loss(logits, y_train_tensor, binary_codes, num_blocks=8, block_size=4, gamma=0.5, mu=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3070 Ti is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: -0.0281\n",
      "Epoch [2/50], Loss: -0.0006\n",
      "Epoch [3/50], Loss: -0.0318\n",
      "Epoch [4/50], Loss: -0.0387\n",
      "Epoch [5/50], Loss: -0.0637\n",
      "Epoch [6/50], Loss: -0.0766\n",
      "Epoch [7/50], Loss: -0.0471\n",
      "Epoch [8/50], Loss: -0.0498\n",
      "Epoch [9/50], Loss: -0.0593\n",
      "Epoch [10/50], Loss: -0.0796\n",
      "Epoch [11/50], Loss: -0.0951\n",
      "Epoch [12/50], Loss: -0.0928\n",
      "Epoch [13/50], Loss: -0.0805\n",
      "Epoch [14/50], Loss: -0.0423\n",
      "Epoch [15/50], Loss: -0.0278\n",
      "Epoch [16/50], Loss: -0.0736\n",
      "Epoch [17/50], Loss: -0.0706\n",
      "Epoch [18/50], Loss: -0.0295\n",
      "Epoch [19/50], Loss: -0.0511\n",
      "Epoch [20/50], Loss: -0.0767\n",
      "Epoch [21/50], Loss: -0.1034\n",
      "Epoch [22/50], Loss: -0.1149\n",
      "Epoch [23/50], Loss: -0.0681\n",
      "Epoch [24/50], Loss: -0.0610\n",
      "Epoch [25/50], Loss: -0.0140\n",
      "Epoch [26/50], Loss: -0.0838\n",
      "Epoch [27/50], Loss: -0.0530\n",
      "Epoch [28/50], Loss: -0.1122\n",
      "Epoch [29/50], Loss: -0.0760\n",
      "Epoch [30/50], Loss: -0.0936\n",
      "Epoch [31/50], Loss: -0.0916\n",
      "Epoch [32/50], Loss: -0.1185\n",
      "Epoch [33/50], Loss: -0.1496\n",
      "Epoch [34/50], Loss: -0.1279\n",
      "Epoch [35/50], Loss: -0.0686\n",
      "Epoch [36/50], Loss: -0.1270\n",
      "Epoch [37/50], Loss: -0.1052\n",
      "Epoch [38/50], Loss: -0.0701\n",
      "Epoch [39/50], Loss: -0.1083\n",
      "Epoch [40/50], Loss: -0.0962\n",
      "Epoch [41/50], Loss: -0.0977\n",
      "Epoch [42/50], Loss: -0.0893\n",
      "Epoch [43/50], Loss: -0.0805\n",
      "Epoch [44/50], Loss: -0.0996\n",
      "Epoch [45/50], Loss: -0.1045\n",
      "Epoch [46/50], Loss: -0.0675\n",
      "Epoch [47/50], Loss: -0.0370\n",
      "Epoch [48/50], Loss: 0.0070\n",
      "Epoch [49/50], Loss: 0.0393\n",
      "Epoch [50/50], Loss: -0.0055\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        model.to('cuda')\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits, binary_codes = model(images, use_one_hot=False)\n",
    "\n",
    "        # Compute loss and update model\n",
    "        loss = compute_total_loss(logits, labels, binary_codes, num_blocks=8, block_size=6, gamma=0.05, mu=0.05)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 645.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP Score: 0.57019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_query_codes, all_query_labels = [], []\n",
    "all_db_codes, all_db_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        \n",
    "        model.to('cuda')\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        _, binary_codes = model(images, use_one_hot=True)\n",
    "\n",
    "        # Ensure binary_codes is a tensor\n",
    "        if not isinstance(binary_codes, torch.Tensor):\n",
    "            raise TypeError(\"Expected binary_codes to be a tensor.\")\n",
    "        \n",
    "        all_db_codes.append(binary_codes)\n",
    "        all_db_labels.append(labels)\n",
    "\n",
    "        if len(all_query_codes) == 0:  \n",
    "            all_query_codes.append(binary_codes.clone())  \n",
    "            all_query_labels.append(labels.clone())\n",
    "\n",
    "# Concatenate all tensors\n",
    "all_query_codes = torch.cat(all_query_codes, dim=0)\n",
    "all_query_labels = torch.cat(all_query_labels, dim=0)\n",
    "all_db_codes = torch.cat(all_db_codes, dim=0)\n",
    "all_db_labels = torch.cat(all_db_labels, dim=0)\n",
    "\n",
    "# Calculate MAP Score\n",
    "map_score = meanAveragePrecision(\n",
    "    all_query_codes.cpu().numpy(),\n",
    "    all_db_codes.cpu().numpy(),\n",
    "    all_query_labels.cpu().numpy(),\n",
    "    all_db_labels.cpu().numpy()\n",
    "    )\n",
    "\n",
    "print(f\"MAP Score: {map_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
