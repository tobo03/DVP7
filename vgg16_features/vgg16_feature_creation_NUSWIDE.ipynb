{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from PIL import Image\n",
    "import io\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Nus wide dataset is loaded from kaggle, the structure is funky, so ... \n",
    "zip_path = r'C:\\Users\\Test\\Desktop\\P7_Data\\Nus_Wide\\archive.zip'\n",
    "image_txt_file = 'NUS-WIDE/database_img.txt'\n",
    "label_txt_file = \"NUS-WIDE/database_label.txt\"\n",
    "\n",
    "# Step 1: Read the image paths from database_img.txt\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    with zip_ref.open(image_txt_file) as file:\n",
    "        image_paths = [line.decode('utf-8').strip() for line in file]\n",
    "# Step 1: Read the image paths from database_img.txt\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    with zip_ref.open(label_txt_file) as file:\n",
    "        labels = [line.decode('utf-8').strip() for line in file]\n",
    "\n",
    "# Ensure image_paths and labels match in length (for simplicity)\n",
    "assert len(image_paths) == len(labels), \"Mismatch between number of images and labels\" # This would be  a problem \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(labels, image_paths)),columns = [\"labels\",\"img_path\"]) \n",
    "df_1 = df[df['labels'].apply(lambda x: len(x) == 1)] # Only keep images with single label (multi label out of scope!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_folder_in_zip = \"NUS-WIDE/\"\n",
    "\n",
    "# The classic transform! \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),             # Resize to 224x224\n",
    "    transforms.ToTensor(),                     # Convert to a PyTorch tensor and scale to [0, 1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 269648/269648 [06:57<00:00, 646.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract the filtered image paths and their corresponding labels\n",
    "image_paths_filtered = df_1['img_path'].tolist()\n",
    "labels_filtered = df_1['labels'].tolist()  \n",
    "image_label_tuples = []\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref: # Preprocess the relevant images, heavy computation, but should work with 32 gb RAM. \n",
    "    # List all files in the 'images' folder inside the zip\n",
    "    image_files = [f for f in zip_ref.namelist() if f.startswith(images_folder_in_zip) and f.endswith('.jpg')]\n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        # Check if the current image is in the filtered list\n",
    "        img_path = image_file.replace(images_folder_in_zip, \"\")  # Remove folder part for matching\n",
    "        if img_path in image_paths_filtered: # Check if relevant image, (1 label only)\n",
    "            idx = image_paths_filtered.index(img_path)\n",
    "            label = labels_filtered[idx][0]  # Since the labels are stored as lists with one element\n",
    "            with zip_ref.open(image_file) as img_file:\n",
    "                # Open image, apply transformations\n",
    "                img = Image.open(io.BytesIO(img_file.read())).convert(\"RGB\")  # Convert to RGB\n",
    "                img_tensor = transform(img)  # Transform to tensor and normalize\n",
    "                \n",
    "                # Append a tuple (image_tensor, label) to the list\n",
    "                image_label_tuples.append((img_tensor, label))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam the doofus had string labels, so he makes it integer based\n",
    "unique_labels = set(label for _, label in image_label_tuples)  # Extract unique string labels\n",
    "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}  # Map labels to integers\n",
    "image_label_tuples_int = [(img_tensor, label_to_int[label]) for img_tensor, label in image_label_tuples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Extracting features: 100%|██████████| 636/636 [05:41<00:00,  1.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([81314, 4096])\n",
      "Labels shape: torch.Size([81314])\n"
     ]
    }
   ],
   "source": [
    "# ved ikke om denne class er vigtig\n",
    "class ImageLabelDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading images and their labels.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_label_tuples):\n",
    "        self.image_label_tuples = image_label_tuples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_label_tuples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor, label = self.image_label_tuples[idx]\n",
    "        return image_tensor, label\n",
    "\n",
    "batch_size = 128\n",
    "dataset = ImageLabelDataset(image_label_tuples_int)  # image_label_tuples_int is a list of (image_tensor, int_label)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(dataloader, model, device):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        # Wrap the DataLoader with tqdm for progress bar\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Extracting features\", unit=\"batch\"):\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Forward pass through the VGG-16 model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Flatten the output from conv layers to (batch_size, feature_dim)\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "\n",
    "            features.append(outputs.cpu())  # Move to CPU and store\n",
    "            labels.append(targets)\n",
    "\n",
    "    # Concatenate all features and labels\n",
    "    features = torch.cat(features)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg16 = models.vgg16(pretrained=True).to(device)\n",
    "\n",
    "# Modify VGG-16 for feature extraction (keep only the convolutional layers)\n",
    "vgg16.classifier = vgg16.classifier[:4]  # Remove fully connected layers\n",
    "vgg16.eval()  # Set to evaluation mode\n",
    "\n",
    "# Extract features using the dataloader and the pre-trained VGG-16 model\n",
    "features, labels = extract_features(dataloader, vgg16, device)\n",
    "\n",
    "# Print the shape of the extracted features\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.cpu().detach().numpy()\n",
    "labels = labels.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Split into train and test (75% train, 16.67% test)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, test_size=0.1667, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split the train set into train and validation (75% train, 8.33% validation)\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    train_features, train_labels, test_size=0.1111, stratify=train_labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for the stratisfied sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60230, 4096)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13556, 4096)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7528, 4096)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6117\n",
       "0    3174\n",
       "8    1264\n",
       "2     721\n",
       "7     627\n",
       "4     433\n",
       "3     422\n",
       "9     403\n",
       "6     370\n",
       "5      25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(test_labels).value_counts() # Kig på det her, Det er repræsentativt !!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    36690\n",
       "0    19036\n",
       "8     7585\n",
       "2     4327\n",
       "7     3759\n",
       "4     2597\n",
       "3     2532\n",
       "9     2420\n",
       "6     2220\n",
       "5      148\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts() # Ift denne basis, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnp.save(r'C:\\\\Users\\\\Test\\\\Desktop\\\\P7_Data\\\\Nus_Wide_preprocessed\\\\train_features_vgg16_NUSWIDE.npy', train_features)\\nnp.save(r'C:\\\\Users\\\\Test\\\\Desktop\\\\P7_Data\\\\Nus_Wide_preprocessed\\\\test_features_vgg16_NUSWIDE.npy', test_features)\\nnp.save(r'C:\\\\Users\\\\Test\\\\Desktop\\\\P7_Data\\\\Nus_Wide_preprocessed\\\\train_labels_vgg16_NUSWIDE.npy', train_labels)\\nnp.save(r'C:\\\\Users\\\\Test\\\\Desktop\\\\P7_Data\\\\Nus_Wide_preprocessed\\\\test_labels_vgg16_NUSWIDE.npy', test_labels)\\nnp.save(r'C:\\\\Users\\\\Test\\\\Desktop\\\\P7_Data\\\\Nus_Wide_preprocessed\\\\val_features_vgg16_NUSWIDE.npy', val_features)\\nnp.save(r'C:\\\\Users\\\\Test\\\\Desktop\\\\P7_Data\\\\Nus_Wide_preprocessed\\\\val_labels_vgg16_NUSWIDE.npy', val_labels)\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"\"\"\n",
    "np.save(r'C:\\Users\\Test\\Desktop\\P7_Data\\Nus_Wide_preprocessed\\train_features_vgg16_NUSWIDE.npy', train_features)\n",
    "np.save(r'C:\\Users\\Test\\Desktop\\P7_Data\\Nus_Wide_preprocessed\\test_features_vgg16_NUSWIDE.npy', test_features)\n",
    "np.save(r'C:\\Users\\Test\\Desktop\\P7_Data\\Nus_Wide_preprocessed\\train_labels_vgg16_NUSWIDE.npy', train_labels)\n",
    "np.save(r'C:\\Users\\Test\\Desktop\\P7_Data\\Nus_Wide_preprocessed\\test_labels_vgg16_NUSWIDE.npy', test_labels)\n",
    "np.save(r'C:\\Users\\Test\\Desktop\\P7_Data\\Nus_Wide_preprocessed\\val_features_vgg16_NUSWIDE.npy', val_features)\n",
    "np.save(r'C:\\Users\\Test\\Desktop\\P7_Data\\Nus_Wide_preprocessed\\val_labels_vgg16_NUSWIDE.npy', val_labels)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5032818 , -0.7674331 , -2.7721295 , ..., -0.9128053 ,\n",
       "        -0.6340852 , -0.38517237],\n",
       "       [-2.2796998 , -1.3048555 , -2.2760553 , ..., -0.13423966,\n",
       "         1.7606338 , -1.4704621 ],\n",
       "       [-1.1428131 , -0.8343891 , -1.9038177 , ..., -1.6542923 ,\n",
       "         0.2152445 ,  0.02428141],\n",
       "       ...,\n",
       "       [-0.46369258, -0.972303  , -2.1136277 , ..., -1.2902718 ,\n",
       "        -0.21800174,  0.01005483],\n",
       "       [-1.9904902 , -1.0046502 , -1.3107908 , ..., -1.8287351 ,\n",
       "         1.3140104 , -0.9812664 ],\n",
       "       [-2.1479685 , -2.970149  , -1.8542986 , ..., -2.571137  ,\n",
       "        -1.0085739 ,  0.6855662 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5033, -0.7674, -2.7721,  ..., -0.9128, -0.6341, -0.3852])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193734"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of multiclass, and class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_class = 0\n",
    "zero_class = 0\n",
    "multi_class = 0 \n",
    "one_label = []\n",
    "for i in range(len(labels)):\n",
    "    if len(labels[i])==1:\n",
    "        one_class += 1 \n",
    "        one_label.append(labels[i])\n",
    "    elif len(labels[i]) == 0: \n",
    "        zero_class += 1 \n",
    "    else:\n",
    "        multi_class += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.Series(one_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    36690\n",
       "4    19036\n",
       "0     7585\n",
       "7     4327\n",
       "3     3759\n",
       "1     2597\n",
       "5     2532\n",
       "6     2420\n",
       "8     2220\n",
       "9      148\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81314"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
